{"meta":{"title":"Dreamcat","subtitle":"ドリームキャット","description":"あなたが好きです","author":"买斯基","url":"http://dreamcat.ink","root":"/"},"pages":[{"title":"about","date":"2020-03-25T16:23:14.000Z","updated":"2020-03-25T16:23:26.064Z","comments":true,"path":"about/index.html","permalink":"http://dreamcat.ink/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-03-25T16:22:14.000Z","updated":"2020-03-25T16:22:24.786Z","comments":true,"path":"categories/index.html","permalink":"http://dreamcat.ink/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-03-25T16:22:52.000Z","updated":"2020-03-25T16:23:03.647Z","comments":true,"path":"tags/index.html","permalink":"http://dreamcat.ink/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"个人吐血系列-总结计算机网络","slug":"个人吐血系列-总结计算机网络","date":"2020-04-01T17:42:22.000Z","updated":"2021-01-01T09:56:49.548Z","comments":true,"path":"2020/04/02/ge-ren-tu-xie-xi-lie-zong-jie-ji-suan-ji-wang-luo/","link":"","permalink":"http://dreamcat.ink/2020/04/02/ge-ren-tu-xie-xi-lie-zong-jie-ji-suan-ji-wang-luo/","excerpt":"","text":"计算机网络，那是必问环节咯，而且问的也都很固定，也多看看以及理解理解。 大纲 网络模型 简要概括 物理层：底层数据传输，如网线；网卡标准。 数据链路层：定义数据的基本格式，如何传输，如何标识；如网卡MAC地址。 网络层：定义IP编址，定义路由功能；如不同设备的数据转发。 传输层：端到端传输数据的基本功能；如 TCP、UDP。 会话层：控制应用程序之间会话能力；如不同软件数据分发给不同软件。 标识层：数据格式标识，基本压缩加密功能。 应用层：各种应用软件，包括 Web 应用。 流程比如，计算机 A 和 计算机 B 要进行信息交互，比如 A 上开发了一个网页，需要 B 去访问。B 发出一个请求给 A，那么请求数据从 B 的 应用层开始向下传到表示层、再从表示层传到会话层直到物理层，通过物理层传递到 A，A 的物理层接到请求后将请求向上传递到自己的应用层，应用层再将要请求的数据向自己的物理层方向传递然后 B 接到数据传递数据到自己的应用层。 说明： 在四层，既传输层数据被称作段（Segments）； 三层网络层数据被称做包（Packages）； 二层数据链路层时数据被称为帧（Frames）； 一层物理层时数据被称为比特流（Bits）。 常见的端口号和协议号 总结 网络七层模型是一个标准，而非实现。 网络四层模型是一个实现的应用模型。 网络四层模型由七层模型简化合并而来。 ping命令基于哪一层协议的原理是什么？ping命令基于网络层的命令，是基于ICMP协议工作的。 DNSDNS是什么官方解释：DNS（Domain Name System，域名系统），因特网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。通过主机名，最终得到该主机名对应的IP地址的过程叫做域名解析（或主机名解析）。 通俗的讲，我们更习惯于记住一个网站的名字，比如www.baidu.com,而不是记住它的ip地址，比如：167.23.10.2。 谈谈DNS解析过程 请求一旦发起，若是chrome浏览器，先在浏览器找之前有没有缓存过的域名所对应的ip地址，有的话，直接跳过dns解析了，若是没有，就会找硬盘的hosts文件，看看有没有，有的话，直接找到hosts文件里面的ip 如果本地的hosts文件没有能的到对应的ip地址，浏览器会发出一个dns请求到本地dns服务器，本地dns服务器一般都是你的网络接入服务器商提供，比如中国电信，中国移动等。 查询你输入的网址的DNS请求到达本地DNS服务器之后，本地DNS服务器会首先查询它的缓存记录，如果缓存中有此条记录，就可以直接返回结果，此过程是递归的方式进行查询。如果没有，本地DNS服务器还要向DNS根服务器进行查询。 本地DNS服务器继续向域服务器发出请求，在这个例子中，请求的对象是.com域服务器。.com域服务器收到请求之后，也不会直接返回域名和IP地址的对应关系，而是告诉本地DNS服务器，你的域名的解析服务器的地址。 最后，本地DNS服务器向域名的解析服务器发出请求，这时就能收到一个域名和IP地址对应关系，本地DNS服务器不仅要把IP地址返回给用户电脑，还要把这个对应关系保存在缓存中，以备下次别的用户查询时，可以直接返回结果，加快网络访问。 DNS查询方式递归解析当局部DNS服务器自己不能回答客户机的DNS查询时，它就需要向其他DNS服务器进行查询。此时有两种方式。局部DNS服务器自己负责向其他DNS服务器进行查询，一般是先向该域名的根域服务器查询，再由根域名服务器一级级向下查询。最后得到的查询结果返回给局部DNS服务器，再由局部DNS服务器返回给客户端。 迭代解析当局部DNS服务器自己不能回答客户机的DNS查询时，也可以通过迭代查询的方式进行解析。局部DNS服务器不是自己向其他DNS服务器进行查询，而是把能解析该域名的其他DNS服务器的IP地址返回给客户端DNS程序，客户端DNS程序再继续向这些DNS服务器进行查询，直到得到查询结果为止。也就是说，迭代解析只是帮你找到相关的服务器而已，而不会帮你去查。比如说：baidu.com的服务器ip地址在192.168.4.5这里，你自己去查吧，本人比较忙，只能帮你到这里了。 DNS负载均衡当一个网站有足够多的用户的时候，假如每次请求的资源都位于同一台机器上面，那么这台机器随时可能会蹦掉。处理办法就是用DNS负载均衡技术，它的原理是在DNS服务器中为同一个主机名配置多个IP地址,在应答DNS查询时,DNS服务器对每个查询将以DNS文件中主机记录的IP地址按顺序返回不同的解析结果,将客户端的访问引导到不同的机器上去,使得不同的客户端访问不同的服务器,从而达到负载均衡的目的｡例如可以根据每台机器的负载量，该机器离用户地理位置的距离等等。 为什么域名解析用UDP协议？因为UDP快啊！UDP的DNS协议只要一个请求、一个应答就好了。而使用基于TCP的DNS协议要三次握手、发送数据以及应答、四次挥手。但是UDP协议传输内容不能超过512字节。不过客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。 为什么区域传送用TCP协议？因为TCP协议可靠性好啊！你要从主DNS上复制内容啊，你用不可靠的UDP？ 因为TCP协议传输的内容大啊，你用最大只能传512字节的UDP协议？万一同步的数据大于512字节，你怎么办？ HTTP请求和相应报文请求报文简单来说： 请求行：Request Line 请求头：Request Headers 请求体：Request Body 响应报文简单来说： 状态行：Status Line 响应头：Response Headers 响应体：Response Body HTTP请求方法GET 获取资源 当前网络请求中，绝大部分使用的是 GET 方法。 HEAD 获取报文头部 和 GET 方法类似，但是不返回报文实体主体部分。 主要用于确认 URL 的有效性以及资源更新的日期时间等。 POST 传输实体主体 POST 主要用来传输数据，而 GET 主要用来获取资源。 更多 POST 与 GET 的比较见后 PUT 上传文件 由于自身不带验证机制，任何人都可以上传文件，因此存在安全性问题，一般不使用该方法。 PATCH 对资源进行部分修改 PUT 也可以用于修改资源，但是只能完全替代原始资源，PATCH 允许部分修改。 DELETE 删除文件 与 PUT 功能相反，并且同样不带验证机制。 OPTINONS 查询支持的方法 查询指定的 URL 能够支持的方法。 会返回 Allow: GET, POST, HEAD, OPTIONS 这样的内容。 CONNECT 要求在与代理服务器通信时建立隧道 使用 SSL（Secure Sockets Layer，安全套接层）和 TLS（Transport Layer Security，传输层安全）协议把通信内容加密后经网络隧道传输。 GET和POST的区别？ GET使用URL或Cookie传参，而POST将数据放在BODY中 GET方式提交的数据有长度限制，则POST的数据则可以非常大 POST比GET安全，因为数据在地址栏上不可见，没毛病 本质区别：GET请求是幂等性的，POST请求不是。 这里的幂等性：幂等性是指一次和多次请求某一个资源应该具有同样的副作用。简单来说意味着对同一URL的多个请求应该返回同样的结果。 正因为它们有这样的区别，所以不应该且不能用get请求做数据的增删改这些有副作用的操作。因为get请求是幂等的，在网络不好的隧道中会尝试重试。如果用get请求增数据，会有重复操作的风险，而这种重复操作可能会导致副作用（浏览器和操作系统并不知道你会用get请求去做增操作）。 HTTP状态码 状态码 类别 含义 1XX Informational（信息性状态码） 接收的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出 1xx 信息100 Continue ：表明到目前为止都很正常，客户端可以继续发送请求或者忽略这个响应。 2xx 成功 200 OK 204 No Content ：请求已经成功处理，但是返回的响应报文不包含实体的主体部分。一般在只需要从客户端往服务器发送信息，而不需要返回数据时使用。 206 Partial Content ：表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。 3xx 重定向 301 Moved Permanently ：永久性重定向 302 Found ：临时性重定向 303 See Other ：和 302 有着相同的功能，但是 303 明确要求客户端应该采用 GET 方法获取资源。 304 Not Modified ：如果请求报文首部包含一些条件，例如：If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since，如果不满足条件，则服务器会返回 304 状态码。 307 Temporary Redirect ：临时重定向，与 302 的含义类似，但是 307 要求浏览器不会把重定向请求的 POST 方法改成 GET 方法。 4xx 客户端错误 400 Bad Request ：请求报文中存在语法错误。 401 Unauthorized ：该状态码表示发送的请求需要有认证信息（BASIC 认证、DIGEST 认证）。如果之前已进行过一次请求，则表示用户认证失败。 403 Forbidden ：请求被拒绝。 404 Not Found 5xx 服务器错误 500 Internal Server Error ：服务器正在执行请求时发生错误。 503 Service Unavailable ：服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。 CookiesHTTP 协议是无状态的，主要是为了让 HTTP 协议尽可能简单，使得它能够处理大量事务。HTTP/1.1 引入 Cookie 来保存状态信息。 Cookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器之后向同一服务器再次发起请求时被携带上，用于告知服务端两个请求是否来自同一浏览器。由于之后每次请求都会需要携带 Cookie 数据，因此会带来额外的性能开销（尤其是在移动环境下）。 Cookie 曾一度用于客户端数据的存储，因为当时并没有其它合适的存储办法而作为唯一的存储手段，但现在随着现代浏览器开始支持各种各样的存储方式，Cookie 渐渐被淘汰。新的浏览器 API 已经允许开发者直接将数据存储到本地，如使用 Web storage API（本地存储和会话存储）或 IndexedDB。 用途 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） Session除了可以将用户信息通过 Cookie 存储在用户浏览器中，也可以利用 Session 存储在服务器端，存储在服务器端的信息更加安全。 Session 可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中，效率会更高。 使用 Session 维护用户登录状态的过程如下： 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中； 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID； 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中； 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。 注意：Session ID 的安全性问题，不能让它被恶意攻击者轻易获取，那么就不能产生一个容易被猜到的 Session ID 值。此外，还需要经常重新生成 Session ID。在对安全性要求极高的场景下，例如转账等操作，除了使用 Session 管理用户状态之外，还需要对用户进行重新验证，比如重新输入密码，或者使用短信验证码等方式。 Cookie和Session的选择 Cookie 只能存储 ASCII 码字符串，而 Session 则可以存储任何类型的数据，因此在考虑数据复杂性时首选 Session； Cookie 存储在浏览器中，容易被恶意查看。如果非要将一些隐私数据存在 Cookie 中，可以将 Cookie 值进行加密，然后在服务器进行解密； 对于大型网站，如果用户所有的信息都存储在 Session 中，那么开销是非常大的，因此不建议将所有的用户信息都存储到 Session 中。 JWTJWT(json web token)是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准。 cookie+session这种模式通常是保存在内存中，而且服务从单服务到多服务会面临的session共享问题，随着用户量的增多，开销就会越大。而JWT不是这样的，只需要服务端生成token，客户端保存这个token，每次请求携带这个token，服务端认证解析就可。 JWT的构成： 第一部分我们称它为头部（header),第二部分我们称其为载荷（payload)，第三部分是签证（signature)。详情请见官网 JWT总结： 因为json的通用性，所以JWT是可以进行跨语言支持的，像JAVA,JavaScript,NodeJS,PHP等很多语言都可以使用。 payload部分，JWT可以在自身存储一些其他业务逻辑所必要的非敏感信息。 便于传输，jwt的构成非常简单，字节占用很小，所以它是非常便于传输的。它不需要在服务端保存会话信息, 所以它易于应用的扩展。 浏览器在与服务器建立了一个 TCP 连接后是否会在一个 HTTP 请求完成后断开？什么情况下会断开？在 HTTP/1.0 中，一个服务器在发送完一个 HTTP 响应后，会断开 TCP 链接。但是这样每次请求都会重新建立和断开 TCP 连接，代价过大。所以虽然标准中没有设定，某些服务器对 Connection: keep-alive 的 Header 进行了支持。意思是说，完成这个 HTTP 请求之后，不要断开 HTTP 请求使用的 TCP 连接。这样的好处是连接可以被重新使用，之后发送 HTTP 请求的时候不需要重新建立 TCP 连接，以及如果维持连接，那么 SSL 的开销也可以避免。 持久连接：既然维持 TCP 连接好处这么多，HTTP/1.1 就把 Connection 头写进标准，并且默认开启持久连接，除非请求中写明 Connection: close，那么浏览器和服务器之间是会维持一段时间的 TCP 连接，不会一个请求结束就断掉。 默认情况下建立 TCP 连接不会断开，只有在请求报头中声明 Connection: close 才会在请求完成后关闭连接。 一个TCP连接可以对应几个HTTP请求？如果维持连接，一个 TCP 连接是可以发送多个 HTTP 请求的。 一个 TCP 连接中 HTTP 请求发送可以一起发送么（比如一起发三个请求，再三个响应一起接收）？HTTP/1.1 存在一个问题，单个 TCP 连接在同一时刻只能处理一个请求，意思是说：两个请求的生命周期不能重叠，任意两个 HTTP 请求从开始到结束的时间在同一个 TCP 连接里不能重叠。 在 HTTP/1.1 存在 Pipelining 技术可以完成这个多个请求同时发送，但是由于浏览器默认关闭，所以可以认为这是不可行的。在 HTTP2 中由于 Multiplexing 特点的存在，多个 HTTP 请求可以在同一个 TCP 连接中并行进行。 那么在 HTTP/1.1 时代，浏览器是如何提高页面加载效率的呢？主要有下面两点： 维持和服务器已经建立的 TCP 连接，在同一连接上顺序处理多个请求。 和服务器建立多个 TCP 连接。 为什么有的时候刷新页面不需要重新建立 SSL 连接？TCP 连接有的时候会被浏览器和服务端维持一段时间。TCP 不需要重新建立，SSL 自然也会用之前的。 浏览器对同一 Host 建立 TCP 连接到数量有没有限制？假设我们还处在 HTTP/1.1 时代，那个时候没有多路传输，当浏览器拿到一个有几十张图片的网页该怎么办呢？肯定不能只开一个 TCP 连接顺序下载，那样用户肯定等的很难受，但是如果每个图片都开一个 TCP 连接发 HTTP 请求，那电脑或者服务器都可能受不了，要是有 1000 张图片的话总不能开 1000 个TCP 连接吧，你的电脑同意 NAT 也不一定会同意。 有。Chrome 最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别。 如果图片都是 HTTPS 连接并且在同一个域名下，那么浏览器在 SSL 握手之后会和服务器商量能不能用 HTTP2，如果能的话就使用 Multiplexing 功能在这个连接上进行多路传输。不过也未必会所有挂在这个域名的资源都会使用一个 TCP 连接去获取，但是可以确定的是 Multiplexing 很可能会被用到。 如果发现用不了 HTTP2 呢？或者用不了 HTTPS（现实中的 HTTP2 都是在 HTTPS 上实现的，所以也就是只能使用 HTTP/1.1）。那浏览器就会在一个 HOST 上建立多个 TCP 连接，连接数量的最大限制取决于浏览器设置，这些连接会在空闲的时候被浏览器用来发送新的请求，如果所有的连接都正在发送请求呢？那其他的请求就只能等等了。 在浏览器中输入url地址后显示主页的过程? 根据域名，进行DNS域名解析； 拿到解析的IP地址，建立TCP连接； 向IP地址，发送HTTP请求； 服务器处理请求； 返回响应结果； 关闭TCP连接； 浏览器解析HTML； 浏览器布局渲染； HTTPSHTTPS是什么HTTPS 并不是新协议，而是让 HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信，也就是说 HTTPS 使用了隧道进行通信。通过使用 SSL，HTTPS 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）。 HTTP的缺点 使用明文进行通信，内容可能会被窃听； 不验证通信方的身份，通信方的身份有可能遭遇伪装； 无法证明报文的完整性，报文有可能遭篡改。 对称密钥加密对称密钥加密（Symmetric-Key Encryption），加密和解密使用同一密钥。 优点：运算速度快 缺点：无法安全地将密钥传输给通信方 非对称密钥加密非对称密钥加密，又称公开密钥加密（Public-Key Encryption），加密和解密使用不同的密钥。 公开密钥所有人都可以获得，通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密，接收方收到通信内容后使用私有密钥解密。 非对称密钥除了用来加密，还可以用来进行签名。因为私有密钥无法被其他人获取，因此通信发送方使用其私有密钥进行签名，通信接收方使用发送方的公开密钥对签名进行解密，就能判断这个签名是否正确。 优点：可以更安全地将公开密钥传输给通信发送方； 缺点：运算速度慢。 HTTPS采用的加密方式HTTPS 采用混合的加密机制，使用非对称密钥加密用于传输对称密钥来保证传输过程的安全性，之后使用对称密钥加密进行通信来保证通信过程的效率。 确保传输安全过程（其实就是rsa原理）： Client给出协议版本号、一个客户端生成的随机数（Client random），以及客户端支持的加密方法。 Server确认双方使用的加密方法，并给出数字证书、以及一个服务器生成的随机数（Server random）。 Client确认数字证书有效，然后生成呀一个新的随机数（Premaster secret），并使用数字证书中的公钥，加密这个随机数，发给Server。 Server使用自己的私钥，获取Client发来的随机数（Premaster secret）。 Client和Server根据约定的加密方法，使用前面的三个随机数，生成”对话密钥”（session key），用来加密接下来的整个对话过程。 认证通过使用 证书 来对通信方进行认证。 数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构。 服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。 进行 HTTPS 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。 HTTP的缺点 因为需要进行加密解密等过程，因此速度会更慢； 需要支付证书授权的高额费用。 TCP/UDPTCPTCP是什么？TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 TCP头部报文source port 和 destination port 两者分别为「源端口号」和「目的端口号」。源端口号就是指本地端口，目的端口就是远程端口。 可以这么理解，我们有很多软件，每个软件都对应一个端口，假如，你想和我数据交互，咱们得互相知道你我的端口号。 再来一个很官方的： 扩展：应用程序的端口号和应用程序所在主机的 IP 地址统称为 socket（套接字），IP:端口号, 在互联网上 socket 唯一标识每一个应用程序，源端口+源IP+目的端口+目的IP称为”套接字对“，一对套接字就是一个连接，一个客户端与服务器之间的连接。 Sequence Number 称为「序列号」。用于 TCP 通信过程中某一传输方向上字节流的每个字节的编号，为了确保数据通信的有序性，避免网络中乱序的问题。接收端根据这个编号进行确认，保证分割的数据段在原始数据包的位置。初始序列号由自己定，而后绪的序列号由对端的 ACK 决定：SN_x = ACK_y (x 的序列号 = y 发给 x 的 ACK)。 说白了，类似于身份证一样，而且还得发送此时此刻的所在的位置，就相当于身份证上的地址一样。 Acknowledge Number 称为「确认序列号」。确认序列号是接收确认端所期望收到的下一序列号。确认序号应当是上次已成功收到数据字节序号加1，只有当标志位中的 ACK 标志为 1 时该确认序列号的字段才有效。主要用来解决不丢包的问题。 TCP FlagTCP 首部中有 6 个标志比特，它们中的多个可同时被设置为 1，主要是用于操控 TCP 的状态机的，依次为URG，ACK，PSH，RST，SYN，FIN。 当然只介绍三个： ACK：这个标识可以理解为发送端发送数据到接收端，发送的时候 ACK 为 0，标识接收端还未应答，一旦接收端接收数据之后，就将 ACK 置为 1，发送端接收到之后，就知道了接收端已经接收了数据。 SYN：表示「同步序列号」，是 TCP 握手的发送的第一个数据包。用来建立 TCP 的连接。SYN 标志位和 ACK 标志位搭配使用，当连接请求的时候，SYN=1，ACK=0连接被响应的时候，SYN=1，ACK=1；这个标志的数据包经常被用来进行端口扫描。扫描者发送一个只有 SYN 的数据包，如果对方主机响应了一个数据包回来 ，就表明这台主机存在这个端口。 FIN：表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送FIN标志位的 TCP 数据包后，连接将被断开。这个标志的数据包也经常被用于进行端口扫描。发送端只剩最后的一段数据了，同时要告诉接收端后边没有数据可以接受了，所以用FIN标识一下，接收端看到这个FIN之后，哦！这是接受的最后的数据，接受完就关闭了；TCP四次分手必然问。 Window size 称为滑动窗口大小。所说的滑动窗口，用来进行流量控制。 TCP三次握手 初始状态：客户端处于 closed(关闭)状态，服务器处于 listen(监听) 状态。 第一次握手：客户端发送请求报文将 SYN = 1同步序列号和初始化序列号seq = x发送给服务端，发送完之后客户端处于SYN_Send状态。（验证了客户端的发送能力和服务端的接收能力） 第二次握手：服务端受到 SYN 请求报文之后，如果同意连接，会以自己的同步序列号SYN(服务端) = 1、初始化序列号 seq = y和确认序列号（期望下次收到的数据包）ack = x+ 1 以及确认号ACK = 1报文作为应答，服务器为SYN_Receive状态。（问题来了，两次握手之后，站在客户端角度上思考：我发送和接收都ok，服务端的发送和接收也都ok。但是站在服务端的角度思考：哎呀，我服务端接收ok，但是我不清楚我的发送ok不ok呀，而且我还不知道你接受能力如何呢？所以老哥，你需要给我三次握手来传个话告诉我一声。你要是不告诉我，万一我认为你跑了，然后我可能出于安全性的考虑继续给你发一次，看看你回不回我。） 第三次握手： 客户端接收到服务端的 SYN + ACK之后，知道可以下次可以发送了下一序列的数据包了，然后发送同步序列号 ack = y + 1和数据包的序列号 seq = x + 1以及确认号ACK = 1确认包作为应答，客户端转为established状态。（分别站在双方的角度上思考，各自ok） TCP四次分手 初始化状态：客户端和服务端都在连接状态，接下来开始进行四次分手断开连接操作。 第一次分手：第一次分手无论是客户端还是服务端都可以发起，因为 TCP 是全双工的。 假如客户端发送的数据已经发送完毕，发送FIN = 1 告诉服务端，客户端所有数据已经全发完了，服务端你可以关闭接收了，但是如果你们服务端有数据要发给客户端，客户端照样可以接收的。此时客户端处于FIN = 1等待服务端确认释放连接状态。 第二次分手：服务端接收到客户端的释放请求连接之后，知道客户端没有数据要发给自己了，然后服务端发送ACK = 1告诉客户端收到你发给我的信息，此时服务端处于 CLOSE_WAIT 等待关闭状态。（服务端先回应给客户端一声，我知道了，但服务端的发送数据能力即将等待关闭，于是接下来第三次就来了。） 第三次分手：此时服务端向客户端把所有的数据发送完了，然后发送一个FIN = 1，用于告诉客户端，服务端的所有数据发送完毕，客户端你也可以关闭接收数据连接了。此时服务端状态处于LAST_ACK状态，来等待确认客户端是否收到了自己的请求。（服务端等客户端回复是否收到呢，不收到的话，服务端不知道客户端是不是挂掉了还是咋回事呢，所以服务端不敢关闭自己的接收能力，于是第四次就来了。） 第四次分手：此时如果客户端收到了服务端发送完的信息之后，就发送ACK = 1，告诉服务端，客户端已经收到了你的信息。有一个 2 MSL 的延迟等待。 为什么要有2MSL等待延迟？对应这样一种情况，最后客户端发送的ACK = 1给服务端的过程中丢失了，服务端没收到，服务端怎么认为的？我已经发送完数据了，怎么客户端没回应我？是不是中途丢失了？然后服务端再次发起断开连接的请求，一个来回就是2MSL。 客户端给服务端发送的ACK = 1丢失，服务端等待 1MSL没收到，然后重新发送消息需要1MSL。如果再次接收到服务端的消息，则重启2MSL计时器，发送确认请求。客户端只需等待2MSL，如果没有再次收到服务端的消息，就说明服务端已经接收到自己确认消息；此时双方都关闭的连接，TCP 四次分手完毕 为什么四次分手？任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。 TCP粘包TCP粘包是指发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。 由TCP连接复用造成的粘包问题。 因为TCP默认会使用Nagle算法，此算法会导致粘包问题。 只有上一个分组得到确认，才会发送下一个分组； 收集多个小分组，在一个确认到来时一起发送。 数据包过大造成的粘包问题。 流量控制，拥塞控制也可能导致粘包。 接收方不及时接收缓冲区的包，造成多个包接收 解决： Nagle算法问题导致的，需要结合应用场景适当关闭该算法 尾部标记序列。通过特殊标识符表示数据包的边界，例如\\n\\r，\\t，或者一些隐藏字符。 头部标记分步接收。在TCP报文的头部加上表示数据长度。 应用层发送数据时定长发送。 TCP 协议如何保证可靠传输？ 确认和重传：接收方收到报文就会确认，发送方发送一段时间后没有收到确认就会重传。 数据校验：TCP报文头有校验和，用于校验报文是否损坏。 数据合理分片和排序：tcp会按最大传输单元(MTU)合理分片，接收方会缓存未按序到达的数据，重新排序后交给应用层。而UDP：IP数据报大于1500字节，大于MTU。这个时候发送方的IP层就需要分片，把数据报分成若干片，是的每一片都小于MTU。而接收方IP层则需要进行数据报的重组。由于UDP的特性，某一片数据丢失时，接收方便无法重组数据报，导致丢弃整个UDP数据报。 流量控制：当接收方来不及处理发送方的数据，能通过滑动窗口，提示发送方降低发送的速率，防止包丢失。 拥塞控制：当网络拥塞时，通过拥塞窗口，减少数据的发送，防止包丢失。 TCP 利用滑动窗口实现流量控制的机制？ 流量控制是为了控制发送方发送速率，保证接收方来得及接收。TCP 利用滑动窗口实现流量控制。 TCP 中采用滑动窗口来进行传输控制，滑动窗口的大小意味着接收方还有多大的缓冲区可以用于接收数据。发送方可以通过滑动窗口的大小来确定应该发送多少字节的数据。当滑动窗口为 0 时，发送方一般不能再发送数据报，但有两种情况除外，一种情况是可以发送紧急数据。 例如，允许用户终止在远端机上的运行进程。另一种情况是发送方可以发送一个 1 字节的数据报来通知接收方重新声明它希望接收的下一字节及发送方的滑动窗口大小。 TCP拥塞控制的机制以及算法？ 在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。 TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。TCP的拥塞控制采用了四种算法，即 慢开始 、 拥塞避免 、快重传 和 快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。 UDP提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。 UDP的特点 UDP是无连接的； UDP使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）； UDP是面向报文的； UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）； UDP支持一对一、一对多、多对一和多对多的交互通信； UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短。 那么，再说一次TCP的特点： TCP是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）； 每一条TCP连接只能有两个端点，每一条TCP连接只能是点对点的（一对一）； TCP提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达； TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据； 面向字节流。TCP中的“流”（stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://dreamcat.ink/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"个人吐血系列-总结RocketMQ","slug":"个人吐血系列-总结RocketMQ","date":"2020-04-01T04:42:56.000Z","updated":"2021-01-01T09:54:57.903Z","comments":true,"path":"2020/04/01/ge-ren-tu-xie-xi-lie-zong-jie-rocketmq/","link":"","permalink":"http://dreamcat.ink/2020/04/01/ge-ren-tu-xie-xi-lie-zong-jie-rocketmq/","excerpt":"","text":"一般面试问消息队列，都是结合自己的项目进行回答的…最好有个项目有消息队列的中间件.本项目使用了RocketMQ 什么是消息队列？消息队列的主要作用是什么？我们可以把消息队列比作是一个存放消息的容器，当我们需要使用消息的时候可以取出消息供自己使用。消息队列是分布式系统中重要的组件，使用消息队列主要是为了通过异步处理提高系统性能和削峰、降低系统耦合性。 异步处理：非核心流程异步化，提高系统响应性能 应用解耦： 系统不是强耦合，消息接受者可以随意增加，而不需要修改消息发送者的代码。消息发送者的成功不依赖消息接受者（比如有些银行接口不稳定，但调用方并不需要依赖这些接口） 消息发送者的成功不依赖消息接受者（比如有些银行接口不稳定，但调用方并不需要依赖这些接口） 最终一致性：最终一致性不是消息队列的必备特性，但确实可以依靠消息队列来做最终一致性的事情。 先写消息再操作，确保操作完成后再修改消息状态。定时任务补偿机制实现消息可靠发送接收、业务操作的可靠执行，要注意消息重复与幂等设计 所有不保证100%不丢消息的消息队列，理论上无法实现最终一致性。 广播：只需要关心消息是否送达了队列，至于谁希望订阅，是下游的事情 流量削峰与监控：当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的“漏斗”。在下游有能力处理的时候，再进行分发。 日志处理：将消息队列用在日志处理中，比如Kafka的应用，解决大量日志传输的问题 消息通讯：消息队列一般都内置了高效的通信机制，因此也可以用于单纯的消息通讯，如实现点对点消息队列或者聊天室等。 推荐浅显易懂的讲解： 《吊打面试官》系列-消息队列基础 面试官问你什么是消息队列？把这篇甩给他！ kafka、activemq、rabbitmq、rocketmq都有什么区别？ ActiveMQ 的社区算是比较成熟，但是较目前来说，ActiveMQ 的性能比较差，而且版本迭代很慢，不推荐使用。 RabbitMQ 在吞吐量方面虽然稍逊于 Kafka 和 RocketMQ ，但是由于它基于 erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为 RabbitMQ 基于 erlang 开发，所以国内很少有公司有实力做erlang源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这四种消息队列中，RabbitMQ 一定是你的首选。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ 阿里出品，Java 系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的MQ，并且 RocketMQ 有阿里巴巴的实际业务场景的实战考验。RocketMQ 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准 JMS 规范走的有些系统要迁移需要修改大量代码。还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用RocketMQ 挺好的 kafka 的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。 MQ在高并发情况下，假设队列满了如何防止消息丢失？ 生产者可以采用重试机制。因为消费者会不停的消费消息，可以重试将消息放入队列。 死信队列，可以理解为备胎(推荐) 即在消息过期，队列满了，消息被拒绝的时候，都可以扔给死信队列。 如果出现死信队列和普通队列都满的情况，此时考虑消费者消费能力不足，可以对消费者开多线程进行处理。 谈谈死信队列死信队列用于处理无法被正常消费的消息，即死信消息。 当一条消息初次消费失败，消息队列 RocketMQ 版会自动进行消息重试；达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时，消息队列 RocketMQ 版不会立刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中，该特殊队列称为死信队列。 死信消息的特点： 不会再被消费者正常消费。 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。因此，请在死信消息产生后的 3 天内及时处理。 死信队列的特点： 一个死信队列对应一个 Group ID， 而不是对应单个消费者实例。 如果一个 Group ID 未产生死信消息，消息队列 RocketMQ 版不会为其创建相应的死信队列。 一个死信队列包含了对应 Group ID 产生的所有死信消息，不论该消息属于哪个 Topic。 消息队列 RocketMQ 版控制台提供对死信消息的查询、导出和重发的功能。 消费者消费消息，如何保证MQ幂等性？幂等性消费者在消费mq中的消息时，mq已把消息发送给消费者，消费者在给mq返回ack时网络中断，故mq未收到确认信息，该条消息会重新发给其他的消费者，或者在网络重连后再次发送给该消费者，但实际上该消费者已成功消费了该条消息，造成消费者消费了重复的消息； 解决方案 MQ消费者的幂等行的解决一般使用全局ID 或者写个唯一标识比如时间戳 或者UUID 或者订单 也可利用mq的该id来判断，或者可按自己的规则生成一个全局唯一id，每次消费消息时用该id先判断该消息是否已消费过。 给消息分配一个全局id，只要消费过该消息，将 &lt; id,message&gt;以K-V形式写入redis。那消费者开始消费前，先去redis中查询有没消费记录即可。 使用异步消息时如何保证数据的一致性 借助数据库的事务：使用异步消息怎么还能借助到数据库事务？这需要在数据库中创建一个本地消息表，这样可以通过一个事务来控制本地业务逻辑更新和本地消息表的写入在同一个事务中，一旦消息落库失败，则直接全部回滚。如果消息落库成功，后续就可以根据情况基于本地数据库中的消息数据对消息进行重投了。关于本地消息表和消息队列中状态如何保持一致，可以采用 2PC 的方式。在发消息之前落库，然后发消息，在得到同步结果或者消息回调的时候更新本地数据库表中消息状态。然后只需要通过定时轮询的方式对状态未已记录但是未发送的消息重新投递就行了。但是这种方案有个前提，就是要求消息的消费者做好幂等控制，这个其实异步消息的消费者一般都需要考虑的。 除了使用数据库以外，还可以使用 Redis 等缓存。这样就是无法利用关系型数据库自带的事务回滚了。 RockMQ不适用Zookeeper作为注册中心的原因，以及自制的NameServer优缺点？ ZooKeeper 作为支持顺序一致性的中间件，在某些情况下，它为了满足一致性，会丢失一定时间内的可用性，RocketMQ 需要注册中心只是为了发现组件地址，在某些情况下，RocketMQ 的注册中心可以出现数据不一致性，这同时也是 NameServer 的缺点，因为 NameServer 集群间互不通信，它们之间的注册信息可能会不一致。 另外，当有新的服务器加入时，NameServer 并不会立马通知到 Produer，而是由 Produer 定时去请求 NameServer 获取最新的 Broker/Consumer 信息（这种情况是通过 Producer 发送消息时，负载均衡解决） 包括组件通信间使用 Netty 的自定义协议 消息重试负载均衡策略（具体参考 Dubbo 负载均衡策略） 消息过滤器（Producer 发送消息到 Broker，Broker 存储消息信息，Consumer 消费时请求 Broker 端从磁盘文件查询消息文件时,在 Broker 端就使用过滤服务器进行过滤） Broker 同步双写和异步双写中 Master 和 Slave 的交互","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://dreamcat.ink/tags/MQ/"}]},{"title":"个人吐血系列-总结Dubbo","slug":"个人吐血系列-总结Dubbo","date":"2020-03-31T16:04:46.000Z","updated":"2021-01-01T09:55:42.986Z","comments":true,"path":"2020/04/01/ge-ren-tu-xie-xi-lie-zong-jie-dubbo/","link":"","permalink":"http://dreamcat.ink/2020/04/01/ge-ren-tu-xie-xi-lie-zong-jie-dubbo/","excerpt":"","text":"微服务和分布式算是一种潮流和趋势，项目中要到了微服务还是准备准备面试的问题吧… 大纲图 Dubbo和SpringCloud的区别 底层：Dubbo底层是使用Netty的NIO框架，基于TCP协议传输，使用Hession序列化完成RPC通信；SpringCloud是基于HTTP协议+REST接口调用远程过程的通信，HTTP请求会有更大的报文，占的带宽也会更多。但是REST相比RPC更为灵活，不存在代码级别的强依赖。 集成：springcloud相关组件多，有自己得注册中心网关等，集成方便，Dubbo需要自己额外去集成。 定位：Dubbo是SOA时代的产物，它的关注点主要在于服务的调用，流量分发、流量监控和熔断。而SpringCloud诞生于微服务架构时代，考虑的是微服务治理的方方面面，另外由于依托了Spirng、SpirngBoot 的优势之上，两个框架在开始目标就不一致，Dubbo定位服务治理、SpirngCloud是一个生态。因此可以大胆地判断，Dubbo未来会在服务治理方面更为出色，而Spring Cloud在微服务治理上面无人能敌。 什么是Dubbo？Dubbo是一款高性能、轻量级的开源Java RPC 框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。简单来说 Dubbo 是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。 什么是RPC？原理是什么？RPCRPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如两个不同的服务 A、B 部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法该怎么办呢？使用 HTTP请求 当然可以，但是可能会比较慢而且一些优化做的并不好。 RPC 的出现就是为了解决这个问题。 RPC原理 服务消费方（client）调用以本地调用方式调用服务； client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； client stub找到服务地址，并将消息发送到服务端； server stub收到消息后进行解码； server stub根据解码结果调用本地的服务； 本地服务执行并将结果返回给server stub； server stub将返回结果打包成消息并发送至消费方； client stub接收到消息，并进行解码； 服务消费方得到最终结果。 为什么要用Dubbo?Dubbo 的诞生和 SOA 分布式架构的流行有着莫大的关系。SOA 面向服务的架构（Service Oriented Architecture），也就是把工程按照业务逻辑拆分成服务层、表现层两个工程。服务层中包含业务逻辑，只需要对外提供服务即可。表现层只需要处理和页面的交互，业务逻辑都是调用服务层的服务来实现。SOA架构中有两个主要角色：服务提供者（Provider）和服务使用者（Consumer）。 我觉得主要可以从 Dubbo 提供的下面四点特性来说为什么要用 Dubbo： 负载均衡——同一个服务部署在不同的机器时该调用那一台机器上的服务。 服务调用链路生成——随着系统的发展，服务越来越多，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。Dubbo 可以为我们解决服务之间互相是如何调用的。 服务访问压力以及时长统计、资源调度和治理——基于访问压力实时管理集群容量，提高集群利用率。 服务降级——某个服务挂掉之后调用备用服务。 另外，Dubbo 除了能够应用在分布式系统中，也可以应用在现在比较火的微服务系统中。不过，由于 Spring Cloud 在微服务中应用更加广泛，所以，我觉得一般我们提 Dubbo 的话，大部分是分布式系统的情况。 什么是分布式？分布式或者说 SOA 分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。比如电商系统可以简单地拆分成订单系统、商品系统、登录系统等等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。 为什么要分布式？ 从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以，每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展。 系统拆分成分布式之后不光便于系统扩展和维护，更能提高整个系统的性能。把整个系统拆分成不同的服务/系统，然后每个服务/系统 单独部署在一台服务器上，是不是很大程度上提高了系统性能呢？ Dubbo的架构图解 Provider： 暴露服务的服务提供方 Consumer： 调用远程服务的服务消费方 Registry： 服务注册与发现的注册中心 Monitor： 统计服务的调用次数和调用时间的监控中心 Container： 服务运行容器 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 各个组件总结： 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 Dubbo工作原理 图中从下至上分为十层，各层均为单向依赖，右边的黑色箭头代表层之间的依赖关系，每一层都可以剥离上层被复用，其中，Service 和 Config 层为 API，其它各层均为 SPI。 第一层：service层，接口层，给服务提供者和消费者来实现的 第二层：config层，配置层，主要是对dubbo进行各种配置的 第三层：proxy层，服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton 第四层：registry层，服务注册层，负责服务的注册与发现 第五层：cluster层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务 第六层：monitor层，监控层，对rpc接口的调用次数和调用时间进行监控 第七层：protocol层，远程调用层，封装rpc调用 第八层：exchange层，信息交换层，封装请求响应模式，同步转异步 第九层：transport层，网络传输层，抽象mina和netty为统一接口 第十层：serialize层，数据序列化层，网络传输需要 Dubbo的负载均衡什么是负载均衡 负载均衡改善了跨多个计算资源（例如计算机，计算机集群，网络链接，中央处理单元或磁盘驱动的的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间，并避免任何单个资源的过载。使用具有负载平衡而不是单个组件的多个组件可以通过冗余提高可靠性和可用性。负载平衡通常涉及专用软件或硬件。—— 够强硬 比如我们的系统中的某个服务的访问量特别大，我们将这个服务部署在了多台服务器上，当客户端发起请求的时候，多台服务器都可以处理这个请求。那么，如何正确选择处理该请求的服务器就很关键。假如，你就要一台服务器来处理该服务的请求，那该服务部署在多台服务器的意义就不复存在了。负载均衡就是为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题，我们从负载均衡的这四个字就能明显感受到它的意义。 Dubbo的负载均衡在集群负载均衡时，Dubbo 提供了多种均衡策略，默认为 random 随机调用。可以自行扩展负载均衡策略 Random LoadBalance(默认，基于权重的随机负载均衡机制) 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance(不推荐，基于权重的轮询负载均衡机制) 轮循，按公约后的权重设置轮循比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastAcive LoadBalace 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash LodaBalance 一致性 Hash，相同参数的请求总是发到同一提供者。(如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性hash策略。) 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 Dubbo配置方式xml配置方式服务端服务级别： &lt;dubbo:service interface=\"...\" loadbalance=\"roundrobin\" /> 客户端服务级别： &lt;dubbo:reference interface=\"...\" loadbalance=\"roundrobin\" /> 服务端方法级别： &lt;dubbo:service interface=\"...\"> &lt;dubbo:method name=\"...\" loadbalance=\"roundrobin\"/> &lt;/dubbo:service> 客户端方法级别： &lt;dubbo:reference interface=\"...\"> &lt;dubbo:method name=\"...\" loadbalance=\"roundrobin\"/> &lt;/dubbo:reference> 注解配置方式：服务级别配置： @Service .... 消费注解配置： @Reference(loadbalance = \"roundrobin\") zookeeper宕机与dubbo直连的情况在实际生产中，假如zookeeper注册中心宕掉，一段时间内服务消费方还是能够调用提供方的服务的，实际上它使用的本地缓存进行通讯，这只是dubbo健壮性的一种体现。 dubbo的健壮性表现： 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 我们前面提到过：注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。所以，我们可以完全可以绕过注册中心——采用 dubbo 直连 ，即在服务消费方配置服务提供方的位置信息。 创作不易哇，觉得有帮助的话，给个小小的star呗。https://github.com/DreamCats/JavaBooks😁😁😁开源的微服务班车预约系统","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://dreamcat.ink/tags/dubbo/"}]},{"title":"个人吐血系列-总结Redis","slug":"个人吐血系列-总结Redis","date":"2020-03-31T13:59:25.000Z","updated":"2021-01-01T09:54:34.848Z","comments":true,"path":"2020/03/31/ge-ren-tu-xie-xi-lie-zong-jie-redis/","link":"","permalink":"http://dreamcat.ink/2020/03/31/ge-ren-tu-xie-xi-lie-zong-jie-redis/","excerpt":"","text":"毕竟Redis还是挺热门的缓存中间件，得好好学习一下子，在高并发的场景当中，也用的比较多，面试也是常问的点。 大纲图 Redis是什么简单来说redis就是一个数据库，不过与传统数据库不同的是redis的数据库是存在内存中，所以读写速度非常快，因此redis被广泛应用于缓存方向。另外，redis也经常用来做分布式锁，redis提供了多种数据类型来支持不同的业务场景。除此之外，redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。 为什么要用Redis 高性能：假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发：直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 使用Redis有哪些好处 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) 支持丰富数据类型，支持string，list，set，sorted set，hash 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 等等… 为什么要用Redis而不用map/guava做缓存缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓 存不具有一致性。 使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致 性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。 Redis相比Memcached有哪些优势 memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型 redis的速度比memcached快很多 redis可以持久化其数据 Redis的线程模型redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，根据 socket 上的事件来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将 socket 产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。 Redis常见性能问题和解决方案 Master最好不要做任何持久化工作，如RDB内存快照和AOF日志文件 如果数据比较重要，某个Slave开启AOF备份数据，策略设置为每秒同步一次 为了主从复制的速度和连接的稳定性，Master和Slave最好在同一个局域网内 尽量避免在压力很大的主库上增加从库 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master &lt;- Slave1 &lt;- Slave2 &lt;- Slave3… Redis常见数据结构以及使用场景分析String 常用命令: set,get,decr,incr,mget 等。 String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。 常规key-value缓存应用； 常规计数：微博数，粉丝数等。 Hash 常用命令： hget,hset,hgetall 等。 Hash 是一个 string 类型的 ﬁeld 和 value 的映射表，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅 仅修改这个对象中的某个字段的值。 比如我们可以Hash数据结构来存储用户信息，商品信息等等。 List 常用命令: lpush,rpush,lpop,rpop,lrange等 list 就是链表，Redis list 的应用场景非常多，也是Redis最重要的数据结构之一，比如微博的关注列表，粉丝列表， 消息列表等功能都可以用Redis的 list 结构来实现。 Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。 另外可以通过 lrange 命令，就是从某个元素开始读取多少个元素，可以基于 list 实现分页查询，这个很棒的一个功 能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西（一页一页的往下走），性能高。 Set 常用命令： sadd,spop,smembers,sunion 等 set 对外提供的功能与list类似是一个列表的功能，特殊之处在于 set 是可以自动排重的。 当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在 一个set集合内的重要接口，这个也是list所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。 比如：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常 方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程，具体命令如下：sinterstore key1 key2 key3将交集存在key1内 Sorted Set 常用命令： zadd,zrange,zrem,zcard等 和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。 举例： 在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维 度的消息排行榜）等信息，适合使用 Redis 中的 SortedSet 结构进行存储。 Redis设置过期时间Redis中有个设置时间过期的功能，即对存储在 redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库， 这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统 的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的 时间。 定期删除+惰性删除 定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删 除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所 有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！ 惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这 就是所谓的惰性删除，也是够懒的哈！ 如果定期删除漏掉了很多过期 key，然后你也没及时去查， 也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题 呢？ redis 内存淘汰机制。 Mysql有2000万数据，redis只存20万，如何保证redis中的数据都是热点数据redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。redis 提供 6种数据淘汰策略： voltile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 Memcache与Redis的区别都有哪些 存储方式 Memecache把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。 Redis有部份存在硬盘上，这样能保证数据的持久性。 数据支持类型 Memcache对数据类型支持相对简单。String Redis有复杂的数据类型。Redis不仅仅支持简单的k/v类型的数据，同时还提供 list，set，zset，hash等数据结构的存储。 使用底层模型不同 它们之间底层实现方式 以及与客户端之间通信的应用协议不一样。 Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 集群模式 memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前 是原生支持 cluster 模式的. Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。 Redis持久化机制很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机 器、机器故障之后回复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB）,另一种方式是只追加文件（append-only ﬁle,AOF）.这两种方法各有千 秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方。 快照（snapshotting）持久化（RDB）Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行 备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性 能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令 创建快照。 save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 AOF（append-only file）持久化与快照持久化相比，AOF持久化的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启 AOF（append only ﬁle）方式的持久化，可以通过appendonly参数开启：appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的 保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能 几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操 作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 Redis 4.0 对于持久化机制的优化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 AOF重写AOF重写可以产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一样，但体积更小。 AOF重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有AOF文件进行任伺读 入、分析或者写入操作。 在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新AOF文件期 间，记录服务器执行的所有写命令。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区中的所有内容 追加到新AOF文件的末尾，使得新旧两个AOF文件所保存的数据库状态一致。最后，服务器用新的AOF文件替换旧的 AOF文件，以此来完成AOF文件重写操作。 Redis事务Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务(transaction)功能。事务提供了一种将多个命令请求打包，然 后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令 请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的可靠性和安全性。在 Redis 中，事务总是具有原子性 （Atomicity)、一致性(Consistency)和隔离性（Isolation），并且当 Redis 运行在某种特定的持久化模式下时，事务 也具有持久性（Durability）。 Redis常见的性能问题都有哪些？如何解决？ Master写内存快照，save命令调度rdbSave函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以Master最好不要写内存快照。 Master AOF持久化，如果不重写AOF文件，这个持久化方式对性能的影响是最小的，但是AOF文件会不断增大，AOF文件过大会影响Master重启的恢复速度。Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化,如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。 Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。 Redis主从复制的性能问题，为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内 Redis的同步机制了解吗？主从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 是否使用Redis集群，集群的原理是什么Redis Sentinel着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。 Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。 缓存雪崩和缓存问题解决方案缓存雪崩缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩 掉。 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存 缓存穿透一般是黑客故意去请求缓存中不存在的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量 请求而崩掉。 解决办法： 有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈 希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压 力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数据不存 在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 如何解决Redis的并发竞争Key问题所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺 序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问 题，不要使用分布式锁，这样会影响性能） 基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的 与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有 序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁 无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 在实践中，当然是从以可靠性为主。所以首推Zookeeper。 如何保证缓存与数据库双写时的数据一致性你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如 何解决一致性问题？ 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的 情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致 的情况 串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://dreamcat.ink/tags/redis/"}]},{"title":"个人吐血系列-总结Mysql","slug":"个人吐血系列-总结Mysql","date":"2020-03-30T07:13:08.000Z","updated":"2021-01-01T09:54:15.396Z","comments":true,"path":"2020/03/30/ge-ren-tu-xie-xi-lie-zong-jie-mysql/","link":"","permalink":"http://dreamcat.ink/2020/03/30/ge-ren-tu-xie-xi-lie-zong-jie-mysql/","excerpt":"","text":"MySQL这一块的知识还是挺多的，问深度的话， 一般都是如何调优的，当然少不了MySQL的基础等知识。 数据库引擎innodb与myisam的区别InnoDB是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。 实现了四个标准的隔离级别，默认级别是可重复读(REPEATABLE READ)。在可重复读隔离级别下，通过多版本并发控制(MVCC)+ 间隙锁(Next-Key Locking)防止幻影读。 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。 内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。 支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。 MyISAM设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。 提供了大量的特性，包括压缩表、空间数据索引等。 不支持事务。 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入(CONCURRENT INSERT)。 可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。 如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。 比较 事务: InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。 并发: MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。 外键: InnoDB 支持外键。 备份: InnoDB 支持在线热备份。 崩溃恢复: MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。 其它特性: MyISAM 支持压缩表和空间数据索引。 MySQL是如何执行一条SQL的 MySQL内部可以分为服务层和存储引擎层两部分： 服务层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认的存储引擎。 Server层按顺序执行sql的步骤为： 客户端请求-&gt;连接器（验证用户身份，给予权限） -&gt; 查询缓存（存在缓存则直接返回，不存在则执行后续操作）-&gt;分析器（对SQL进行词法分析和语法分析操作） -&gt; 优化器（主要对执行的sql优化选择最优的执行方案方法） -&gt; 执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口）-&gt;去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果） 简单概括： 连接器：管理连接、权限验证； 查询缓存：命中缓存则直接返回结果； 分析器：对SQL进行词法分析、语法分析；（判断查询的SQL字段是否存在也是在这步） 优化器：执行计划生成、选择索引； 执行器：操作引擎、返回结果； 存储引擎：存储数据、提供读写接口。 mysql的acid原理ACID嘛，原子性(Atomicity)、一致性(Consistency)、隔离性(Isolation)、持久性(Durability)！ 我们以从A账户转账50元到B账户为例进行说明一下ACID，四大特性。 原子性根据定义，原子性是指一个事务是一个不可分割的工作单位，其中的操作要么都做，要么都不做。即要么转账成功，要么转账失败，是不存在中间的状态！ 如果无法保证原子性会怎么样？ OK，就会出现数据不一致的情形，A账户减去50元，而B账户增加50元操作失败。系统将无故丢失50元~ 隔离性根据定义，隔离性是指多个事务并发执行的时候，事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 如果无法保证隔离性会怎么样？ OK，假设A账户有200元，B账户0元。A账户往B账户转账两次，金额为50元，分别在两个事务中执行。如果无法保证隔离性，会出现下面的情形 如图所示，如果不保证隔离性，A扣款两次，而B只加款一次，凭空消失了50元，依然出现了数据不一致的情形！ 持久性根据定义，持久性是指事务一旦提交，它对数据库的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。 如果无法保证持久性会怎么样？ 在MySQL中，为了解决CPU和磁盘速度不一致问题，MySQL是将磁盘上的数据加载到内存，对内存进行操作，然后再回写磁盘。好，假设此时宕机了，在内存中修改的数据全部丢失了，持久性就无法保证。 设想一下，系统提示你转账成功。但是你发现金额没有发生任何改变，此时数据出现了不合法的数据状态，我们将这种状态认为是数据不一致的情形。 一致性根据定义，一致性是指事务执行前后，数据处于一种合法的状态，这种状态是语义上的而不是语法上的。 那什么是合法的数据状态呢？ oK，这个状态是满足预定的约束就叫做合法的状态，再通俗一点，这状态是由你自己来定义的。满足这个状态，数据就是一致的，不满足这个状态，数据就是不一致的！ 如果无法保证一致性会怎么样？ 例一:A账户有200元，转账300元出去，此时A账户余额为-100元。你自然就发现了此时数据是不一致的，为什么呢？因为你定义了一个状态，余额这列必须大于0。 例二:A账户200元，转账50元给B账户，A账户的钱扣了，但是B账户因为各种意外，余额并没有增加。你也知道此时数据是不一致的，为什么呢？因为你定义了一个状态，要求A+B的余额必须不变。 mysql怎么保证一致性？OK，这个问题分为两个层面来说。 从数据库层面，数据库通过原子性、隔离性、持久性来保证一致性。也就是说ACID四大特性之中，C(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是手段，是为了保证一致性，数据库提供的手段。数据库必须要实现AID三大特性，才有可能实现一致性。例如，原子性无法保证，显然一致性也无法保证。 但是，如果你在事务里故意写出违反约束的代码，一致性还是无法保证的。例如，你在转账的例子中，你的代码里故意不给B账户加钱，那一致性还是无法保证。因此，还必须从应用层角度考虑。 从应用层面，通过代码判断数据库数据是否有效，然后决定回滚还是提交数据！ mysql怎么保证原子性OK，是利用Innodb的undo log。 undo log名为回滚日志，是实现原子性的关键，当事务回滚时能够撤销所有已经成功执行的sql语句，他需要记录你要回滚的相应日志信息。 例如 (1)当你delete一条数据的时候，就需要记录这条数据的信息，回滚的时候，insert这条旧数据 (2)当你update一条数据的时候，就需要记录之前的旧值，回滚的时候，根据旧值执行update操作 (3)当年insert一条数据的时候，就需要这条记录的主键，回滚的时候，根据主键执行delete操作 undo log记录了这些回滚需要的信息，当事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。 mysql怎么保证持久性的OK，是利用Innodb的redo log。 正如之前说的，Mysql是先把磁盘上的数据加载到内存中，在内存中对数据进行修改，再刷回磁盘上。如果此时突然宕机，内存中的数据就会丢失。 怎么解决这个问题？ 简单啊，事务提交前直接把数据写入磁盘就行啊。 这么做有什么问题？ 只修改一个页面里的一个字节，就要将整个页面刷入磁盘，太浪费资源了。毕竟一个页面16kb大小，你只改其中一点点东西，就要将16kb的内容刷入磁盘，听着也不合理。 毕竟一个事务里的SQL可能牵涉到多个数据页的修改，而这些数据页可能不是相邻的，也就是属于随机IO。显然操作随机IO，速度会比较慢。 于是，决定采用redo log解决上面的问题。当做数据修改的时候，不仅在内存中操作，还会在redo log中记录这次操作。当事务提交的时候，会将redo log日志进行刷盘(redo log一部分在内存中，一部分在磁盘上)。当数据库宕机重启的时候，会将redo log中的内容恢复到数据库中，再根据undo log和binlog内容决定回滚数据还是提交数据。 采用redo log的好处？ 其实好处就是将redo log进行刷盘比对数据页刷盘效率高，具体表现如下 redo log体积小，毕竟只记录了哪一页修改了啥，因此体积小，刷盘快。 redo log是一直往末尾进行追加，属于顺序IO。效率显然比随机IO来的快。 mysql怎么保证隔离性利用的是锁和MVCC机制。 并发事务带来的问题脏读 丢弃修改T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 幻读T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 不可重复读和幻读区别不可重复读的重点是修改，幻读的重点在于新增或者删除。 例1（同样的条件, 你读取过的数据, 再次读取出来发现值不一样了 ）：事务1中的A先生读取自己的工资为 1000的操 作还没完成，事务2中的B先生就修改了A的工资为2000，导 致A再读自己的工资时工资变为 2000；这就是不可重复读。 例2（同样的条件, 第1次和第2次读出来的记录数不一样 ）：假某工资单表中工资大于3000的有4人，事务1读取了所 有工资大于3000的人，共查到4条记录，这时事务2 又插入了一条工资大于3000的记录，事务1再次读取时查到的记 录就变为了5条，这样就导致了幻读。 数据库的隔离级别 未提交读，事务中发生了修改，即使没有提交，其他事务也是可见的，比如对于一个数A原来50修改为100，但是我还没有提交修改，另一个事务看到这个修改，而这个时候原事务发生了回滚，这时候A还是50，但是另一个事务看到的A是100.可能会导致脏读、幻读或不可重复读 提交读，对于一个事务从开始直到提交之前，所做的任何修改是其他事务不可见的，举例就是对于一个数A原来是50，然后提交修改成100，这个时候另一个事务在A提交修改之前，读取的A是50，刚读取完，A就被修改成100，这个时候另一个事务再进行读取发现A就突然变成100了；可以阻止脏读，但是幻读或不可重复读仍有可能发生 可重复读，就是对一个记录读取多次的记录是相同的，比如对于一个数A读取的话一直是A，前后两次读取的A是一致的；可以阻止脏读和不可重复读，但幻读仍有可能发生。 可串行化读，在并发情况下，和串行化的读取的结果是一致的，没有什么不同，比如不会发生脏读和幻读；该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻影读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。 这里需要注意的是：与 SQL 标准不同的地方在于InnoDB 存储引擎在 REPEATABLE-READ（可重读）事务隔离级别 下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以 说InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要 求，即达到了 SQL标准的SERIALIZABLE(可串行化)隔离级别。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内 容):，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读）并不会有任何性能损失。 InnoDB 存储引擎在分布式事务 的情况下一般会用到SERIALIZABLE(可串行化)隔离级别。 为什么使用索引 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 帮助服务器避免排序和临时表 将随机IO变为顺序IO。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 索引这么多优点，为什么不对表总的每一列创建一个索引 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立簇索引，那么需要的空间就会更大。 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加 索引如何提高查询速度的将无序的数据变成相对有序的数据（就像查有目的一样） 使用索引的注意事项 在经常需要搜索的列上，可以加快搜索的速度； 在经常使用在where子句中的列上面创建索引，加快条件的判断速度。 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间 在中到大型表索引都是非常有效的，但是特大型表的维护开销会很大，不适合建索引 在经常用到连续的列上，这些列主要是由一些外键，可以加快连接的速度 避免where子句中对字段施加函数，这会造成无法命中索引 在使用InnoDB时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。 将打算加索引的列设置为NOT NULL，否则将导致引擎放弃使用索引而进行全表扫描 删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗 在使用limit offset查询缓存时，可以借助索引来提高性能。 MySQL索引主要使用的两种数据结构 哈希索引，对于哈希索引来说，底层的数据结构肯定是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引 BTree索引，Mysql的BTree索引使用的是B树中的B+Tree但对于主要的两种存储引擎（MyISAM和InnoDB）的实现方式是不同的。 myisam和innodb实现btree索引方式的区别 MyISAM，B+Tree叶节点的data域存放的是数据记录的地址，在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的key存在，则取出其data域的值，然后以data域的值为地址读区相应的数据记录，这被称为“非聚簇索引” InnoDB，其数据文件本身就是索引文件，相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，树的节点data域保存了完整的数据记录，这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“聚簇索引”或者聚集索引，而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方，在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，在走一遍主索引。因此，在设计表的时候，不建议使用过长的字段为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。 数据库结构优化 范式优化： 比如消除冗余（节省空间。。） 反范式优化：比如适当加冗余等（减少join） 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 拆分表：分区将数据在物理上分隔开，不同分区的数据可以制定保存在处于不同磁盘上的数据文件里。这样，当对这个表进行查询时，只需要在表分区中进行扫描，而不必进行全表扫描，明显缩短了查询时间，另外处于不同磁盘的分区也将对这个表的数据传输分散在不同的磁盘I/O，一个精心设置的分区可以将数据传输对磁盘I/O竞争均匀地分散开。对数据量大的时时表可采取此方法。可按月自动建表分区。 拆分其实又分垂直拆分和水平拆分： 案例： 简单购物系统暂设涉及如下表： 1.产品表（数据量10w，稳定） 2.订单表（数据量200w，且有增长趋势） 3.用户表 （数据量100w，且有增长趋势） 以mysql为例讲述下水平拆分和垂直拆分，mysql能容忍的数量级在百万静态数据可以到千万 垂直拆分： 解决问题：表与表之间的io竞争 不解决问题：单表中数据量增长出现的压力 方案： 把产品表和用户表放到一个server上 订单表单独放到一个server上 水平拆分： 解决问题：单表中数据量增长出现的压力 不解决问题：表与表之间的io争夺 方案：用户表 通过性别拆分为男用户表和女用户表，订单表 通过已完成和完成中拆分为已完成订单和未完成订单，产品表 未完成订单放一个server上，已完成订单表盒男用户表放一个server上，女用户表放一个server上(女的爱购物 哈哈)。 主键超键候选键外键是什么 超键：在关系中能唯一标识元组的属性集称为关系模式的超键 候选键：不含有多余属性的超键称为候选键。也就是在候选键中，若再删除属性，就不是键了！ 主键：用户选作元组标识的一个候选键程序主键 外键：如果关系模式R中属性K是其它模式的主键，那么k在模式R中称为外键。 举例： 学号 姓名 性别 年龄 系别 专业 20020612 李辉 男 20 计算机 软件开发 20060613 张明 男 18 计算机 软件开发 20060614 王小玉 女 19 物理 力学 20060615 李淑华 女 17 生物 动物学 20060616 赵静 男 21 化学 食品化学 20060617 赵静 女 20 生物 植物学 超键：于是我们从例子中可以发现 学号是标识学生实体的唯一标识。那么该元组的超键就为学号。除此之外我们还可以把它跟其他属性组合起来，比如：(学号，性别)，(学号，年龄) 候选键：根据例子可知，学号是一个可以唯一标识元组的唯一标识，因此学号是一个候选键，实际上，候选键是超键的子集，比如 （学号，年龄）是超键，但是它不是候选键。因为它还有了额外的属性。 主键：简单的说，例子中的元组的候选键为学号，但是我们选定他作为该元组的唯一标识，那么学号就为主键。 外键是相对于主键的，比如在学生记录里，主键为学号，在成绩单表中也有学号字段，因此学号为成绩单表的外键，为学生表的主键。 主键为候选键的子集，候选键为超键的子集，而外键的确定是相对于主键的。 drop,delete与truncate的区别 drop直接删掉表; truncate删除表中数据，再插入时自增长id又从1开始 ; delete删除表中数据，可以加where字句。 DELETE语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 表和索引所占空间。当表被TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小，而DELETE操作不会减少表或索引所占用的空间。drop语句将表所占用的空间全释放掉。 一般而言，drop &gt; truncate &gt; delete 应用范围。TRUNCATE 只能对TABLE；DELETE可以是table和view TRUNCATE 和DELETE只删除数据，而DROP则删除整个表（结构和数据）。 truncate与不带where的delete ：只删除数据，而不删除表的结构（定义）drop语句将删除表的结构被依赖的约束（constrain),触发器（trigger)索引（index);依赖于该表的存储过程/函数将被保留，但其状态会变为：invalid。 delete语句为DML（Data Manipulation Language),这个操作会被放到 rollback segment中,事务提交后才生效。如果有相应的 tigger,执行的时候将被触发。 truncate、drop是DDL（Data Define Language),操作立即生效，原数据不放到 rollback segment中，不能回滚 在没有备份情况下，谨慎使用 drop 与 truncate。要删除部分数据行采用delete且注意结合where来约束影响范围。回滚段要足够大。要删除表用drop;若想保留表而将表中数据删除，如果于事务无关，用truncate即可实现。如果和事务有关，或老是想触发trigger,还是用delete。 Truncate table 表名 速度快,而且效率高,因为: truncate table 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 TRUNCATE TABLE 删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 DROP TABLE 语句。 对于由 FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句。由于 TRUNCATE TABLE 不记录在日志中，所以它不能激活触发器。 视图的作用，视图可以更改吗视图是虚拟的表，与包含数据的表不一样，视图只包含使用时动态检索数据的查询；不包含任何列或数据。使用视图可以简化复杂的sql操作，隐藏具体的细节，保护数据；视图创建后，可以使用与表相同的方式利用它们。 视图不能被索引，也不能有关联的触发器或默认值，如果视图本身内有order by 则对视图再次order by将被覆盖。 创建视图：create view xxx as xxxx 对于某些视图比如未使用联结子查询分组聚集函数Distinct Union等，是可以对其更新的，对视图的更新将对基表进行更新；但是视图主要用于简化检索，保护数据，并不用于更新，而且大部分视图都不可以更新。 数据库范式第一范式在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库。 所谓第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。如果出现重复的属性，就可能需要定义一个新的实体，新的实体由重复的属性构成，新实体与原实体之间为一对多关系。在第一范式（1NF）中表的每一行只包含一个实例的信息。简而言之，第一范式就是无重复的列。 第二范式第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。这个惟一属性列被称为主关键字或主键、主码。 第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。简而言之，第二范式就是非主属性非部分依赖于主关键字。 第三范式满足第三范式（3NF）必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。例如，存在一个部门信息表，其中每个部门有部门编号（dept_id）、部门名称、部门简介等信息。那么在员工信息表中列出部门编号后就不能再将部门名称、部门简介等与部门有关的信息再加入员工信息表中。如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余。简而言之，第三范式就是属性不依赖于其它非主属性。（我的理解是消除冗余） 什么是覆盖索引如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称 之为“覆盖索引”。我们知道在InnoDB存储引 擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次,这样就 会比较慢。覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！ 创作不易哇，觉得有帮助的话，给个小小的star呗。github地址😁😁😁","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"db","slug":"db","permalink":"http://dreamcat.ink/tags/db/"}]},{"title":"个人吐血系列-总结Mybatis","slug":"个人吐血系列-总结Mybatis","date":"2020-03-29T12:15:48.000Z","updated":"2021-01-01T09:53:53.702Z","comments":true,"path":"2020/03/29/ge-ren-tu-xie-xi-lie-zong-jie-mybatis/","link":"","permalink":"http://dreamcat.ink/2020/03/29/ge-ren-tu-xie-xi-lie-zong-jie-mybatis/","excerpt":"","text":"个人感觉，这部分源码的重要基础之一就是反射，不过这里就不贴源码，好好学习Java的反射吧。 大纲图 什么是数据持久化？ 数据持久化是将内存中的数据模型转换为存储模型，以及将存储模型转换为内存中的数据模型的统称。例如，文件的存储、数据的读取等都是数据持久化操作。数据模型可以是任何数据结构或对象的模型、XML、二进制流等。 当我们编写应用程序操作数据库，对表数据进行增删改查的操作的时候就是数据持久化的操作。 Mybatis框架简介 MyBatis框架是一个开源的数据持久层框架。 它的内部封装了通过JDBC访问数据库的操作，支持普通的SQL查询、存储过程和高级映射，几乎消除了所有的JDBC代码和参数的手工设置以及结果集的检索。 MyBatis作为持久层框架，其主要思想是将程序中的大量SQL语句剥离出来，配置在配置文件当中，实现SQL的灵活配置。 这样做的好处是将SQL与程序代码分离，可以在不修改代码的情况下，直接在配置文件当中修改SQL。 什么是ORM？ORM（Object/Relational Mapping）即对象关系映射，是一种数据持久化技术。它在对象模型和关系型数据库直接建立起对应关系，并且提供一种机制，通过JavaBean对象去操作数据库表的数据。MyBatis通过简单的XML或者注解的方式进行配置和原始映射，将实体类和SQL语句之间建立映射关系，是一种半自动（之所以说是半自动，因为我们要自己写SQL）的ORM实现。 MyBatis框架的优缺点及其适用的场合优点 与JDBC相比，减少了50%以上的代码量。 MyBatis是易学的持久层框架，小巧并且简单易学。 MyBatis相当灵活，不会对应用程序或者数据库的现有设计强加任何影响，SQL写在XML文件里，从程序代码中彻底分离，降低耦合度，便于统一的管理和优化，并可重用。 提供XML标签，支持编写动态的SQL，满足不同的业务需求。 提供映射标签，支持对象与数据库的ORM字段关系映射。 缺点 SQL语句的编写工作量较大，对开发人员编写SQL的能力有一定的要求。 SQL语句依赖于数据库，导致数据库不具有好的移植性，不可以随便更换数据库。 适用场景MyBatis专注于SQL自身，是一个足够灵活的DAO层解决方案。对性能的要求很高，或者需求变化较多的项目，例如Web项目，那么MyBatis是不二的选择。 MyBatis与Hibernate有哪些不同？ Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句。 Mybatis直接编写原生态sql，可以严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，因为这类软件需求变化频繁，一但需求变化要求迅速输出成果。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件，则需要自定义多套sql映射文件，工作量大。 Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件，如果用hibernate开发可以节省很多代码，提高效率。 #{}和${}的区别是什么？ #{} 是预编译处理，${}是字符串替换。 Mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值； Mybatis在处理${}时，就是把${}替换成变量的值。 使用#{}可以有效的防止SQL注入，提高系统安全性。 当实体类中的属性名和表中的字段名不一样，怎么办？ 第1种： 通过在查询的sql语句中定义字段名的别名，让字段名的别名和实体类的属性名一致。 第2种： 通过 &lt;resultMap 来映射字段名和实体类属性名的一一对应的关系。 模糊查询like语句该怎么写？ 第1种：在Java代码中添加sql通配符。 第2种：在sql语句中拼接通配符，会引起sql注入 Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？Dao接口即Mapper接口。接口的全限名，就是映射文件中的namespace的值；接口的方法名，就是映射文件中Mapper的Statement的id值；接口方法内的参数，就是传递给sql的参数。Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MapperStatement。在Mybatis中每&lt;select、&lt;insert、&lt;update、&lt;delete标签，都会被解析为一个MapperStatement对象。Mapper接口里的方法，是不能重载的，因为是使用 全限名+方法名 的保存和寻找策略。Mapper 接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Mapper接口生成代理对象proxy，代理对象会拦截接口方法，转而执行MapperStatement所代表的sql，然后将sql执行结果返回。 Mybatis是如何进行分页的？分页插件的原理是什么？Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页。可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。 Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？ 第一种是使用&lt;resultMap标签，逐一定义数据库列名和对象属性名之间的映射关系。 第二种是使用sql列的别名功能，将列的别名书写为对象属性名。 有了列名与属性名的映射关系后，Mybatis通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。 Mybatis动态sql有什么用？执行原理？有哪些动态sql？Mybatis动态sql可以在Xml映射文件内，以标签的形式编写动态sql，执行原理是根据表达式的值完成逻辑判断并动态拼接sql的功能。 Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复；原因就是namespace+id是作为Map &lt;String,MapperStatement 的key使用的，如果没有namespace，就剩下id，那么，id重复会导致数据互相覆盖。有了namespace，自然id就可以重复，namespace不同，namespace+id自然也就不同。 为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。 MyBatis实现一对一有几种方式?具体怎么操作的？有联合查询和嵌套查询,联合查询是几个表联合查询,只查询一次, 通过在resultMap里面配置association节点配置一对一的类就可以完成；嵌套查询是先查一个表，根据这个表里面的结果的 外键id，去再另外一个表里面查询数据,也是通过association配置，但另外一个表的查询通过select属性配置。 MyBatis实现一对多有几种方式,怎么操作的？有联合查询和嵌套查询。联合查询是几个表联合查询,只查询一次,通过在resultMap里面的collection节点配置一对多的类就可以完成；嵌套查询是先查一个表,根据这个表里面的 结果的外键id,去再另外一个表里面查询数据,也是通过配置collection,但另外一个表的查询通过select节点配置。 Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。 Mybatis的一级、二级缓存 一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存。 二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态),可在它的映射文件中配置 &lt;cache/ ； 对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。 什么是MyBatis的接口绑定？有哪些实现方式？接口绑定，就是在MyBatis中任意定义接口,然后把接口里面的方法和SQL语句绑定, 我们直接调用接口方法就可以,这样比起原来了SqlSession提供的方法我们可以有更加灵活的选择和设置。接口绑定有两种实现方式： 注解绑定，就是在接口的方法上面加上 @Select、@Update等注解，里面包含Sql语句来绑定； 外一种就是通过xml里面写SQL来绑定, 在这种情况下,要指定xml映射文件里面的namespace必须为接口的全路径名。当Sql语句比较简单时候,用注解绑定, 当SQL语句比较复杂时候,用xml绑定,一般用xml绑定的比较多。 使用MyBatis的mapper接口调用时有哪些要求？ Mapper接口方法名和mapper.xml中定义的每个sql的id相同； Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同； Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同； Mapper.xml文件中的namespace即是mapper接口的类路径。 mybatis是如何防止SQL注入的？ 首先看一下下面两个sql语句的区别： &lt;select id=\"selectByNameAndPassword\" parameterType=\"java.util.Map\" resultMap=\"BaseResultMap\" select id, username, password, role from user where username = #{username,jdbcType=VARCHAR} and password = #{password,jdbcType=VARCHAR} &lt;/select &lt;select id=\"selectByNameAndPassword\" parameterType=\"java.util.Map\" resultMap=\"BaseResultMap\" select id, username, password, role from user where username = ${username,jdbcType=VARCHAR} and password = ${password,jdbcType=VARCHAR} &lt;/select mybatis中的#和$的区别： #将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：where username=#{username}，如果传入的值是111,那么解析成sql时的值为where username=&quot;111&quot;, 如果传入的值是id，则解析成的sql为where username=&quot;id&quot;. $将传入的数据直接显示生成在sql中。如：where username=${username}，如果传入的值是111,那么解析成sql时的值为where username=111；如果传入的值是：drop table user;，则解析成的sql为：select id, username, password, role from user where username=;drop table user; #方式能够很大程度防止sql注入，$方式无法防止Sql注入。 $方式一般用于传入数据库对象，例如传入表名. 一般能用#的就别用$，若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止sql注入攻击。 在MyBatis中，“${xxx}”这样格式的参数会直接参与SQL编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式。所以，这样的参数需要我们在代码中手工进行处理来防止注入。 sql注入： SQL注入，大家都不陌生，是一种常见的攻击方式。攻击者在界面的表单信息或URL上输入一些奇怪的SQL片段（例如“or ‘1’=’1’”这样的语句），有可能入侵参数检验不足的应用程序。所以，在我们的应用中需要做一些工作，来防备这样的攻击方式。在一些安全性要求很高的应用中（比如银行软件），经常使用将SQL语句全部替换为存储过程这样的方式，来防止SQL注入。这当然是一种很安全的方式，但我们平时开发中，可能不需要这种死板的方式。 mybatis是如何做到防止sql注入的 MyBatis框架作为一款半自动化的持久层框架，其SQL语句都要我们自己手动编写，这个时候当然需要防止SQL注入。其实，MyBatis的SQL是一个具有“输入+输出”的功能，类似于函数的结构，参考上面的两个例子。其中，parameterType表示了输入的参数类型，resultType表示了输出的参数类型。回应上文，如果我们想防止SQL注入，理所当然地要在输入参数上下功夫。上面代码中使用#的即输入参数在SQL中拼接的部分，传入参数后，打印出执行的SQL语句，会看到SQL是这样的： select id, username, password, role from user where username=? and password=? 不管输入什么参数，打印出的SQL都是这样的。这是因为MyBatis启用了预编译功能，在SQL执行前，会先将上面的SQL发送给数据库进行编译；执行时，直接使用编译好的SQL，替换占位符“?”就可以了。因为SQL注入只能对编译过程起作用，所以这样的方式就很好地避免了SQL注入的问题。 [底层实现原理]MyBatis是如何做到SQL预编译的呢？其实在框架底层，是JDBC中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。这种“准备好”的方式不仅能提高安全性，而且在多次执行同一个SQL时，能够提高效率。原因是SQL已编译好，再次执行时无需再编译。 //安全的，预编译了的 Connection conn = getConn();//获得连接 String sql = \"select id, username, password, role from user where id=?\"; //执行sql前会预编译号该条语句 PreparedStatement pstmt = conn.prepareStatement(sql); pstmt.setString(1, id); ResultSet rs=pstmt.executeUpdate(); ...... //不安全的，没进行预编译 private String getNameByUserId(String userId) { Connection conn = getConn();//获得连接 String sql = \"select id,username,password,role from user where id=\" + id; //当id参数为\"3;drop table user;\"时，执行的sql语句如下: //select id,username,password,role from user where id=3; drop table user; PreparedStatement pstmt = conn.prepareStatement(sql); ResultSet rs=pstmt.executeUpdate(); ...... } 结论： #{}：相当于JDBC中的PreparedStatement ${}：是输出变量的值 简单说，#{}是经过预编译的，是安全的；${}是未经过预编译的，仅仅是取变量的值，是非安全的，存在SQL注入。 创作不易哇，觉得有帮助的话，给个小小的star呗。github地址😁😁😁","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://dreamcat.ink/tags/mybatis/"}]},{"title":"个人吐血系列-总结Spring","slug":"个人吐血系列-总结Spring","date":"2020-03-29T07:45:57.000Z","updated":"2021-01-01T09:55:15.383Z","comments":true,"path":"2020/03/29/ge-ren-tu-xie-xi-lie-zong-jie-spring/","link":"","permalink":"http://dreamcat.ink/2020/03/29/ge-ren-tu-xie-xi-lie-zong-jie-spring/","excerpt":"","text":"个人感觉，Spring这一块，不仅会用，还得知道底层的源码或者是说初始化加载等过程吧，这篇就不介绍如何配置以及各个注解的使用，在这里不是重点。 大纲图 什么是Spring框架我们一般说 Spring 框架指的都是 Spring Framework，它是很多模块的集合，使用这些模块可以很方便地协助我们进行开发。这些模块是：核心容器、数据访问/集成,、Web、AOP（面向切面编程）、工具、消息和测试模块。比如：Core Container 中的 Core 组件是Spring 所有组件的核心，Beans 组件和 Context 组件是实现IOC和依赖注入的基础，AOP组件用来实现面向切面编程。 Spring 官网列出的 Spring 的 6 个特征: 核心技术 ：依赖注入(DI)，AOP，事件(events)，资源，i18n，验证，数据绑定，类型转换，SpEL。 测试 ：模拟对象，TestContext框架，Spring MVC 测试，WebTestClient。 数据访问 ：事务，DAO支持，JDBC，ORM，编组XML。 Web支持 : Spring MVC和Spring WebFlux Web框架。 集成 ：远程处理，JMS，JCA，JMX，电子邮件，任务，调度，缓存。 语言 ：Kotlin，Groovy，动态语言。 列举一些重要的Spring模块 Spring Core： 基础,可以说 Spring 其他所有的功能都需要依赖于该类库。主要提供 IoC 依赖注入功能。 Spring Aspects ： 该模块为与AspectJ的集成提供支持。 Spring AOP ：提供了面向切面的编程实现。 Spring JDBC : Java数据库连接。 Spring JMS ：Java消息服务。 Spring ORM : 用于支持Hibernate等ORM工具。 Spring Web : 为创建Web应用程序提供支持。 Spring Test : 提供了对 JUnit 和 TestNG 测试的支持。 @RestController VS ControllerControllerController 返回一个页面 单独使用 @Controller 不加 @ResponseBody的话一般使用在要返回一个视图的情况，这种情况属于比较传统的Spring MVC 的应用，对应于前后端不分离的情况。 RestController@RestController 返回JSON 或 XML 形式数据 但@RestController只返回对象，对象数据直接以 JSON 或 XML 形式写入 HTTP 响应(Response)中，这种情况属于 RESTful Web服务，这也是目前日常开发所接触的最常用的情况（前后端分离）。 @Controller +@ResponseBody 返回JSON 或 XML 形式数据 如果你需要在Spring4之前开发 RESTful Web服务的话，你需要使用@Controller 并结合@ResponseBody注解，也就是说@Controller +@ResponseBody= @RestController（Spring 4 之后新加的注解）。 谈谈SpringIOC和AOPIOCIoC（Inverse of Control:控制反转）是一种设计思想，就是 将原本在程序中手动创建对象的控制权，交由Spring框架来管理。 IoC 在其他语言中也有应用，并非 Spring 特有。 IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。 将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注入。这样可以很大程度上简化应用的开发，把应用从复杂的依赖关系中解放出来。 IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 在实际项目中一个 Service 类可能有几百甚至上千个类作为它的底层，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。 Spring 时代我们一般通过 XML 文件来配置 Bean，后来开发人员觉得 XML 文件来配置不太好，于是 SpringBoot 注解配置就慢慢开始流行起来。 推荐阅读：https://www.zhihu.com/question/23277575/answer/169698662 ioc容器初始化流程可以看到有很多PostProcessors的后置处理器 @Override public void refresh() throws BeansException, IllegalStateException { // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛 synchronized (this.startupShutdownMonitor) { // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 prepareRefresh(); // Tell the subclass to refresh the internal bean factory. //获得容器ApplicationContext的子类BeanFactory。步骤如下： //1.如果已经有了BeanFactory就销毁它里面的单例Bean并关闭这个BeanFactory。 //2.创建一个新的BeanFactory。 //3.对这个BeanFactory进行定制（customize),如allowBeanDefinitionOverriding等参数 //4.转载BeanDefinitions(读取配置文件，将xml转换成对应得BeanDefinition) //5.检查是否同时启动了两个BeanFactory。 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean // 这块待会会展开说 prepareBeanFactory(beanFactory); try { // 设置beanFactory的后置处理器 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用beanFactory的后置处理器 // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类（bean的后置处理器） // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。注意，到这里 Bean 还没初始化 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了 initApplicationEventMulticaster(); // 模板方法(钩子方法)， // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过 registerListeners(); // 重点，重点，重点 // 初始化所有的 singleton beans //（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成 finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); } // Destroy already created singletons to avoid dangling resources. // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // 把异常往外抛 throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } 大概总结一下 Spring容器在启动的时候，先会保存所有注册进来的Bean的定义信息； xml注册bean； 注解注册Bean；@Service、@Component、@Bean、xxx Spring容器会合适的时机创建这些Bean 用到这个bean的时候；利用getBean创建bean；创建好以后保存在容器中； 统一创建剩下所有的bean的时候；finishBeanFactoryInitialization()； 后置处理器；BeanPostProcessor 每一个bean创建完成，都会使用各种后置处理器进行处理；来增强bean的功能；比如 AutowiredAnnotationBeanPostProcessor:处理自动注入 AnnotationAwareAspectJAutoProxyCreator:来做AOP功能； xxx 事件驱动模型； ApplicationListener；事件监听； ApplicationEventMulticaster；事件派发： 更详细的源码可看 http://dreamcat.ink/2020/01/31/spring-springaop-yuan-ma-fen-xi/ https://javadoop.com/post/spring-ioc AOPAOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。Spring AOP就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理。当然你也可以使用 AspectJ ,Spring AOP 已经集成了AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。 Spring AOP 和 AspectJ AOP 有什么区别Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单，如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多。 源码总结 @EnableAspectJAutoProxy 开启AOP功能 @EnableAspectJAutoProxy 会给容器中注册一个组件 AnnotationAwareAspectJAutoProxyCreator AnnotationAwareAspectJAutoProxyCreator是一个后置处理器； 容器的创建流程： registerBeanPostProcessors（）注册后置处理器；创建AnnotationAwareAspectJAutoProxyCreator对象（Spring源码） finishBeanFactoryInitialization（）初始化剩下的单实例bean（Spring源码） 创建业务逻辑组件和切面组件 AnnotationAwareAspectJAutoProxyCreator拦截组件的创建过程 组件创建完之后，判断组件是否需要增强；是-&gt;切面的通知方法，包装成增强器（Advisor）;给业务逻辑组件创建一个代理对象（cglib）； 执行目标方法： 代理对象执行目标方法 CglibAopProxy.intercept()； 得到目标方法的拦截器链（增强器包装成拦截器MethodInterceptor） 利用拦截器的链式机制，依次进入每一个拦截器进行执行； 效果： 正常执行：前置通知-》目标方法-》后置通知-》返回通知 出现异常：前置通知-》目标方法-》后置通知-》异常通知 Spring Bean作用域 singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。 session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。 global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话 Spring的单例有线程安全问题吗大部分时候我们并没有在系统中使用多线程，所以很少有人会关注这个问题。单例 bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候，对这个对象的非静态成员变量的写操作会存在线程安全问题。 常见的有两种解决办法： 在Bean对象中尽量避免定义可变的成员变量（不太现实）。 在类中定义一个ThreadLocal成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。 Component和Bean的区别 作用对象不同: @Component 注解作用于类，而@Bean注解作用于方法。 @Component通常是通过类路径扫描来自动侦测以及自动装配到Spring容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,@Bean告诉了Spring这是某个类的示例，当我需要用它的时候还给我。 @Bean 注解比 Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册bean。比如当我们引用第三方库中的类需要装配到 Spring容器时，则只能通过 @Bean来实现。 将类声明为bean有哪些我们一般使用 @Autowired 注解自动装配 bean，要想把类标识成可用于 @Autowired 注解自动装配的 bean 的类,采用以下注解可实现： @Component ：通用的注解，可标注任意类为 Spring 组件。如果一个Bean不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao层。 @Controller : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。 Bean的声明周期 Bean 容器找到配置文件中 Spring Bean 的定义。 Bean 容器利用 Java Reflection API 创建一个Bean的实例。 如果涉及到一些属性值 利用 set()方法设置一些属性值。 如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入Bean的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法 当要销毁 Bean 的时候，如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法。 SpringMVCMVC 是一种设计模式,Spring MVC 是一款很优秀的 MVC 框架。Spring MVC 可以帮助我们进行更简洁的Web层的开发，并且它天生与 Spring 框架集成。Spring MVC 下我们一般把后端项目分为 Service层（处理业务）、Dao层（数据库操作）、Entity层（实体类）、Controller层(控制层，返回数据给前台页面)。 工作原理 流程说明（重要）： 客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。 解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 HandlerAdapter 会根据 Handler来调用真正的处理器开处理请求，并处理相应的业务逻辑。 处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器） Spring都用到了哪些设计模式 工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 …… Spring事务管理事物有几种 编程式事务，在代码中硬编码。 声明式事务，在配置文件中配置 声明式事务又分为两种： 基于XML的声明式事务 基于注解的声明式事务 隔离级别隔离级别就跟mysql几乎差不多 源码分析 开启@EnableTransactionManagement 利用TransactionManagementConfigurationSelector给容器中会导入组件 AutoProxyRegistrar ProxyTransactionManagementConfiguration AutoProxyRegistrar： 给容器中注册一个 InfrastructureAdvisorAutoProxyCreator 组件； 利用后置处理器机制在对象创建以后，包装对象，返回一个代理对象（增强器），代理对象执行方法利用拦截器链进行调用； ProxyTransactionManagementConfiguration 做了什么？ 给容器中注册事务增强器； 事务增强器要用事务注解的信息，AnnotationTransactionAttributeSource解析事务注解 事务拦截器： TransactionInterceptor；保存了事务属性信息，事务管理器； 他是一个 MethodInterceptor；在目标方法执行的时候； 先获取事务相关的属性 再获取PlatformTransactionManager，如果事先没有添加指定任何transactionmanger，最终会从容器中按照类型获取一个PlatformTransactionManager； 执行目标方法 如果异常，获取到事务管理器，利用事务管理回滚操作； 如果正常，利用事务管理器，提交事务 Springboot启动流程SpringApplication实例public SpringApplication(ResourceLoader resourceLoader, Class... primarySources) { this.sources = new LinkedHashSet(); // 1. this.bannerMode = Mode.CONSOLE; this.logStartupInfo = true; this.addCommandLineProperties = true; this.headless = true; this.registerShutdownHook = true; this.additionalProfiles = new HashSet(); this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet(Arrays.asList(primarySources)); this.webApplicationType = this.deduceWebApplicationType(); this.setInitializers(this.getSpringFactoriesInstances(ApplicationContextInitializer.class)); // 3. this.setListeners(this.getSpringFactoriesInstances(ApplicationListener.class)); // 4. this.mainApplicationClass = this.deduceMainApplicationClass(); // 5. } com.example.helloworld.HelloworldApplication放入到Set的集合中 判断是否为Web环境：存在（javax.servlet.Servlet &amp;&amp; org.springframework.web.context.ConfigurableWebApplicationContext ）类 创建并初始化ApplicationInitializer列表 （spring.factories） 创建并初始化ApplicationListener列表 （spring.factories） 初始化主类mainApplicatioClass (DemoApplication) 总结：上面就是SpringApplication初始化的代码，new SpringApplication()没做啥事情 ，主要加载了META-INF/spring.factories 下面定义的事件监听器接口实现类 ConfigurableApplicationContext的run方法public ConfigurableApplicationContext run(String... args) { StopWatch stopWatch = new StopWatch(); // 1. 创建计时器StopWatch stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter> exceptionReporters = new ArrayList(); this.configureHeadlessProperty(); SpringApplicationRunListeners listeners = this.getRunListeners(args); // 2. 获取SpringApplicationRunListeners并启动 listeners.starting(); // Collection exceptionReporters; try { ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); // 创建ApplicationArguments ConfigurableEnvironment environment = this.prepareEnvironment(listeners, applicationArguments); // 创建并初始化ConfigurableEnvironment this.configureIgnoreBeanInfo(environment); // Banner printedBanner = this.printBanner(environment); // 打印Banner context = this.createApplicationContext(); // 创建ConfigurableApplicationContext exceptionReporters = this.getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[]{ConfigurableApplicationContext.class}, context); this.prepareContext(context, environment, listeners, applicationArguments, printedBanner);// 准备ConfigurableApplicationContext this.refreshContext(context); // 刷新ConfigurableApplicationContext，这个refreshContext()加载了bean，还启动了内置web容器，需要细细的去看看 this.afterRefresh(context, applicationArguments); // 容器刷新后动作，啥都没做 stopWatch.stop();// 计时器停止计时 if (this.logStartupInfo) { (new StartupInfoLogger(this.mainApplicationClass)).logStarted(this.getApplicationLog(), stopWatch); } listeners.started(context); this.callRunners(context, applicationArguments); } catch (Throwable var10) { this.handleRunFailure(context, var10, exceptionReporters, listeners); throw new IllegalStateException(var10); } try { listeners.running(context); return context; } catch (Throwable var9) { this.handleRunFailure(context, var9, exceptionReporters, (SpringApplicationRunListeners)null); throw new IllegalStateException(var9); } } 创建计时器StopWatch 获取SpringApplicationRunListeners并启动 创建ApplicationArguments 创建并初始化ConfigurableEnvironment 打印Banner 创建ConfigurableApplicationContext 准备ConfigurableApplicationContext 刷新ConfigurableApplicationContext，这个refreshContext()加载了bean，还启动了内置web容器，需要细细的去看看 容器刷新后动作，啥都没做 计时器停止计时 refreshContext()该源码中其实就是Spring源码的refresh()的源码 不过这里的refresh()是在AbstractApplicationContext抽象类上 其他就不提了，关注点在onrefresh()方法上，但是个空方法，毕竟是抽象类，去找其子类继承的它 debug调试可以找到ServletWebServerApplicationContext ServletWebServerApplicationContext onRefresh()-&gt;createWebServer()-&gt;getWebServerFactory()，此时已经加载了个web容器 可以返回刚才的createWebServer()，然后看factory.getWebServer public WebServer getWebServer(ServletContextInitializer... initializers) { //tomcat这位大哥出现了 Tomcat tomcat = new Tomcat(); File baseDir = (this.baseDirectory != null ? this.baseDirectory : createTempDir(\"tomcat\")); tomcat.setBaseDir(baseDir.getAbsolutePath()); Connector connector = new Connector(this.protocol); tomcat.getService().addConnector(connector); customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); configureEngine(tomcat.getEngine()); for (Connector additionalConnector : this.additionalTomcatConnectors) { tomcat.getService().addConnector(additionalConnector); } prepareContext(tomcat.getHost(), initializers); return getTomcatWebServer(tomcat); } 内置的Tomcat就出现了 总结：run() 方法主要调用了spring容器启动方法扫描配置，加载bean到spring容器中；启动的内置Web容器 SpringBootApplication的注解主要是三个注解 @SpringBootConfiguration:允许在上下文中注册额外的bean或导入其他配置类。 @EnableAutoConfiguration:启用 SpringBoot 的自动配置机制 @ComponentScan: 扫描常用的注解","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"}]},{"title":"个人吐血系列-总结JVM","slug":"个人吐血系列-总结JVM","date":"2020-03-28T15:42:45.000Z","updated":"2021-01-01T09:53:13.830Z","comments":true,"path":"2020/03/28/ge-ren-tu-xie-xi-lie-zong-jie-jvm/","link":"","permalink":"http://dreamcat.ink/2020/03/28/ge-ren-tu-xie-xi-lie-zong-jie-jvm/","excerpt":"","text":"个人感觉JVM这一块，了解和背的知识点挺多，代码并不是特别多，主要是后期调优，需要大量的经验罢了。不过JVM这一块一定要深刻理解。 大纲图 类文件结构概述在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。 可以说.class文件是不同的语言在 Java 虚拟机之间的重要桥梁，同时也是支持 Java 跨平台很重要的一个原因。 类文件总结构ClassFile { u4 magic; // 魔数 Class 文件的标志 这都没话可说的 u2 minor_version;//Class 的小版本号 u2 major_version;//Class 的大版本号 u2 constant_pool_count;//常量池的数量 cp_info constant_pool[constant_pool_count-1];//常量池 待会说 u2 access_flags;//Class 的访问标记 u2 this_class;//当前类 u2 super_class;//父类 u2 interfaces_count;//接口 u2 interfaces[interfaces_count];//一个类可以实现多个接口 u2 fields_count;//Class 文件的字段属性 field_info fields[fields_count];//一个类会可以有个字段 描述接口或类中声明的变量 但不包括在方法内部声明的局部变量. u2 methods_count;//Class 文件的方法数量 method_info methods[methods_count];//一个类可以有个多个方法 和字段表性质一样 u2 attributes_count;//此类的属性表中的属性数 attribute_info attributes[attributes_count];//属性表集合 } 静态常量池常量池主要存放两大常量：字面量和符号引用。字面量比较接近于 Java 语言层面的的常量概念，如文本字符串、声明为 final 的常量值等。而符号引用则属于编译原理方面的概念。包括下面三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 常量池的好处：常量池是为了避免频繁的创建和销毁对象而影响系统性能，其实现了对象的共享。 举个例子：字符串常量池，在编译阶段就把所有的字符串文字放到一个常量池中。 节省内存空间：常量池中所有相同的字符串常量被合并，只占用一个空间。 节省运行时间：比较字符串时，==比equals()快。对于两个引用变量，只用==判断引用是否相等，也就可以判断实际值是否相等。 静态常量池用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。其中符号引用其实引用的就是常量池里面的字符串，但符号引用不是直接存储字符串，而是存储字符串在常量池里的索引。 当Class文件被加载完成后，java虚拟机会将静态常量池里的内容转移到运行时常量池里，在静态常量池的符号引用有一部分是会被转变为直接引用的，比如说类的静态方法或私有方法，实例构造方法，父类方法，这是因为这些方法不能被重写其他版本，所以能在加载的时候就可以将符号引用转变为直接引用，而其他的一些方法是在这个方法被第一次调用的时候才会将符号引用转变为直接引用的。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。对于运行时常量池，Java虚拟机规范没有做任何细节的要求，不同的提供商实现的虚拟机可以按照自己的需要来实现这个内存区域。不过，一般来说，除了保存Class文件中描述的符号引用外，还会把翻译出来的直接引用也存储在运行时常量池中。 运行时常量池还有个更重要的的特征：动态性。Java要求，编译期的常量池的内容可以进入运行时常量池，运行时产生的常量也可以放入池中。常用的是String类的intern()方法。 既然运行时常量池是方法区的一部分自然会受到方法区内存的限制，当常量池无法再申请到内存时会抛出OutOfMemoryError异常。 字符串常量池字符串常量池存在运行时常量池之中（在JDK7之前存在运行时常量池之中，在JDK7已经将其转移到堆中）。 字符串常量池的存在使JVM提高了性能和减少了内存开销。 每当我们使用字面量（String s=“1”;）创建字符串常量时，JVM会首先检查字符串常量池，如果该字符串已经存在常量池中，那么就将此字符串对象的地址赋值给引用s（引用s在Java栈。如果字符串不存在常量池中，就会实例化该字符串并且将其放到常量池中，并将此字符串对象的地址赋值给引用s（引用s在Java栈中）。 每当我们使用关键字new（String s=new String(”1”);）创建字符串常量时，JVM会首先检查字符串常量池，如果该字符串已经存在常量池中，那么不再在字符串常量池创建该字符串对象，而直接堆中创建该对象的副本，然后将堆中对象的地址赋值给引用s，如果字符串不存在常量池中，就会实例化该字符串并且将其放到常量池中，然后在堆中创建该对象的副本，然后将堆中对象的地址赋值给引用s。 三者关系JVM在执行某个类的时候，必须经过加载、连接、初始化，而连接又包括验证、准备、解析三个阶段。 静态常量池用于存放编译期生成的各种字面量和符号引用，而当类加载到内存中后，jvm就会将静态常量池中的内容存放到运行时常量池中。而字符串常量池存的是引用值，其存在于运行时常量池之中。 版本变化1.6 静态常量池在Class文件中。 运行时常量池在Perm Gen区(也就是方法区)中。（所谓的方法区是在Java堆的一个逻辑部分，为了与Java堆区别开来，也称其为非堆（Non-Heap），那么Perm Gen（永久代）区也被视为方法区的一种实现。） 字符串常量池在运行时常量池中。 1.7 静态常量池在Class文件中。 运行时常量池依然在Perm Gen区(也就是方法区)中。在JDK7版本中，永久代的转移工作就已经开始了，将譬如符号引用(Symbols)转移到了native heap；字面量(interned strings)转移到了java heap；类的静态变量(class statics)转移到了java heap。但是运行时常量池依然还存在，只是很多内容被转移，其只存着这些被转移的引用。网上流传的一些测试运行时常量池转移的方式或者代码，其实是对字符串常量池转移的测试。 字符串常量池被分配到了Java堆的主要部分（known as the young and old generations）。也就是字符串常量池从运行时常量池分离出来了。 1.8 静态常量池在Class文件中。 JVM已经将运行时常量池从方法区中移了出来，在Java 堆（Heap）中开辟了一块区域存放运行时常量池。同时永久代被移除，以元空间代替。元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。其主要用于存放一些元数据。 字符串常量池存在于Java堆中。 类加载过程系统加载 Class 类型的文件主要三步:加载-&gt;连接-&gt;初始化。连接过程又可分为三步:验证-&gt;准备-&gt;解析。 加载类加载过程的第一步，主要完成下面3件事情： 通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的 Class 对象,作为方法区这些数据的访问入口 加载阶段和连接阶段的部分内容是交叉进行的，加载阶段尚未结束，连接阶段可能就已经开始了。 验证 文件格式验证：主要验证Class文件是否规范等。 元数据验证：对字节码描述的信息语义分析等。 字节码验证：确保语义是ok的。 符号引用验证：确保解析动作能执行。 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 这里所设置的初始值”通常情况”下是数据类型默认的零值（如0、0L、null、false等），比如我们定义了public static int value=111 ，那么 value 变量在准备阶段的初始值就是 0 而不是111（初始化阶段才会复制）。特殊情况：比如给 value 变量加上了 fianl 关键字public static final int value=111 ，那么准备阶段 value 的值就被复制为 111。 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。 符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。在程序实际运行时，只有符号引用是不够的，举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java 虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方发表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。 综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。 初始化初始化是类加载的最后一步，也是真正执行类中定义的 Java 程序代码(字节码)，初始化阶段是执行类构造器 &lt;clinit&gt; ()方法的过程。 对于&lt;clinit&gt;（） 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 &lt;clinit&gt;（） 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起死锁，并且这种死锁很难被发现。 对于初始化阶段，虚拟机严格规范了有且只有5种情况下，必须对类进行初始化： 当遇到 new 、 getstatic、putstatic或invokestatic 这4条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。 使用 java.lang.reflect 包的方法对类进行反射调用时 ，如果类没初始化，需要触发其初始化。 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。 当使用 JDK1.7 的动态动态语言时，如果一个 MethodHandle 实例的最后解析结构为 REF_getStatic、REF_putStatic、REF_invokeStatic、的方法句柄，并且这个句柄没有初始化，则需要先触发器初始化。 类加载器JVM 中内置了三个重要的 ClassLoader，除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader： BootstrapClassLoader(启动类加载器) ：最顶层的加载类，由C++实现，负责加载 %JAVA_HOME%/lib目录下的jar包和类或者或被 -Xbootclasspath参数指定的路径中的所有类。 ExtensionClassLoader(扩展类加载器) ：主要负责加载目录 %JRE_HOME%/lib/ext 目录下的jar包和类，或被 java.ext.dirs 系统变量所指定的路径下的jar包。 AppClassLoader(应用程序类加载器) :面向我们用户的加载器，负责加载当前应用classpath下的所有jar包和类。 双亲委派每一个类都有一个对应它的类加载器。系统中的 ClassLoder 在协同工作的时候会默认使用 双亲委派模型 。即在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派该父类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为null时，会使用启动类加载器 BootstrapClassLoader 作为父类加载器。 public class ClassLoaderDemo { public static void main(String[] args) { System.out.println(\"ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader()); System.out.println(\"The Parent of ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader().getParent()); System.out.println(\"The GrandParent of ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader().getParent().getParent()); } } // ClassLodarDemo's ClassLoader is sun.misc.Launcher$AppClassLoader@18b4aac2 // The Parent of ClassLodarDemo's ClassLoader is sun.misc.Launcher$ExtClassLoader@1b6d3586 // The GrandParent of ClassLodarDemo's ClassLoader is null AppClassLoader的父类加载器为ExtClassLoaderExtClassLoader的父类加载器为null，null并不代表ExtClassLoader没有父类加载器，而是 BootstrapClassLoader 。 其实这个双亲翻译的容易让别人误解，我们一般理解的双亲都是父母，这里的双亲更多地表达的是“父母这一辈”的人而已，并不是说真的有一个 Mother ClassLoader 和一个 Father ClassLoader 。另外，类加载器之间的“父子”关系也不是通过继承来体现的，是由“优先级”来决定。 双亲委派的好处： 双亲委派模型保证了Java程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现多个不同的 Object 类。 自定义类加载器除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显需要继承 ClassLoader。 jvm内存(运行时区域)1.8之前 1.8之后 按照1.8进行总结 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 直接内存 (非运行时数据区的一部分) 程序计数器说白了就是 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 Java虚拟机栈与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 局部变量表 操作数栈 8大基本类型 对象引用：可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置 动态链接 方法出口 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 异常。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出 OutOfMemoryError 异常。 Java 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。 本地方法栈和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 其他和Java虚拟机差不多的 堆Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 上图所示的 eden 区、s0(“From”) 区、s1(“To”) 区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s1(“To”)，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。经过这次GC后，Eden区和”From”区已经被清空。这个时候，”From”和”To”会交换他们的角色，也就是新的”To”就是上次GC前的“From”，新的”From”就是上次GC前的”To”。不管怎样，都会保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，直到“To”区被填满，”To”区被填满之后，会将所有对象移动到年老代中。 方法区方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 运行时常量池运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）以及符号引用替换为直接引用。JDK1.8 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 异常出现。本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 对象创建 类加载检查，虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 分配内存，在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的 内存分配并发问题（补充内容，需要掌握） CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 初始化零值，内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象头，初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 执行init方法，在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 内存布局在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。 Hotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的自身运行时数据（哈希码、GC 分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对象的访问方式建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种： 使用句柄如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 直接指针如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。 垃圾回收Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 对象优先在Eden区分配目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.下面我们来进行实际测试以下。 在测试之前我们先来看看 Minor GC 和 Full GC 有什么不同呢？ 新生代 GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC 非常频繁，回收速度一般也比较快。 老年代 GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC 经常会伴随至少一次的 Minor GC（并非绝对），Major GC 的速度一般会比 Minor GC 的慢 10 倍以上。 大对象直接进入老年代大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 为什么要这样呢？为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。 长期存活的对象进入老年代如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为 1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 如何判断对象死亡 引用计数法给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 所谓对象之间的相互引用问题，如下面代码所示：除了对象 objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为 0，于是引用计数算法无法通知 GC 回收器回收他们。 public class ReferenceCountingGc { Object instance = null; public static void main(String[] args) { ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; // 循环引用 objB.instance = objA; // 循环引用 objA = null; objB = null; } } 可达性分析这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 哪些可以作为GC Roots的根 虚拟机栈（栈帧中的局部变量区，也叫局部变量表）中应用的对象。 方法区中的类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI（native方法）引用的对象 在这里就聊一下引用 四大引用1．强引用（StrongReference） 以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。 2．软引用（SoftReference） 如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。 3．弱引用（WeakReference） 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。 4．虚引用（PhantomReference） “虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用主要用来跟踪对象被垃圾回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 不可达的对象并非“非死不可”即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。 被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。 如何判断一个常量是废弃常量运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？ 假如在常量池中存在字符串 “abc”，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池。 如何判断一个类是无用的类判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 垃圾回收算法标记-清除算法该算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 标记-整理算法根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 复制算法为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 分代收集算法比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 垃圾收集器 Serial收集器 ParNew收集器 Parallel Scavenge收集器 CMS收集器 G1收集器 Serial收集器Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。 新生代采用复制算法，老年代采用标记-整理算法。 虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。 但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。 ParNew收集器ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 新生代采用复制算法，老年代采用标记-整理算法。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。 Parallel Scavenge收集器Parallel Scavenge 收集器也是使用复制算法的多线程收集器，它看上去几乎和ParNew都一样。 Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在困难的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。 新生代采用复制算法，老年代采用标记-整理算法。 Serial Old 收集器Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。 Parallel Old 收集器 Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。 CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时 GC 线程开始对为标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对 CPU 资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 G1收集器G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征. 被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备一下特点： 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的“标记–清理”算法不同，G1 从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。 G1 收集器的运作大致分为以下几个步骤： 初始标记 并发标记 最终标记 筛选回收 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 GF 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 如何选择垃圾回收器 单CPU或小内存，单机内存 -XX:+UseSerialGC 多CPU，需要最大吞吐量，如后台计算型应用 -XX:+UseParallelGC -XX:+UseParallelOldGC 多CPU，最求低停顿时间，需快速相应，如互联网应用 -XX:+ParNewGC -XX:+UseConcMarkSweepGC 参数 新生代垃圾收集器 新生代算法 老年代垃圾收集器 老年代算法 UseSerialGC SerialGC 复制 SerialOldGC 标整 UseParNewGC ParNew 复制 SerialOldGC 标整 UseParallelGCUseParallelOldGC Parallel[Scavenge] 复制 Parallel Old 标整 UseConcMarkSweepGC ParNew 复制 CMS+Serial Old的收集器组合(Serial Old 作为CMS出错的后备收集器) 标清 UseG1GC G1整体上采用标整 局部是通过复制算法 JVM调优参数 这里只介绍一些常用的，还有更多的后续讲解… -Xms128m -Xmx4096m -Xss1024K -XX:MetaspaceSize=512m -XX:+PrintCommandLineFlags -XX:+PrintGCDetails -XX:+UseSerialGC","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://dreamcat.ink/tags/jvm/"}]},{"title":"个人吐血系列-总结Java集合","slug":"个人吐血系列-总结Java集合","date":"2020-03-27T16:22:40.000Z","updated":"2021-01-01T09:51:59.752Z","comments":true,"path":"2020/03/28/ge-ren-tu-xie-xi-lie-zong-jie-java-ji-he/","link":"","permalink":"http://dreamcat.ink/2020/03/28/ge-ren-tu-xie-xi-lie-zong-jie-java-ji-he/","excerpt":"","text":"个人感觉掌握常用的集合类，看其中的源码即可，有很多其实都差不多的，把个别不同的源码多看看，其实就是增删查 比如，常见的ArrayList、LinkedList、HashMap和ConcurrentHashMap经常被问到的多准备准备。 这一块就是看源码分析，没别的 ArrayList概述 ArrayList实现了List接口，是顺序容器，即元素存放的数据与放进去的顺序相同，允许放入null元素，底层通过数组实现。 除该类未实现同步外，其余跟Vector大致相同。 每个ArrayList都有一个容量（capacity），表示底层数组的实际大小，容器内存储元素的个数不能多于当前容量。 当向容器中添加元素时，如果容量不足，容器会自动增大底层数组的大小。 前面已经提过，Java泛型只是编译器提供的语法糖，所以这里的数组是一个Object数组，以便能够容纳任何类型的对象。 size(), isEmpty(), get(), set()方法均能在常数时间内完成，add()方法的时间开销跟插入位置有关，addAll()方法的时间开销跟添加元素的个数成正比。其余方法大都是线性时间。 为追求效率，ArrayList没有实现同步（synchronized），如果需要多个线程并发访问，用户可以手动同步，也可使用Vector替代。 实现底层数据结构transient Object[] elementData; // Object 数组 private int size; // 大小 构造函数 // 参数为容量的构造参数 public ArrayList(int initialCapacity) { if (initialCapacity > 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; // 默认 } else { throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); } } // 无参的构造参数 public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; // 默认容量 } public ArrayList(Collection&lt;? extends E> c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; } } 自动扩容 public void ensureCapacity(int minCapacity) { int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity > minExpand) { ensureExplicitCapacity(minCapacity); } } private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length > 0) grow(minCapacity); } private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity >> 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE > 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } private static int hugeCapacity(int minCapacity) { if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity > MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } 每当向数组中添加元素时，都要去检查添加后元素的个数是否会超出当前数组的长度，如果超出，数组将会进行扩容，以满足添加数据的需求。 数组扩容通过一个公开的方法ensureCapacity(int minCapacity)来实现。在实际添加大量元素前，我也可以使用ensureCapacity来手动增加ArrayList实例的容量，以减少递增式再分配的数量。 数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍。 这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。 当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。 或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。 add()是向容器中添加新元素，这可能会导致capacity不足，因此在添加元素之前，都需要进行剩余空间检查，如果需要则自动扩容。扩容操作最终是通过grow()方法完成的。 public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! // 多线程容易出问题 elementData[size++] = e; // 这里也是 return true; } public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } get()get()方法同样很简单，唯一要注意的是由于底层数组是Object[]，得到元素后需要进行类型转换。 public E get(int index) { rangeCheck(index); return (E) elementData[index];//注意类型转换 } remove()remove()方法也有两个版本，一个是remove(int index)删除指定位置的元素，另一个是remove(Object o)删除第一个满足o.equals(elementData[index])的元素。删除操作是add()操作的逆过程，需要将删除点之后的元素向前移动一个位置。需要注意的是为了让GC起作用，必须显式的为最后一个位置赋null值。 public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved > 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; //清除该位置的引用，让GC起作用 return oldValue; } indexOf()循环遍历用equals public int indexOf(Object o) { if (o == null) { for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; } else { for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; } return -1; } Fail-Fast机制ArrayList也采用了快速失败的机制，通过记录modCount参数来实现。在面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险。 LinkedList概述LinkedList同时实现了list接口和Deque接口，也就是说它既可以看作一个顺序容器，又可以看作一个队列（Queue），同时又可以看作一个栈（Stack）。这样看来，LinkedList简直就是个全能冠军。当你需要使用栈或者队列时，可以考虑使用LinkedList，一方面是因为Java官方已经声明不建议使用Stack类，更遗憾的是，Java里根本没有一个叫做Queue的类（它是个接口名字）。关于栈或队列，现在的首选是ArrayDeque，它有着比LinkedList（当作栈或队列使用时）有着更好的性能。 LinkedList的实现方式决定了所有跟下标相关的操作都是线性时间，而在首段或者末尾删除元素只需要常数时间。为追求效率LinkedList没有实现同步（synchronized），如果需要多个线程并发访问，可以先采用Collections.synchronizedList()方法对其进行包装。 实现底层数据接口 transient int size = 0; transient Node&lt;E> first; // 经常用到 transient Node&lt;E> last; // 也经常用到 // Node是私有的内部类 private static class Node&lt;E> { E item; Node&lt;E> next; Node&lt;E> prev; Node(Node&lt;E> prev, E element, Node&lt;E> next) { this.item = element; this.next = next; this.prev = prev; } } LinkedList底层通过双向链表实现，本节将着重讲解插入和删除元素时双向链表的维护过程，也即是之间解跟List接口相关的函数。双向链表的每个节点用内部类Node表示。LinkedList通过first和last引用分别指向链表的第一个和最后一个元素。注意这里没有所谓的哑元，当链表为空的时候first和last都指向null。 构造函数 public LinkedList() { } public LinkedList(Collection&lt;? extends E> c) { this(); addAll(c); } getFirst()，getLast()本身在数据结构中，维护了first和last的变量，因此其实挺简单的。 public E getFirst() { final Node&lt;E> f = first; // 获取第一个元素 if (f == null) throw new NoSuchElementException(); return f.item; } public E getLast() { final Node&lt;E> l = last; // 获取最后一个元素 if (l == null) throw new NoSuchElementException(); return l.item; } removeFirst()，removeLast()，remove(e)，remove(index) 删除元素 - 指的是删除第一次出现的这个元素, 如果没有这个元素，则返回false；判读的依据是equals方法， 如果equals，则直接unlink这个node；由于LinkedList可存放null元素，故也可以删除第一次出现null的元素； public boolean remove(Object o) { if (o == null) { for (Node&lt;E> x = first; x != null; x = x.next) { if (x.item == null) { unlink(x); return true; } } } else { for (Node&lt;E> x = first; x != null; x = x.next) { if (o.equals(x.item)) { // 循环遍历 用equals判断 unlink(x); return true; } } } return false; } E unlink(Node&lt;E> x) { // assert x != null; final E element = x.item; // 当前元素 final Node&lt;E> next = x.next; // 指向下一个节点 final Node&lt;E> prev = x.prev; // 上一个节点 if (prev == null) {// 第一个元素，如果该节点的上节点为空，那么就把该节点的下个节点放在第一个位置 first = next; } else { prev.next = next; // 不为空，则把上个节点指向该节点的下个节点 x.prev = null; } if (next == null) {// 最后一个元素 last = prev; } else { next.prev = prev; x.next = null; } x.item = null; // GC size--; modCount++; return element; } remove(int index)使用的是下标计数， 只需要判断该index是否有元素即可，如果有则直接unlink这个node。 public E remove(int index) { checkElementIndex(index); return unlink(node(index)); } removeFirst()其实挺简单的 public E removeFirst() { final Node&lt;E> f = first; // 拿到firs直接unlink if (f == null) throw new NoSuchElementException(); return unlinkFirst(f); } private E unlinkFirst(Node&lt;E> f) { // assert f == first &amp;&amp; f != null; final E element = f.item; // first e final Node&lt;E> next = f.next; // first 没有 pre ， 只有next f.item = null; f.next = null; // help GC first = next; // 让first指向next if (next == null) // 如果next为空，则当前元素已经是最后一个元素了，那么last自然为空 last = null; else next.prev = null; // 如果不为空，next的上个节点指向为空 size--; modCount++; return element; } removLast()其实挺简单的，和上面差不多 public E removeLast() { final Node&lt;E> l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l); } private E unlinkLast(Node&lt;E> l) { // assert l == last &amp;&amp; l != null; final E element = l.item; final Node&lt;E> prev = l.prev; l.item = null; l.prev = null; // help GC last = prev; if (prev == null) first = null; else prev.next = null; size--; modCount++; return element; } add() public boolean add(E e) { linkLast(e); // 在链表末尾插入元素，所以常数时间 return true; } void linkLast(E e) { // 其实就是最后面修改引用 final Node&lt;E> l = last; final Node&lt;E> newNode = new Node&lt;>(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++; } add(int index, E element), 当index==size时，等同于add(E e); 如果不是，则分两步：1.先根据index找到要插入的位置,即node(index)方法；2.修改引用，完成插入操作，其实想就是遍历插入。 indexOf()循环遍历equals，找到对应的下标 public int indexOf(Object o) { int index = 0; // 维护index if (o == null) { for (Node&lt;E> x = first; x != null; x = x.next) { if (x.item == null) return index; index++; } } else { for (Node&lt;E> x = first; x != null; x = x.next) { if (o.equals(x.item)) // 用equals return index; index++; } } return -1; } HashMap(面试常问)众所周知，HashMap的底层结构是数组和链表组成的，不过在jdk1.7和jdk1.8中具体实现略有不同。 1.7的实现成员变量介绍成员变量： 初始化桶大小，因为底层是数组，所以这是数组默认的大小。 桶最大值。 默认的负载因子（0.75） table真正存放数据的数组。 map存放数量的大小 桶大小，可在构造函数时显式指定。 负载因子，可在构造函数时显式指定。 负载因子public HashMap() { this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR); // 桶和负载因子 } public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity > MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // 只能获取默认的最大值 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; in 给定的默认容量为16，负载因子为0.75. Map在使用过程中不断的往里面存放数据，当数量达到了16 * 0.75 = 12就需要将当前16的容量进行扩容，而扩容这个过程涉及到rehash（重新哈希）、复制数据等操作，所有非常消耗性能。 因此通常建议能提前预估HashMap的大小最好，尽量的减少扩容带来的额外性能损耗。 关于这部分后期专门出一篇文章进行讲解。 EntryEntry是Hashmap中的一个内部类，从他的成员变量很容易看出： key就是写入时的键 value自然就是值 开始的时候就提到HashMap是由数组和链表组成，所以这个next就是用于实现链表结构 hash存放的是当前key的hashcode put(重点来了)public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); // 判断数组是否需要初始化 } if (key == null) return putForNullKey(value); // 判断key是否为空 int hash = hash(key); // 计算hashcode int i = indexFor(hash, table.length); // 计算桶 for (Entry&lt;K,V> e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { // 遍历判断链表中的key和hashcode是否相等，等就替换 V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); // 没有就添加新的呗 return null; } 判断当前数组是否需要初始化 如果key为空，则put一个空值进去 根据key计算hashcode 根据计算的hashcode定位index的桶 如果桶是一个链表，则需要遍历判断里面的hashcode、key是否和传入的key相等，如果相等则进行覆盖，并返回原来的值 如果桶是空的，说明当前位置没有数据存入，此时新增一个Entry对象写入当前位置。 void addEntry(int hash, K key, V value, int bucketIndex) { if ((size >= threshold) &amp;&amp; (null != table[bucketIndex])) {// 是否扩容 resize(2 * table.length); // 两倍扩容 重新哈希 hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex); } void createEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V> e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;>(hash, key, value, e); size++; } 当调用addEntry写入Entry时需要判断是否需要扩容 如果需要就进行两倍扩充，并将当前的key重新hash并定位。 而在createEntry中会将当前位置的桶传入到新建的桶中，如果当前桶有值就会在位置形成链表。 getpublic V get(Object key) { if (key == null) // 判断key是否为空 return getForNullKey(); // 为空，就返回空值 Entry&lt;K,V> entry = getEntry(key); // get entry return null == entry ? null : entry.getValue(); } final Entry&lt;K,V> getEntry(Object key) { if (size == 0) { return null; } int hash = (key == null) ? 0 : hash(key); //根据key和hashcode for (Entry&lt;K,V> e = table[indexFor(hash, table.length)]; //循环遍历equals key拿值 e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } return null; } 首先根据key计算hashcode，然后定位具体的桶 判断该位置是否为链表 不是链接就根据key和hashcode是否相等来返回值 为链表则需要遍历直到key和hashcode相等就返回值 啥都没得，就返回null 1.8的实现不知道 1.7 的实现大家看出需要优化的点没有？ 其实一个很明显的地方就是链表 当 Hash 冲突严重时，在桶上形成的链表会变的越来越长，这样在查询时的效率就会越来越低；时间复杂度为 O(N)。 成员变量static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /** * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; static final int TREEIFY_THRESHOLD = 8; transient Node&lt;K,V>[] table; /** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */ transient Set&lt;Map.Entry&lt;K,V>> entrySet; /** * The number of key-value mappings contained in this map. */ transient int size; TREEIFY_THRESHOLD 用于判断是否需要将链表转换为红黑树的阈值。 HashEntry 修改为 Node。 Node 的核心组成其实也是和 1.7 中的 HashEntry 一样，存放的都是 key value hashcode next 等数据。 put /** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V>[] tab; Node&lt;K,V> p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) // 1. 判断当前桶是否为空，空的就需要初始化（resize中会判断是否进行初始化） n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) // 2. 根据当前key的hashcode定位到具体的桶中并判断是否为空，为空则表明没有Hash冲突，就直接在当前位置创建一个新桶 tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V> e; K k; if (p.hash == hash &amp;&amp; // 3. 如果当前桶有值（Hash冲突），那么就要比较当前桶中的key、key的hashcode与写入的key是否相等，相等就赋值给e，在第8步的时候会统一进行赋值及返回 ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) // 4. 如果当前桶为红黑树，那就要按照红黑树的方式写入数据 e = ((TreeNode&lt;K,V>)p).putTreeVal(this, tab, hash, key, value); else { // 5. 如果是个链表，就需要将当前的key、value封装称一个新节点写入到当前桶的后面形成链表。 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st // 6. 接着判断当前链表的大小是否大于预设的阈值，大于就要转换成为红黑树 - treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; // 7. 如果在遍历过程中找到key相同时直接退出遍历。 ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key 8. 如果`e != null`就相当于存在相同的key，那就需要将值覆盖。 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size > threshold) // 9. 最后判断是否需要进行扩容。 resize(); afterNodeInsertion(evict); return null; } 判断当前桶是否为空，空的就需要初始化（resize中会判断是否进行初始化） 根据当前key的hashcode定位到具体的桶中并判断是否为空，为空则表明没有Hash冲突，就直接在当前位置创建一个新桶 如果当前桶有值（Hash冲突），那么就要比较当前桶中的key、key的hashcode与写入的key是否相等，相等就赋值给e，在第8步的时候会统一进行赋值及返回 如果当前桶为红黑树，那就要按照红黑树的方式写入数据 如果是个链表，就需要将当前的key、value封装称一个新节点写入到当前桶的后面形成链表。 接着判断当前链表的大小是否大于预设的阈值，大于就要转换成为红黑树 如果在遍历过程中找到key相同时直接退出遍历。 如果e != null就相当于存在相同的key，那就需要将值覆盖。 最后判断是否需要进行扩容。 getpublic V get(Object key) { Node&lt;K,V> e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node&lt;K,V> getNode(int hash, Object key) { Node&lt;K,V>[] tab; Node&lt;K,V> first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) > 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode&lt;K,V>)first).getTreeNode(hash, key); do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } 首先将key hash之后取得所定位的桶 如果桶为空，则直接返回null 否则判断桶的第一个位置（有可能是链表、红黑树）的key是否为查询的key，是就直接返回value 如果第一个不匹配，则判断它的下一个是红黑树还是链表 红黑树就按照树的查找方式返回值 不然就按照链表的方式遍历匹配返回值 从这两个核心方法（get/put）可以看出 1.8 中对大链表做了优化，修改为红黑树之后查询效率直接提高到了 O(logn)。 问题但是 HashMap 原有的问题也都存在，比如在并发场景下使用时容易出现死循环。 final HashMap&lt;String, String> map = new HashMap&lt;String, String>(); for (int i = 0; i &lt; 1000; i++) { new Thread(new Runnable() { @Override public void run() { map.put(UUID.randomUUID().toString(), \"\"); } }).start(); } HashMap扩容的时候会调用resize()方法，就是这里的并发操作容易在一个桶上形成环形链表 这样当获取一个不存在的key时，计算出的index正好是环形链表的下标就会出现死循环。 但是1.7的头插法造成的问题，1.8改变了插入顺序，就解决了这个问题，但是为了内存可见性等安全性，还是需要ConCurrentHashMap 参考：hashMap死循环分析 hashMap死循环分析 还有一个值得注意的是 HashMap 的遍历方式，通常有以下几种： Iterator&lt;Map.Entry&lt;String, Integer>> entryIterator = map.entrySet().iterator(); while (entryIterator.hasNext()) { Map.Entry&lt;String, Integer> next = entryIterator.next(); System.out.println(\"key=\" + next.getKey() + \" value=\" + next.getValue()); } Iterator&lt;String> iterator = map.keySet().iterator(); while (iterator.hasNext()){ String key = iterator.next(); System.out.println(\"key=\" + key + \" value=\" + map.get(key)); } 建议使用第一种，同时可以把key value取出。 第二种还需要通过key取一次key，效率较低。 ConcurrentHashMap1.7 Segment数组 HashEntry组成 和HashMap一样，仍然是数组加链表 /** * Segment 数组，存放数据时首先需要定位到具体的 Segment 中。 */ final Segment&lt;K,V>[] segments; transient Set&lt;K> keySet; transient Set&lt;Map.Entry&lt;K,V>> entrySet; Segment 是 ConcurrentHashMap 的一个内部类，主要的组成如下： static final class Segment&lt;K,V> extends ReentrantLock implements Serializable { private static final long serialVersionUID = 2249069246763182397L; // 和 HashMap 中的 HashEntry 作用一样，真正存放数据的桶 transient volatile HashEntry&lt;K,V>[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor; } 唯一的区别就是其中的核心数据如 value ，以及链表都是 volatile 修饰的，保证了获取时的可见性。 ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。 不会像HashTable那样不管是put还是get操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。 每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 putpublic V put(K key, V value) { Segment&lt;K,V> s; if (value == null) throw new NullPointerException(); int hash = hash(key); int j = (hash >>> segmentShift) &amp; segmentMask; if ((s = (Segment&lt;K,V>)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); return s.put(key, hash, value, false); } 通过key定位到Segment，之后在对应的Segment中进行具体的put final V put(K key, int hash, V value, boolean onlyIfAbsent) { HashEntry&lt;K,V> node = tryLock() ? null : scanAndLockForPut(key, hash, value); // 1. 加锁处理 V oldValue; try { HashEntry&lt;K,V>[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V> first = entryAt(tab, index); for (HashEntry&lt;K,V> e = first;;) { if (e != null) { K k; if ((k = e.key) == key || // 2. 遍历该HashEntry，如果不为空则判断传入的key和当前遍历的key是否相等，相等则覆盖旧的value (e.hash == hash &amp;&amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { e.value = value; ++modCount; } break; } e = e.next; } else { // 3. 不为空则需要新建一个HashEntry并加入到Segment中，同时会先判断是否需要扩容 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V>(hash, key, value, first); int c = count + 1; if (c > threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; } } } finally { unlock(); // 4. 解锁 } return oldValue; } 虽然HashEntry中的value是用volatile关键字修饰的，但是并不能保证并发的原子性，所以put操作仍然需要加锁处理。 首先第一步的时候会尝试获取锁，如果获取失败肯定就是其他线程存在竞争，则利用 scanAndLockForPut() 自旋获取锁。 尝试获取自旋锁 如果重试的次数达到了MAX_SCAN_RETRIES 则改为阻塞锁获取，保证能获取成功。 总的来说： 将当前的Segment中的table通过key的hashcode定位到HashEntry 遍历该HashEntry，如果不为空则判断传入的key和当前遍历的key是否相等，相等则覆盖旧的value 不为空则需要新建一个HashEntry并加入到Segment中，同时会先判断是否需要扩容 最后会解除在1中所获取当前Segment的锁。 getpublic V get(Object key) { Segment&lt;K,V> s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V>[] tab; int h = hash(key); long u = (((h >>> segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; if ((s = (Segment&lt;K,V>)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) { for (HashEntry&lt;K,V> e = (HashEntry&lt;K,V>) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) { K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; } } return null; } 只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。 由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。 ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。 1.8那就是查询遍历链表效率太低。 其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性 put final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V>[] tab = table;;) { // 1. 根据key计算出hashcode Node&lt;K,V> f; int n, i, fh; if (tab == null || (n = tab.length) == 0) // 2. 判断是否需要进行初始化 tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { // 3. f即为当前key定位出的Node，如果为空表示当前位置可以写入数据，利用CAS尝试写入，失败则自旋保证成功。 if (casTabAt(tab, i, null, new Node&lt;K,V>(hash, key, value, null))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) // 4. 如果当前位置的`hashcode == MOVED == -1`，则需要进行扩容 tab = helpTransfer(tab, f); else { V oldVal = null; synchronized (f) { // 5. 如果都不满足，则利用synchronized锁写入数据 if (tabAt(tab, i) == f) { if (fh >= 0) { binCount = 1; for (Node&lt;K,V> e = f;; ++binCount) { K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node&lt;K,V> pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V>(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { Node&lt;K,V> p; binCount = 2; if ((p = ((TreeBin&lt;K,V>)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { if (binCount >= TREEIFY_THRESHOLD) // 6. 如果数量大于`TREEIFY_THRESHOLD` 则要转换为红黑树。 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null; } 根据key计算出hashcode 判断是否需要进行初始化 f即为当前key定位出的Node，如果为空表示当前位置可以写入数据，利用CAS尝试写入，失败则自旋保证成功。 如果当前位置的hashcode == MOVED == -1，则需要进行扩容 如果都不满足，则利用synchronized锁写入数据 如果数量大于TREEIFY_THRESHOLD 则要转换为红黑树。 get public V get(Object key) { Node&lt;K,V>[] tab; Node&lt;K,V> e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) > 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) { if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; } else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) { if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; } } return null; } 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 就不满足那就按照链表的方式遍历获取值。 1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。 总结套路： 谈谈你理解的 HashMap，讲讲其中的 get put 过程。 1.8 做了什么优化？ 是线程安全的嘛？ 不安全会导致哪些问题？ 如何解决？有没有线程安全的并发容器？ ConcurrentHashMap 是如何实现的？ 1.7、1.8 实现有何不同？为什么这么做？ 创作不易哇，觉得有帮助的话，给个小小的star呗。github地址😁😁😁","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"个人吐血系列-总结Java基础","slug":"个人吐血系列-总结Java基础","date":"2020-03-27T07:07:33.000Z","updated":"2021-01-01T09:51:02.670Z","comments":true,"path":"2020/03/27/ge-ren-tu-xie-xi-lie-zong-jie-java-ji-chu/","link":"","permalink":"http://dreamcat.ink/2020/03/27/ge-ren-tu-xie-xi-lie-zong-jie-java-ji-chu/","excerpt":"","text":"大纲图 简述线程、程序、进程的基本概念。以及他们之间关系是什么?线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 程序是含有指令和数据的文件，被存储在磁盘或其他的数据存储设备中，也就是说程序是静态的代码。 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。 线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。 Java 语言有哪些特点? 简单易学； 面向对象（封装，继承，多态）； 平台无关性（ Java 虚拟机实现平台无关性）； 可靠性； 安全性； 支持多线程（ C++ 语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 语言却提供了多线程支持）； 支持网络编程并且很方便（ Java 语言诞生本身就是为简化网络编程设计的，因此 Java 语言不仅支持网络编程而且很方便）； 编译与解释并存； 面向对象和面向过程的区别 面向过程 ：面向过程性能比面向对象高。 因为类调用时需要实例化，开销比较大，比较消耗资源，所以当性能是最重要的考量因素的时候，比如单片机、嵌入式开发、Linux/Unix等一般采用面向过程开发。但是，面向过程没有面向对象易维护、易复用、易扩展。 面向对象 ：面向对象易维护、易复用、易扩展。 因为面向对象有封装、继承、多态性的特性，所以可以设计出低耦合的系统，使系统更加灵活、更加易于维护。但是，面向对象性能比面向过程低。 这个并不是根本原因，面向过程也需要分配内存，计算内存偏移量，Java性能差的主要原因并不是因为它是面向对象语言，而是Java是半编译语言，最终的执行代码并不是可以直接被CPU执行的二进制机械码。 而面向过程语言大多都是直接编译成机械码在电脑上执行，并且其它一些面向过程的脚本语言性能也并不一定比Java好。 ==、hashcode和equals==它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。 equals() 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来比较两个对象的内容是否相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。 public class test1 { public static void main(String[] args) { String a = new String(\"ab\"); // a 为一个引用 String b = new String(\"ab\"); // b为另一个引用,对象的内容一样 String aa = \"ab\"; // 放在常量池中 String bb = \"ab\"; // 从常量池中查找 if (aa == bb) // true System.out.println(\"aa==bb\"); if (a == b) // false，非同一对象 System.out.println(\"a==b\"); if (a.equals(b)) // true System.out.println(\"aEQb\"); if (42 == 42.0) { // true System.out.println(\"true\"); } } } String 中的 equals 方法是被重写过的，因为 object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。 hashcodehashcode:hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在JDK的Object.java中，这就意味着Java中的任何类都包含有hashCode() 函数。 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） 为什么要有hashcode我们先以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的Java启蒙书《Head first java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 通过我们可以看出：hashCode() 的作用就是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()在散列表中才有用，在其它情况下没用。在散列表中hashCode() 的作用是获取对象的散列码，进而确定该对象在散列表中的位置。 hashcode和equals的相关规定 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个对象分别调用equals方法都返回true 两个对象有相同的hashcode值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） JVM JDK 和 JRE 最详细通俗的解答JVMJava虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。 在 Java 中，JVM可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java程序无须重新编译便可在多种不同操作系统的计算机上运行。 JDK和JREJDK是Java Development Kit，它是功能齐全的Java SDK。它拥有JRE所拥有的一切，还有编译器（javac）和工具（如javadoc和jdb）。它能够创建和编译程序。 JRE 是 Java运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java虚拟机（JVM），Java类库，java命令和其他的一些基础构件。但是，它不能用于创建新程序。 如果你只是为了运行一下 Java 程序的话，那么你只需要安装 JRE 就可以了。如果你需要进行一些 Java 编程方面的工作，那么你就需要安装JDK了。但是，这不是绝对的。有时，即使您不打算在计算机上进行任何Java开发，仍然需要安装JDK。例如，如果要使用JSP部署Web应用程序，那么从技术上讲，您只是在应用程序服务器中运行Java程序。那你为什么需要JDK呢？因为应用程序服务器会将 JSP 转换为 Java servlet，并且需要使用 JDK 来编译 servlet。 Java和C++的区别? 都是面向对象的语言，都支持封装、继承和多态 Java 不提供指针来直接访问内存，程序内存更加安全 Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。 Java 有自动内存管理机制，不需要程序员手动释放无用内存 基本类型字符型常量和字符串常量的区别? 形式上: 字符常量是单引号引起的一个字符; 字符串常量是双引号引起的若干个字符 含义上: 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置) 占内存大小 字符常量只占2个字节; 字符串常量占若干个字节(至少一个字符结束标志) (注意： char在Java中占两个字节) 自动装箱与拆箱 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 说说&amp;和&amp;&amp;的区别 &amp;和&amp;&amp;都可以用作逻辑与的运算符，表示逻辑与（and） 当运算符两边的表达式的结果都为 true 时， 整个运算结果才为 true，否则，只要有一方为 false，则结果为 false。 &amp;&amp;还具有短路的功能，即如果第一个表达式为 false，则不再计算第二个表达式 &amp;还可以用作位运算符，当&amp;操作符两边的表达式不是 boolean 类型时，&amp;表示按位与操作，我们通常 使用 0x0f 来与一个整数进行&amp;运算，来获取该整数的最低 4 个 bit 位 short s1 = 1; s1 = s1 + 1;有什么错? short s1 = 1; s1 += 1; 有什么错? 对于 short s1 = 1; s1 = s1 + 1; 由于 s1+1 运算时会自动提升表达式的类型，所以结果是 int 型，再赋值 给 short 类型 s1 时，编译器将报告需要强制转换类型的错误。 对于 short s1 = 1; s1 += 1;由于 += 是 java 语言规定的运算符，java 编译器会对它进行特殊处理，因此 可以正确编译。 public class TypeConvert { public static void main(String[] args) { // 字面量属于 double 类型 // 不能直接将 1.1 直接赋值给 float 变量，因为这是向下转型 // Java 不能隐式执行向下转型，因为这会使得精度降低。 // float f = 1.1; float f = 1.1f; // 因为字面量 1 是 int 类型，它比 short 类型精度要高，因此不能隐式地将 int 类型下转型为 short 类型。 short s1 = 1; // s1 = s1 + 1; // 但是使用 += 运算符可以执行隐式类型转换。 s1 += 1; // 上面的语句相当于将 s1 + 1 的计算结果进行了向下转型: s1 = (short) (s1 + 1); } } char 型变量中能不能存贮一个中文汉字?为什么? char 型变量是用来存储 Unicode 编码的字符的，unicode 编码字符集中包含了汉字，所以，char 型变量 中当然可以存储汉字啦。 如果某个特殊的汉字没有被包含在 unicode 编码字符集中，那么，这个 char 型变量中就不能存储这个特殊汉字。 unicode 编码占用两个字节，所以，char 类型的变量也是占 用两个字节。 整形包装类缓存池public class IntegerPackDemo { public static void main(String[] args) { Integer x = 3; // 装箱 int z = x; // 拆箱 Integer y = 3; System.out.println(x == y); // true // ------------------------- Integer a = new Integer(3); Integer b = new Integer(3); System.out.println(a == b); // false 老生常谈了，就不说为什么了 System.out.println(a.equals.(b)); // true // 这里是用重写了equals方法，比较的是值，而不是对象的地址 // ------------------------ // 缓存池 Integer aa = Integer.valueOf(123); Integer bb = Integer.valueOf(123); System.out.println(aa == bb); // true /** * valueOf的源码 * public static Integer valueOf(int i) { * // 判断是否在Integer的范围内 * if (i >= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) * return IntegerCache.cache[i + (-IntegerCache.low)]; * return new Integer(i); * } */ } } 当使用自动装箱方式创建一个Integer对象时，当数值在-128 ~127时，会将创建的 Integer 对象缓存起来，当下次再出现该数值时，直接从缓存中取出对应的Integer对象。所以上述代码中，x和y引用的是相同的Integer对象。 面向对象Java 面向对象编程三大特性: 封装 继承 多态封装封装把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。 继承继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。 多态所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。 在Java中有两种形式可以实现多态：继承（多个子类对同一方法的重写）和接口（实现接口并覆盖接口中同一方法）。 构造器 Constructor 是否可被 override?在讲继承的时候我们就知道父类的私有属性和构造方法并不能被继承，所以 Constructor 也就不能被 override（重写）,但是可以 overload（重载）,所以你可以看到一个类中有多个构造函数的情况。 构造方法有哪些特性 名字与类名相同。 没有返回值，但不能用void声明构造函数。 生成类的对象时自动执行，无需调用。 接口和抽象类的区别是什么？ 接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），而抽象类可以有非抽象的方法。 接口中除了static、final变量，不能有其他变量，而抽象类中则不一定。 一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过implement关键字扩展多个接口。 接口方法默认修饰符是public，抽象方法可以有public、protected和default这些修饰符（抽象方法就是为了被重写所以不能使用private关键字修饰！）。 从设计层面来说，抽象是对类的抽象，是一种模板设计，而接口是对行为的抽象，是一种行为的规范。 备注：在JDK8中，接口也可以定义静态方法，可以直接用接口名调用。实现类和实现是不可以调用的。如果同时实现两个接口，接口中定义了一样的默认方法，则必须重写，不然会报错。 成员变量与局部变量的区别有哪些？ 从语法形式上看:成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被 public,private,static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。 从变量在内存中的存储方式来看:如果成员变量是使用static修饰的，那么这个成员变量是属于类的，如果没有使用static修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。 从变量在内存中的生存时间上看:成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。 成员变量如果没有被赋初值:则会自动以类型的默认值而赋值（一种情况例外:被 final 修饰的成员变量也必须显式地赋值），而局部变量则不会自动赋值。 重载和重写的区别重载发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。 重写重写是子类对父类的允许访问的方法的实现过程进行重新编写,发生在子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类。另外，如果父类方法访问修饰符为 private 则子类就不能重写该方法。也就是说方法提供的行为改变，而方法的外貌并没有改变。 创建一个对象用什么运算符?对象实体与对象引用有何不同?new运算符，new创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。一个对象引用可以指向0个或1个对象（一根绳子可以不系气球，也可以系一个气球）;一个对象可以有n个引用指向它（可以用n条绳子系住一个气球）。 对象的相等与指向他们的引用相等,两者有什么不同?对象的相等，比的是内存中存放的内容是否相等。而引用相等，比较的是他们指向的内存地址是否相等。 Java值传递按值调用(call by value)表示方法接收的是调用者提供的值，而按引用调用（call by reference)表示方法接收的是调用者提供的变量地址。一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。 接下来三个例子瞧一瞧： 基本类型传递 public static void main(String[] args) { int num1 = 10; int num2 = 20; swap(num1, num2); System.out.println(\"num1 = \" + num1); System.out.println(\"num2 = \" + num2); } public static void swap(int a, int b) { int temp = a; a = b; b = temp; System.out.println(\"a = \" + a); System.out.println(\"b = \" + b); } // a = 20 // b = 10 // num1 = 10 // num2 = 20 在 swap 方法中，a、b 的值进行交换，并不会影响到 num1、num2。因为，a、b 中的值，只是从 num1、num2 的复制过来的。也就是说，a、b 相当于 num1、num2 的副本，副本的内容无论怎么修改，都不会影响到原件本身。 接下来看数组： public static void main(String[] args) { int[] arr = { 1, 2, 3, 4, 5 }; System.out.println(arr[0]); change(arr); System.out.println(arr[0]); // 法得到的是对象引用的拷贝，对象引用及其他的拷贝同时引用同一个对象。 } private static void change(int[] array) { // 修改数组中的一个元素 array[0] = 0; } // 1 // 0 array 被初始化 arr 的拷贝也就是一个对象的引用，也就是说 array 和 arr 指向的是同一个数组对象。 因此，外部对引用对象的改变会反映到所对应的对象上。 通过 example2 我们已经看到，实现一个改变对象参数状态的方法并不是一件难事。理由很简单，方法得到的是对象引用的拷贝，对象引用及其他的拷贝同时引用同一个对象。 再看对象引用例子： public static void main(String[] args) { // 有些程序员（甚至本书的作者）认为 Java 程序设计语言对对象采用的是引用调用，实际上，这种理解是不对的。 Student s1 = new Student(\"Mai\"); Student s2 = new Student(\"Feng\"); swap2(s1, s2); System.out.println(\"s1:\" + s1.getName()); System.out.println(\"s2:\" + s2.getName()); // 方法并没有改变存储在变量 s1 和 s2 中的对象引用。 // swap 方法的参数 x 和 y 被初始化为两个对象引用的拷贝，这个方法交换的是这两个拷贝 } private static void swap2(Student x, Student y) { Student temp = x; x = y; y = temp; System.out.println(\"x:\" + x.getName()); System.out.println(\"y:\" + y.getName()); } // x:Feng // y:Mai // s1:Mai // s2:Feng 方法并没有改变存储在变量 s1 和 s2 中的对象引用。swap 方法的参数 x 和 y 被初始化为两个对象引用的拷贝，这个方法交换的是这两个拷贝 下面再总结一下 Java 中方法参数的使用情况： 一个方法不能修改一个基本数据类型的参数（即数值型或布尔型）。 一个方法可以改变一个对象参数的状态。 一个方法不能让对象参数引用一个新的对象。 StringString StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的?可变性简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以 String 对象是不可变的。而StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 线程安全性String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用String 单线程操作字符串缓冲区下操作大量数据: 适用StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用StringBuffer 代码例子 public static void main(String[] args) { // String String str = \"hello\"; long start = System.currentTimeMillis(); for (int i = 0; i &lt; 100000; i++) { str += i; // 创建多少个对象，， } System.out.println(\"String: \" + (System.currentTimeMillis() - start)); // StringBuffer StringBuffer sb = new StringBuffer(\"hello\"); long start1 = System.currentTimeMillis(); for (int i = 0; i &lt; 1000000; i++) { sb.append(i); } System.out.println(\"StringBuffer: \" + (System.currentTimeMillis() - start1)); // StringBuilder StringBuilder stringBuilder = new StringBuilder(\"hello\"); long start2 = System.currentTimeMillis(); for (int i = 0; i &lt; 1000000; i++) { stringBuilder.append(i); } System.out.println(\"StringBuilder: \" + (System.currentTimeMillis() - start2)); } String A = “123”; String B = new String(“123”);生成几个对象？如果常量池中，原来没有“123”那么就是生成了2个对象，如果常量池中有“123”那么只要1个对象生成 聊一聊String.intern()这个方法public class StringTest { public static void main(String[] args) { String str1 = \"todo\"; String str2 = \"todo\"; String str3 = \"to\"; String str4 = \"do\"; String str5 = str3 + str4; String str6 = new String(str1); System.out.println(\"------普通String测试结果------\"); System.out.print(\"str1 == str2 ? \"); System.out.println( str1 == str2); System.out.print(\"str1 == str5 ? \"); System.out.println(str1 == str5); System.out.print(\"str1 == str6 ? \"); System.out.print(str1 == str6); System.out.println(); System.out.println(\"---------intern测试结果---------\"); System.out.print(\"str1.intern() == str2.intern() ? \"); System.out.println(str1.intern() == str2.intern()); System.out.print(\"str1.intern() == str5.intern() ? \"); System.out.println(str1.intern() == str5.intern()); System.out.print(\"str1.intern() == str6.intern() ? \"); System.out.println(str1.intern() == str6.intern()); System.out.print(\"str1 == str6.intern() ? \"); System.out.println(str1 == str6.intern()); } } 运行结果： ------普通String测试结果------ str1 == str2 ? true str1 == str5 ? false str1 == str6 ? false ---------intern测试结果--------- str1.intern() == str2.intern() ? true str1.intern() == str5.intern() ? true str1.intern() == str6.intern() ? true str1 == str6.intern() ? true 普通String代码结果分析： Java语言会使用常量池保存那些在编译期就已确定的已编译的class文件中的一份数据。主要有类、接口、方法中的常量，以及一些以文本形式出现的符号引用，如类和接口的全限定名、字段的名称和描述符、方法和名称和描述符等。因此在编译完Intern类后，生成的class文件中会在常量池中保存“todo”、“to”和“do”三个String常量。变量str1和str2均保存的是常量池中“todo”的引用，所以str1==str2成立；在执行 str5 = str3 + str4这句时，JVM会先创建一个StringBuilder对象，通过StringBuilder.append()方法将str3与str4的值拼接，然后通过StringBuilder.toString()返回一个堆中的String对象的引用，赋值给str5，因此str1和str5指向的不是同一个String对象，str1 == str5不成立；String str6 = new String(str1)一句显式创建了一个新的String对象，因此str1 == str6不成立便是显而易见的事了。 intern代码结果分析： String.intern()是一个Native方法，底层调用C++的 StringTable::intern方法实现。当通过语句str.intern()调用intern()方法后，JVM 就会在当前类的常量池中查找是否存在与str等值的String，若存在则直接返回常量池中相应Strnig的引用；若不存在，则会在常量池中创建一个等值的String，然后返回这个String在常量池中的引用。因此，只要是等值的String对象，使用intern()方法返回的都是常量池中同一个String引用，所以，这些等值的String对象通过intern()后使用==是可以匹配的。由此就可以理解上面代码中——intern——部分的结果了。因为str1、str5和str6是三个等值的String，所以通过intern()方法，他们均会指向常量池中的同一个String引用，因此str1.intern() == str5.intern() == str6.intern()均为true。 jdk6Jdk6中常量池位于PermGen（永久代）中，PermGen是一块主要用于存放已加载的类信息和字符串池的大小固定的区域。执行intern()方法时，若常量池中不存在等值的字符串，JVM就会在常量池中创建一个等值的字符串，然后返回该字符串的引用。除此以外，JVM 会自动在常量池中保存一份之前已使用过的字符串集合。Jdk6中使用intern()方法的主要问题就在于常量池被保存在PermGen中：首先，PermGen是一块大小固定的区域，一般不同的平台PermGen的默认大小也不相同，大致在32M到96M之间。所以不能对不受控制的运行时字符串（如用户输入信息等）使用intern()方法，否则很有可能会引发PermGen内存溢出；其次String对象保存在Java堆区，Java堆区与PermGen是物理隔离的，因此如果对多个不等值的字符串对象执行intern操作，则会导致内存中存在许多重复的字符串，会造成性能损失。 jdk7Jdk7将常量池从PermGen区移到了Java堆区，执行intern操作时，如果常量池已经存在该字符串，则直接返回字符串引用，否则复制该字符串对象的引用到常量池中并返回。堆区的大小一般不受限，所以将常量池从PremGen区移到堆区使得常量池的使用不再受限于固定大小。除此之外，位于堆区的常量池中的对象可以被垃圾回收。当常量池中的字符串不再存在指向它的引用时，JVM就会回收该字符串。可以使用 -XX:StringTableSize 虚拟机参数设置字符串池的map大小。字符串池内部实现为一个HashMap，所以当能够确定程序中需要intern的字符串数目时，可以将该map的size设置为所需数目*2（减少hash冲突），这样就可以使得String.intern()每次都只需要常量时间和相当小的内存就能够将一个String存入字符串池中。 适用场景Jdk6中常量池位于PermGen区，大小受限，所以不建议适用intern()方法，当需要字符串池时，需要自己使用HashMap实现。Jdk7、8中，常量池由PermGen区移到了堆区，还可以通过-XX:StringTableSize参数设置StringTable的大小，常量池的使用不再受限，由此可以重新考虑使用intern()方法。intern(）方法优点：执行速度非常快，直接使用==进行比较要比使用equals(）方法快很多；内存占用少。虽然intern()方法的优点看上去很诱人，但若不是在恰当的场合中使用该方法的话，便非但不能获得如此好处，反而还可能会有性能损失。 关键字finalfinal关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。 final修饰有啥好处 final的关键字提高了性能，JVM和java应用会缓存final变量； final变量可以在多线程环境下保持线程安全； 使用final的关键字提高了性能，JVM会对方法变量类进行优化； static 修饰成员变量和成员方法: 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享，可以并且建议通过类名调用。被static 声明的成员变量属于静态成员变量，静态变量 存放在 Java 内存区域的方法区。调用格式：类名.静态变量名 类名.静态方法名() 静态代码块: 静态代码块定义在类中方法外, 静态代码块在非静态代码块之前执行(静态代码块—&gt;非静态代码块—&gt;构造方法)。 该类不管创建多少对象，静态代码块只执行一次. 静态内部类（static修饰类的话只能修饰内部类）： 静态内部类与非静态内部类之间存在一个最大的区别: 非静态内部类在编译完成之后会隐含地保存着一个引用，该引用是指向创建它的外围类，但是静态内部类却没有。没有这个引用就意味着：1. 它的创建是不需要依赖外围类的创建。2. 它不能使用任何外围类的非static成员变量和方法。 静态导包(用来导入类中的静态资源，1.5之后的新特性): 格式为：import static 这两个关键字连用可以指定导入某个类中的指定静态资源，并且不需要使用类名调用类中静态成员，可以直接使用类中静态成员变量和成员方法。 thisclass Manager { Employees[] employees; void manageEmployees() { int totalEmp = this.employees.length; System.out.println(\"Total employees: \" + totalEmp); this.report(); } void report() { } } 在上面的示例中，this关键字用于两个地方： this.employees.length：访问类Manager的当前实例的变量。 this.report（）：调用类Manager的当前实例的方法。 此关键字是可选的，这意味着如果上面的示例在不使用此关键字的情况下表现相同。 但是，使用此关键字可能会使代码更易读或易懂。 superpublic class Super { protected int number; protected showNumber() { System.out.println(\"number = \" + number); } } public class Sub extends Super { void bar() { super.number = 10; super.showNumber(); } } 在上面的例子中，Sub 类访问父类成员变量 number 并调用其其父类 Super 的 showNumber（） 方法。 使用 this 和 super 要注意的问题： 在构造器中使用 super（） 调用父类中的其他构造方法时，该语句必须处于构造器的首行，否则编译器会报错。另外，this 调用本类中的其他构造方法时，也要放在首行。 this、super不能用在static方法中。 简单解释一下： 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享。而 this 代表对本类对象的引用，指向本类对象；而 super 代表对父类对象的引用，指向父类对象；所以， this和super是属于对象范畴的东西，而静态方法是属于类范畴的东西。 final, finally, finalize 的区别 final 用于声明属性，方法和类，分别表示属性不可变，方法不可覆盖，类不可继承。 内部类要访问局部变量，局部变量必须定义成 final 类型 finally 是异常处理语句结构的一部分，表示总是执行。 finalize 是 Object 类的一个方法，在垃圾收集器执行的时候会调用被回收对象的此方法， 可以覆盖此方法提供垃圾收集时的其他资源回收，例如关闭文件等。JVM 不保证此方法总被调用 请说出作用域 public，private，protected 作用域 当前类 同package 子孙类 其他package public √ √ √ √ protected √ √ √ × friednly √ √ × × private √ × × × 异常处理在 Java 中，所有的异常都有一个共同的祖先java.lang包中的 Throwable类。Throwable： 有两个重要的子类：Exception（异常） 和 Error（错误） ，二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（Virtual MachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 Exception是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 异常由Java虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以0时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。 处理 try 块： 用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。 catch 块： 用于处理try捕获到的异常。 finally 块： 无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return 语句时，finally语句块将在方法返回之前被执行。 在以下4种特殊情况下，finally块不会被执行： 在finally语句块第一行发生了异常。 因为在其他行，finally块还是会得到执行 在前面的代码中用了System.exit(int)已退出程序。 exit是带参函数 ；若该语句在异常语句之后，finally会执行 程序所在的线程死亡。 关闭CPU。 注意： 当try语句和finally语句中都有return语句时，在方法返回之前，finally语句的内容将被执行，并且finally语句的返回值将会覆盖原始的返回值。如下： public static int f(int value) { try { return value * value; } finally { if (value == 2) { return 0; } } } 如果调用 f(2)，返回值将是0，因为finally语句的返回值覆盖了try语句块的返回值。 最常见到的 5 个 runtime exception RuntimeException NullPointerException、ArrayIndexOutOfBoundsException、ClassCastException。 Throwable图解 IO获取用键盘输入常用的两种方法方法1：通过 Scanner Scanner input = new Scanner(System.in); String s = input.nextLine(); input.close(); 方法2：通过 BufferedReader BufferedReader input = new BufferedReader(new InputStreamReader(System.in)); String s = input.readLine(); Java 中 IO 流分为几种? 按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。 Java Io流共涉及40多个类，这些类看上去很杂乱，但实际上很有规则，而且彼此之间存在非常紧密的联系， Java I0流的40多个类都是从如下4个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 既然有了字节流,为什么还要有字符流?问题本质想问：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？ 回答：字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以， I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。 BIO,NIO,AIO 有什么区别? BIO (Blocking I/O): 同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 NIO (New I/O): NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了NIO框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发 AIO (Asynchronous I/O): AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。 反射反射式什么？Java反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能成为java语言的反射机制。 静态编译和动态编译 静态编译：在编译时确定类型，绑定对象 动态编译：运行时确定类型，绑定对象 反射机制优缺点 优点：运行期间类型的判断，动态加载类，提高代码的灵活度。 缺点：性能瓶颈：反射相当于一系列解释操作，通知JVM要做的事情，性能比直接的java代码要慢很多。 反射的应用场景在我们平时的项目开发过程中，基本上很少会直接使用的反射机制，但这不能说明反射机制没有用，实际上有很多设计、开发都与反射机制有关，例如模块化的开发，通过反射去调用对应的字节码；动态代理设计模型也采用了反射机制，还有我们日常使用的Spring / Hibernate等框架也大量使用到了反射机制。 我们在使用JDBC连接数据库时使用Class.forName()通过反射加载数据看的驱动程序； Spring框架也用到很多反射机制，最经典的就是xml的配置模式。Spring通过XML配置模式装载Bean的过程； 将程序内所有XML或Properties配置文件加载入内存中； Java类里面解析xml或properties里面的内容，得到对应实体类的字节码字符串以及相关的属性信息； 使用反射机制，根据这个字符串获得某个类的Class实例 动态配置实例的属性 反射得到的Class对象的三种方式public class ReflectDemo { public static void main(String[] args) throws ClassNotFoundException { // 第一种方式获取Class对象 Student student = new Student(); // 这一new 产生一个Student对象，一个Class对象。 Class studentClass = student.getClass(); System.out.println(studentClass.getName()); // com.reflect.Student // 第二种方式获取Class对象 Class studentClass2 = Student.class; System.out.println(studentClass == studentClass2); //判断第一种方式获取的Class对象和第二种方式获取的是否是同一个 //第三种方式获取Class对象 Class studentClass3 = Class.forName(\"com.reflect.Student\"); //注意此字符串必须是真实路径，就是带包名的类路径，包名.类名 System.out.println(studentClass3 == studentClass2); // //判断三种方式是否获取的是同一个Class对象 // 三种方式常用第三种，第一种对象都有了还要反射干什么。 // 第二种需要导入类的包，依赖太强，不导包就抛编译错误。 // 一般都第三种，一个字符串可以传入也可写在配置文件中等多种方法。 } } 反射访问并调用构造方法public class ConstructorsDemo { public static void main(String[] args) throws Exception { // 1. 加载Class对象 Class clazz = Class.forName(\"com.reflect.Student\"); // 2. 获取所有公有构造方法 System.out.println(\"**********************所有公有构造方法*********************************\"); Constructor[] constructors = clazz.getConstructors(); for (Constructor constructor : constructors) { System.out.println(constructor); } // 3. System.out.println(\"************所有的构造方法(包括：私有、受保护、默认、公有)***************\"); Constructor[] declaredConstructors = clazz.getDeclaredConstructors(); for (Constructor declaredConstructor : declaredConstructors) { System.out.println(declaredConstructor); } // 4. System.out.println(\"*****************获取公有、无参的构造方法*******************************\"); Constructor constructor = clazz.getConstructor(); System.out.println(constructor); // 调用构造方法 Object object = constructor.newInstance(); System.out.println(object); // System.out.println(\"******************获取私有构造方法，并调用*******************************\"); Constructor constructor1 = clazz.getDeclaredConstructor(char.class); System.out.println(constructor1); // 调用构造方法 constructor1.setAccessible(true); // 暴力访问 Object object2 = constructor1.newInstance('买'); System.out.println(object2); } } 反射访问并调用成员变量public class FieldDemo { public static void main(String[] args) throws Exception { // 1. 获取class对象 Class clazz = Class.forName(\"com.reflect.Student\"); // 2. 获取所有字段 System.out.println(\"************获取所有公有的字段********************\"); Field[] fields = clazz.getFields(); for (Field field : fields) { System.out.println(field); } // System.out.println(\"************获取所有的字段(包括私有、受保护、默认的)********************\"); Field[] fields1 = clazz.getDeclaredFields(); for (Field field : fields1) { System.out.println(field); } // System.out.println(\"*************获取公有字段**并调用***********************************\"); Field gender = clazz.getField(\"gender\"); System.out.println(gender); // 获取一个对象 Object o = clazz.getConstructor().newInstance(); gender.set(o, \"男\"); Student stu = (Student) o; System.out.println(\"验证性别：\" + stu.getGender()); // System.out.println(\"*************获取公有字段**并调用***********************************\"); Field name = clazz.getDeclaredField(\"name\"); System.out.println(name); name.setAccessible(true); //暴力反射，解除私有限定 name.set(o, \"买\"); System.out.println(\"验证姓名：\" + stu); } } 反射访问并调用成员方法public class MethodDemo { public static void main(String[] args) throws Exception { // 1. 获取对象 Class clazz = Class.forName(\"com.reflect.Student\"); // 2. 获取所有公有方法 System.out.println(\"***************获取所有的”公有“方法*******************\"); Method[] methods = clazz.getMethods(); for (Method method : methods) { System.out.println(method); } System.out.println(\"***************获取所有的方法，包括私有的*******************\"); Method[] declaredMethods = clazz.getDeclaredMethods(); for (Method declaredMethod : declaredMethods) { System.out.println(declaredMethod); } System.out.println(\"***************获取公有的show1()方法*******************\"); Method m = clazz.getMethod(\"show1\", String.class); System.out.println(m); // 实例化对象 Object o = clazz.getConstructor().newInstance(); m.invoke(o, \"买\"); System.out.println(\"***************获取私有的show4()方法******************\"); m = clazz.getDeclaredMethod(\"show4\", int.class); System.out.println(m); m.setAccessible(true); // 暴力解除 私有 Object result = m.invoke(o, 20);//需要两个参数，一个是要调用的对象（获取有反射），一个是实参 System.out.println(\"返回值：\" + result); } } 深拷贝VS浅拷贝 浅拷贝：对基本数据类型进行值传递，对引用数据类型进行引用传递般的拷贝，此为浅拷贝。 深拷贝：对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容，此为深拷贝。 浅拷贝例子public class ShallowCopyDemo { public static void main(String[] args) throws CloneNotSupportedException { // 原始对象 Student student = new Student(new Subject(\"买\"), \"峰\"); System.out.println(\"原始对象: \" + student.getName() + \" - \" + student.getSubject().getName()); // 拷贝对象 Student cloneStu = (Student) student.clone(); System.out.println(\"拷贝对象: \" + cloneStu.getName() + \" - \" + cloneStu.getSubject().getName()); // 原始对象和拷贝对象是否一样： System.out.println(\"原始对象和拷贝对象是否一样: \" + (student == cloneStu)); // 原始对象和拷贝对象的name属性是否一样 System.out.println(\"原始对象和拷贝对象的name属性是否一样: \" + (student.getName() == cloneStu.getName())); // 原始对象和拷贝对象的subj属性是否一样 System.out.println(\"原始对象和拷贝对象的subj属性是否一样: \" + (student.getSubject() == cloneStu.getSubject())); student.setName(\"小\"); student.getSubject().setName(\"疯\"); System.out.println(\"更新后的原始对象: \" + student.getName() + \" - \" + student.getSubject().getName()); System.out.println(\"更新原始对象后的克隆对象: \" + cloneStu.getName() + \" - \" + cloneStu.getSubject().getName()); // 在这个例子中，让要拷贝的类Student实现了Clonable接口并重写Object类的clone()方法，然后在方法内部调用super.clone()方法。 // 从输出结果中我们可以看到，对原始对象stud的\"name\"属性所做的改变并没有影响到拷贝对象clonedStud； // 但是对引用对象subj的\"name\"属性所做的改变影响到了拷贝对象clonedStud。 } } 结果如下： 原始对象: 峰 - 买 拷贝对象: 峰 - 买 原始对象和拷贝对象是否一样: false 原始对象和拷贝对象的name属性是否一样: true 原始对象和拷贝对象的subj属性是否一样: true 更新后的原始对象: 小 - 疯 更新原始对象后的克隆对象: 峰 - 疯可以看到，浅拷贝中的引用类型并没有拷贝一份，都指向同一个对象。 深拷贝例子 // 重写 student的clone方法，例子还是如上。 @Override protected Object clone() throws CloneNotSupportedException { // 浅拷贝 // return super.clone(); // 深拷贝 Student student = new Student(new Subject(subject.getName()), name); return student; // 因为它是深拷贝，所以你需要创建拷贝类的一个对象。 // 因为在Student类中有对象引用，所以需要在Student类中实现Cloneable接口并且重写clone方法。 } 结果如下： 原始对象: 峰 - 买 拷贝对象: 峰 - 买 原始对象和拷贝对象是否一样: false 原始对象和拷贝对象的name属性是否一样: true 原始对象和拷贝对象的subj属性是否一样: false 更新后的原始对象: 小 - 疯 更新原始对象后的克隆对象: 峰 - 买可以看到，浅拷贝中的引用类型创建一份新对象。 创作不易哇，觉得有帮助的话，给个小小的star呗。github地址😁😁😁","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"java基础","slug":"java基础","permalink":"http://dreamcat.ink/tags/java%E5%9F%BA%E7%A1%80/"}]},{"title":"个人吐血系列-总结Java多线程","slug":"个人吐血系列-总结Java多线程","date":"2020-03-25T15:07:33.000Z","updated":"2021-01-01T09:52:41.700Z","comments":true,"path":"2020/03/25/ge-ren-tu-xie-xi-lie-zong-jie-java-duo-xian-cheng/","link":"","permalink":"http://dreamcat.ink/2020/03/25/ge-ren-tu-xie-xi-lie-zong-jie-java-duo-xian-cheng/","excerpt":"","text":"大纲图 什么是线程和进程？进程进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 比如：当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。 线程 线程是一个比进程更小的执行单位 一个进程在其执行的过程中可以产生多个线程 与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程 并发和并行的区别？并发：同一时间段，多个任务都在执行 (单位时间内不一定同时执行)；并行：单位时间内，多个任务同时执行。为什么使用多线程？ 从计算机底层来说： 线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。 从当代互联网发展趋势来说：现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 使用多线程可能会带来什么问题？可能会带来内存泄漏、上下文切换、死锁有受限于硬件和软件的资源闲置问题。 说说线程的生命周期？ 线程创建之后它将处于New（新建）状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。 当线程执行 wait()方法之后，线程进入 WAITING（等待） 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞）状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 什么是上下文切换？多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。 实际上就是任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 什么是死锁？如何避免死锁？如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 学过操作系统的朋友都知道产生死锁必须具备以下四个条件： 互斥条件：该资源任意一个时刻只由一个线程占用。(同一时刻，这个碗是我的，你不能碰) 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。（我拿着这个碗一直不放） 不剥夺条件:线程已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。（我碗中的饭没吃完，你不能抢，释放权是我自己的，我想什么时候放就什么时候放） 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。（我拿了A碗，你拿了B碗，但是我还想要你的B碗，你还想我的A碗） public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -> { synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); } } }, \"线程 1\").start(); new Thread(() -> { synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource1\"); synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); } } }, \"线程 2\").start(); } } Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。 如何找到死锁通过jdk常用的命令jsp和jstack，jsp查看java程序的id，jstack查看方法的栈信息等。 说说Sleep和Wait方法的区别 两者最主要的区别在于：sleep 方法没有释放锁，而 wait 方法释放了锁 。 两者都可以暂停线程的执行。 Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用wait(long timeout)超时后线程会自动苏醒。 Synchronzed使用方式 修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 。 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 单例public class Singleton { private volatile static Singleton uniqueInstance; // 第一步 private Singleton() { // 第二步，私有 } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) { // 双重校验 //类对象加锁 synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 注意：uniqueInstance 采用 volatile 关键字修饰也是很有必要 uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 Synchronized 和 ReenTrantLock 的对比 两者都是可重入锁:两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 Synchronized 依赖于 JVM 而 ReenTrantLock 依赖于 API:synchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReenTrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ReenTrantLock 比 Synchronized 增加了一些高级功能 等待可中断：过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 可实现公平锁 可实现选择性通知（锁可以绑定多个条件）：线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” 性能已不是选择标准：在jdk1.6之前synchronized 关键字吞吐量随线程数的增加，下降得非常严重。1.6之后，synchronized 和 ReenTrantLock 的性能基本是持平了。 底层原理synchronized 关键字底层原理属于 JVM 层面。 synchronized 同步语句块的情况 synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权.当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 synchronized 修饰方法的的情况 synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法，JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 1.6版本的优化在 Java 早期版本中，synchronized 属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 锁主要存在四中状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 偏向锁 引入偏向锁的目的和引入轻量级锁的目的很像，他们都是为了没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。但是不同是：轻量级锁在无竞争的情况下使用 CAS 操作去代替使用互斥量。而偏向锁在无竞争的情况下会把整个同步都消除掉。 偏向锁的“偏”就是偏心的偏，它的意思是会偏向于第一个获得它的线程，如果在接下来的执行中，该锁没有被其他线程获取，那么持有偏向锁的线程就不需要进行同步！ 但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。 升级过程： 访问Mark Word中偏向锁的标识是否设置成1，锁标识位是否为01，确认偏向状态 如果为可偏向状态，则判断当前线程ID是否为偏向线程 如果偏向线程未当前线程，则通过cas操作竞争锁，如果竞争成功则操作Mark Word中线程ID设置为当前线程ID 如果cas偏向锁获取失败，则挂起当前偏向锁线程，偏向锁升级为轻量级锁。 轻量级锁 倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)。轻量级锁不是为了代替重量级锁，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗，因为使用轻量级锁时，不需要申请互斥量。另外，轻量级锁的加锁和解锁都用到了CAS操作。 轻量级锁能够提升程序同步性能的依据是“对于绝大部分锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。如果没有竞争，轻量级锁使用 CAS 操作避免了使用互斥操作的开销。但如果存在锁竞争，除了互斥量开销外，还会额外发生CAS操作，因此在有锁竞争的情况下，轻量级锁比传统的重量级锁更慢！如果锁竞争激烈，那么轻量级将很快膨胀为重量级锁！ 升级过程： 线程由偏向锁升级为轻量级锁时，会先把锁的对象头MarkWord复制一份到线程的栈帧中，建立一个名为锁记录空间（Lock Record），用于存储当前Mark Word的拷贝。 虚拟机使用cas操作尝试将对象的Mark Word指向Lock Record的指针，并将Lock record里的owner指针指对象的Mark Word。 如果cas操作成功，则该线程拥有了对象的轻量级锁。第二个线程cas自旋锁等待锁线程释放锁。 如果多个线程竞争锁，轻量级锁要膨胀为重量级锁，Mark Word中存储的就是指向重量级锁（互斥量）的指针。其他等待线程进入阻塞状态。 自旋锁和自适应自旋 轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。 互斥同步对性能最大的影响就是阻塞的实现，因为挂起线程/恢复线程的操作都需要转入内核态中完成（用户态转换到内核态会耗费时间）。 一般线程持有锁的时间都不是太长，所以仅仅为了这一点时间去挂起线程/恢复线程是得不偿失的。 所以，虚拟机的开发团队就这样去考虑：“我们能不能让后面来的请求获取锁的线程等待一会而不被挂起呢？看看持有锁的线程是否很快就会释放锁”。为了让一个线程等待，我们只需要让线程执行一个忙循环（自旋），这项技术就叫做自旋。 在 JDK1.6 中引入了自适应的自旋锁。自适应的自旋锁带来的改进就是：自旋的时间不在固定了，而是和前一次同一个锁上的自旋时间以及锁的拥有者的状态来决定，虚拟机变得越来越“聪明”了。 锁消除 锁消除理解起来很简单，它指的就是虚拟机即使编译器在运行时，如果检测到那些共享数据不可能存在竞争，那么就执行锁消除。锁消除可以节省毫无意义的请求锁的时间。 锁粗化 原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小，——直在共享数据的实际作用域才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待线程也能尽快拿到锁。 总结升级过程： 检测Mark Word里面是不是当前线程的ID，如果是，表示当前线程处于偏向锁 如果不是，则使用CAS将当前线程的ID替换Mard Word，如果成功则表示当前线程获得偏向锁，置偏向标志位1 如果失败，则说明发生竞争，撤销偏向锁，进而升级为轻量级锁。 当前线程使用CAS将对象头的Mark Word替换为锁记录指针，如果成功，当前线程获得锁 如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 如果自旋成功则依然处于轻量级状态。 如果自旋失败，则升级为重量级锁。 VolatileVolatile的特性 可见性 volatile的可见性是指当一个变量被volatile修饰后，这个变量就对所有线程均可见。白话点就是说当一个线程修改了一个volatile修饰的变量后，其他线程可以立刻得知这个变量的修改，拿到最这个变量最新的值。 public class VolatileVisibleDemo { // private boolean isReady = true; private volatile boolean isReady = true; void m() { System.out.println(Thread.currentThread().getName() + \" m start...\"); while (isReady) { } System.out.println(Thread.currentThread().getName() + \" m end...\"); } public static void main(String[] args) { VolatileVisibleDemo demo = new VolatileVisibleDemo(); new Thread(() -> demo.m(), \"t1\").start(); try {TimeUnit.SECONDS.sleep(1);} catch (InterruptedException e) {e.printStackTrace();} demo.isReady = false; // 刚才一秒过后开始执行 } } 分析：一开始isReady为true，m方法中的while会一直循环，而主线程开启开线程之后会延迟1s将isReady赋值为false，若不加volatile修饰，则程序一直在运行，若加了volatile修饰，则程序最后会输出t1 m end… 有序性 有序性是指程序代码的执行是按照代码的实现顺序来按序执行的；volatile的有序性特性则是指禁止JVM指令重排优化。 public class Singleton { private static Singleton instance = null; // valotile //private static volatile Singleton instance = null; private Singleton() { } // 私有 public static Singleton getInstance() { // 双重校验 //第一次判断 if(instance == null) { synchronized (Singleton.class) { // 加锁 if(instance == null) { //初始化，并非原子操作 instance = new Singleton(); // 这一行代码展开其实分三步走 } } } return instance; } } 上面的代码是一个很常见的单例模式实现方式，但是上述代码在多线程环境下是有问题的。为什么呢，问题出在instance对象的初始化上，因为instance = new Singleton();这个初始化操作并不是原子的，在JVM上会对应下面的几条指令： memory =allocate(); //1. 分配对象的内存空间 ctorInstance(memory); //2. 初始化对象 instance =memory; //3. 设置instance指向刚分配的内存地址上面三个指令中，步骤2依赖步骤1，但是步骤3不依赖步骤2，所以JVM可能针对他们进行指令重拍序优化，重排后的指令如下： memory =allocate(); //1. 分配对象的内存空间 instance =memory; //3. 设置instance指向刚分配的内存地址 ctorInstance(memory); //2. 初始化对象 这样优化之后，内存的初始化被放到了instance分配内存地址的后面，这样的话当线程1执行步骤3这段赋值指令后，刚好有另外一个线程2进入getInstance方法判断instance不为null，这个时候线程2拿到的instance对应的内存其实还未初始化，这个时候拿去使用就会导致出错。 所以我们在用这种方式实现单例模式时，会使用volatile关键字修饰instance变量，这是因为volatile关键字除了可以保证变量可见性之外，还具有防止指令重排序的作用。当用volatile修饰instance之后，JVM执行时就不会对上面提到的初始化指令进行重排序优化，这样也就不会出现多线程安全问题了。 不能保证原子性 volatile关键字能保证变量的可见性和代码的有序性，但是不能保证变量的原子性，下面我再举一个volatile与原子性的例子： public class VolatileAtomicDemo { public static volatile int count = 0; public static void increase() { count++; } public static void main(String[] args) { Thread[] threads = new Thread[20]; for(int i = 0; i &lt; threads.length; i++) { threads[i] = new Thread(() -> { for(int j = 0; j &lt; 1000; j++) { increase(); } }); threads[i].start(); } //等待所有累加线程结束 while (Thread.activeCount() > 1) { Thread.yield(); } System.out.println(count); } } 上面这段代码创建了20个线程，每个线程对变量count进行1000次自增操作，如果这段代码并发正常的话，结果应该是20000，但实际运行过程中经常会出现小于20000的结果，因为count++这个自增操作不是原子操作。看图 内存屏障Java的Volatile的特征是任何读都能读到最新值，本质上是JVM通过内存屏障来实现的；为了实现volatile内存语义，JMM会分别限制重排序类型。下面是JMM针对编译器制定的volatile重排序规则表： 是否能重排序 第二个操作 第一个操作 普通读/写 volatile读 volatile写 普通读/写 no volatile读 no no no volatile写 no no 从上表我们可以看出： 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能，为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略： 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 volatile写插入内存指令图： 上图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。 这里比较有意思的是volatile写后面的StoreLoad屏障。这个屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面，是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在这里采取了保守策略：在每个volatile写的后面或在每个volatile读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里我们可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。 volatile读插入内存指令图： 上图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。下面我们通过具体的示例代码来说明： class VolatileBarrierExample { int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() { int i = v1; //第一个volatile读 int j = v2; // 第二个volatile读 a = i + j; //普通写 v1 = i + 1; // 第一个volatile写 v2 = j * 2; //第二个 volatile写 } } 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化： 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器常常会在这里插入一个StoreLoad屏障。 volatile汇编0x000000011214bb49: mov %rdi,%rax 0x000000011214bb4c: dec %eax 0x000000011214bb4e: mov %eax,0x10(%rsi) 0x000000011214bb51: lock addl $0x0,(%rsp) ;*putfield v1 ; - com.earnfish.VolatileBarrierExample::readAndWrite@21 (line 35) 0x000000011214bb56: imul %edi,%ebx 0x000000011214bb59: mov %ebx,0x14(%rsi) 0x000000011214bb5c: lock addl $0x0,(%rsp) ;*putfield v2 ; - com.earnfish.VolatileBarrierExample::readAndWrite@28 (line 36) v1 = i - 1; // 第一个volatile写 v2 = j * i; // 第二个volatile写 可见其本质是通过一个lock指令来实现的。那么lock是什么意思呢？ 它的作用是使得本CPU的Cache写入了内存，该写入动作也会引起别的CPU invalidate其Cache。所以通过这样一个空操作，可让前面volatile变量的修改对其他CPU立即可见。 锁住内存 任何读必须在写完成之后再执行 使其它线程这个值的栈缓存失效 CAS我们在读Concurrent包下的类的源码时，发现无论是ReenterLock内部的AQS，还是各种Atomic开头的原子类，内部都应用到了CAS public class Test { public AtomicInteger i; public void add() { i.getAndIncrement(); } } 我们来看getAndIncrement的内部： public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); } 再深入到getAndAddInt(): public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; } 现在重点来了，compareAndSwapInt（var1, var2, var5, var5 + var4）其实换成compareAndSwapInt（obj, offset, expect, update）比较清楚，意思就是如果obj内的value和expect相等，就证明没有其他线程改变过这个变量，那么就更新它为update，如果这一步的CAS没有成功，那就采用自旋的方式继续进行CAS操作，取出乍一看这也是两个步骤了啊，其实在JNI里是借助于一个CPU指令完成的。所以还是原子操作。 CAS底层UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper(\"Unsafe_CompareAndSwapInt\"); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e; UNSAFE_END p是取出的对象，addr是p中offset处的地址，最后调用了Atomic::cmpxchg(x, addr, e), 其中参数x是即将更新的值，参数e是原内存的值。代码中能看到cmpxchg有基于各个平台的实现。 ABA问题描述: 第一个线程取到了变量 x 的值 A，然后巴拉巴拉干别的事，总之就是只拿到了变量 x 的值 A。这段时间内第二个线程也取到了变量 x 的值 A，然后把变量 x 的值改为 B，然后巴拉巴拉干别的事，最后又把变量 x 的值变为 A （相当于还原了）。在这之后第一个线程终于进行了变量 x 的操作，但是此时变量 x 的值还是 A，所以 compareAndSet 操作是成功。 目前在JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。 public class ABADemo { static AtomicInteger atomicInteger = new AtomicInteger(100); static AtomicStampedReference&lt;Integer> atomicStampedReference = new AtomicStampedReference&lt;>(100, 1); public static void main(String[] args) { System.out.println(\"=====ABA的问题产生=====\"); new Thread(() -> { atomicInteger.compareAndSet(100, 101); atomicInteger.compareAndSet(101, 100); }, \"t1\").start(); new Thread(() -> { // 保证线程1完成一次ABA问题 try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(atomicInteger.compareAndSet(100, 2020) + \" \" + atomicInteger.get()); }, \"t2\").start(); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"=====解决ABA的问题=====\"); new Thread(() -> { int stamp = atomicStampedReference.getStamp(); // 第一次获取版本号 System.out.println(Thread.currentThread().getName() + \" 第1次版本号\" + stamp); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } atomicStampedReference.compareAndSet(100, 101, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"\\t第2次版本号\" + atomicStampedReference.getStamp()); atomicStampedReference.compareAndSet(101, 100, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"\\t第3次版本号\" + atomicStampedReference.getStamp()); }, \"t3\").start(); new Thread(() -> { int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + \"\\t第1次版本号\" + stamp); try { TimeUnit.SECONDS.sleep(4); } catch (InterruptedException e) { e.printStackTrace(); } boolean result = atomicStampedReference.compareAndSet(100, 2020, stamp, stamp + 1); System.out.println(Thread.currentThread().getName() + \"\\t修改是否成功\" + result + \"\\t当前最新实际版本号：\" + atomicStampedReference.getStamp()); System.out.println(Thread.currentThread().getName() + \"\\t当前最新实际值：\" + atomicStampedReference.getReference()); }, \"t4\").start(); } } ThreadLocal如果想实现每一个线程都有自己的专属本地变量该如何解决呢？ JDK中提供的ThreadLocal类正是为了解决这样的问题。 ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。**如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get（） 和 set（） 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。** 原理public class Thread implements Runnable { ...... //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; ...... } 从上面Thread类 源代码可以看出Thread 类中有一个 threadLocals 和 一个 inheritableThreadLocals 变量，它们都是 ThreadLocalMap 类型的变量,我们可以把 ThreadLocalMap 理解为ThreadLocal 类实现的定制化的 HashMap。默认情况下这两个变量都是null，只有当前线程调用 ThreadLocal 类的 set或get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是ThreadLocalMap类对应的 get()、set()方法。 ThreadLocal类的set()方法 public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 每个Thread中都具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为key的键值对。 比如我们在同一个线程中声明了两个 ThreadLocal 对象的话，会使用 Thread内部都是使用仅有那个ThreadLocalMap 存放数据的，ThreadLocalMap的 key 就是 ThreadLocal对象，value 就是 ThreadLocal 对象调用set方法设置的值。ThreadLocal 是 map结构是为了让每个线程可以关联多个 ThreadLocal变量。这也就解释了ThreadLocal声明的变量为什么在每一个线程都有自己的专属本地变量。 内存泄露ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候会 key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法 并发集合容器为什么说ArrayList线程不安全？看add方法的源码 public boolean add(E e) { /** * 添加一个元素时，做了如下两步操作 * 1.判断列表的capacity容量是否足够，是否需要扩容 * 2.真正将元素放在列表的元素数组里面 */ ensureCapacityInternal(size + 1); // Increments modCount!! // 可能因为该操作，导致下一步发生数组越界 elementData[size++] = e; // 可能null值 return true; } 数组越界 列表大小为9，即size=9 线程A开始进入add方法，这时它获取到size的值为9，调用ensureCapacityInternal方法进行容量判断。 线程B此时也进入add方法，它获取到size的值也为9，也开始调用ensureCapacityInternal方法。 线程A发现需求大小为10，而elementData的大小就为10，可以容纳。于是它不再扩容，返回。 线程B也发现需求大小为10，也可以容纳，返回。 线程A开始进行设置值操作， elementData[size++] = e 操作。此时size变为10。 线程B也开始进行设置值操作，它尝试设置elementData[10] = e，而elementData没有进行过扩容，它的下标最大为9。于是此时会报出一个数组越界的异常ArrayIndexOutOfBoundsException. null值情况 elementData[size++] = e不是一个原子操作： elementData[size] = e; size = size + 1; 逻辑： 列表大小为0，即size=0 线程A开始添加一个元素，值为A。此时它执行第一条操作，将A放在了elementData下标为0的位置上。 接着线程B刚好也要开始添加一个值为B的元素，且走到了第一步操作。此时线程B获取到size的值依然为0，于是它将B也放在了elementData下标为0的位置上。 线程A开始将size的值增加为1 线程B开始将size的值增加为2 这样线程AB执行完毕后，理想中情况为size为2，elementData下标0的位置为A，下标1的位置为B。而实际情况变成了size为2，elementData下标为0的位置变成了B，下标1的位置上什么都没有。并且后续除非使用set方法修改此位置的值，否则将一直为null，因为size为2，添加元素时会从下标为2的位置上开始。 解决非安全集合的并发都有哪些？ArrayList-&gt;Vector-&gt;SynchronizedList-&gt;CopyOnWriteArrayList ArraySet-&gt;SynchronizedSet-&gt;CopyOnWriteArraySet HashMap-&gt;SynchronizedMap-&gt;ConcurrentHashMap 并发同步容器AQS原理AQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过 protected 类型的getState，setState，compareAndSetState进行操作 //返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } AQS 定义两种资源共享方式 Exclusive（独占）只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁,ReentrantLock 同时支持两种锁。 总结：公平锁和非公平锁只有两处不同： 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。 Share（共享）多个线程可同时执行，如 Semaphore/CountDownLatch。Semaphore、 CyclicBarrier、ReadWriteLock 。 AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法： isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 CountDownLatchCountDownLatch是共享锁的一种实现,它默认构造 AQS 的 state 值为 count。当线程使用countDown方法时,其实使用了tryReleaseShared方法以CAS的操作来减少state,直至state为0就代表所有的线程都调用了countDown方法。当调用await方法的时候，如果state不为0，就代表仍然有线程没有调用countDown方法，那么就把已经调用过countDown的线程都放入阻塞队列Park,并自旋CAS判断state == 0，直至最后一个线程调用了countDown，使得state == 0，于是阻塞的线程便判断成功，全部往下执行。 三种用法： 某一线程在开始运行前等待 n 个线程执行完毕。将 CountDownLatch 的计数器初始化为 n ：new CountDownLatch(n)，每当一个任务线程执行完毕，就将计数器减 1 countdownlatch.countDown()，当计数器的值变为 0 时，在CountDownLatch上 await() 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。 实现多个线程开始执行任务的最大并行性。注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 ：new CountDownLatch(1)，多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为 0，多个线程同时被唤醒。 死锁检测：一个非常方便的使用场景是，你可以使用 n 个线程访问共享资源，在每次测试阶段的线程数目是不同的，并尝试产生死锁。 public class CountDownLatchDemo { public static void main(String[] args) throws InterruptedException { countDownLatchTest(); // general(); } public static void general() { for (int i = 0; i &lt; 6; i++) { new Thread(() -> { System.out.println(Thread.currentThread().getName() + \" 上完自习，离开教师\"); }, \"Thread --> \" + i).start(); } while (Thread.activeCount() > 2) { try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \" ====班长最后走人\"); } } public static void countDownLatchTest() throws InterruptedException { CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 0; i &lt; 6; i++) { new Thread(() -> { System.out.println(Thread.currentThread().getName() + \" 上完自习，离开教师\"); countDownLatch.countDown(); }, \"Thread --> \" + i).start(); } countDownLatch.await(); System.out.println(Thread.currentThread().getName() + \" ====班长最后走人\"); } } CyclicBarrierCyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 public class CyclicBarrierDemo { public static void main(String[] args) { cyclicBarrierTest(); } public static void cyclicBarrierTest() { CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -> { System.out.println(\"====召唤神龙====\"); }); for (int i = 0; i &lt; 7; i++) { final int tempInt = i; new Thread(() -> { System.out.println(Thread.currentThread().getName() + \" 收集到第\" + tempInt + \"颗龙珠\"); try { cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } }, \"\" + i).start(); } } } 当调用 CyclicBarrier 对象调用 await() 方法时，实际上调用的是dowait(false, 0L)方法。 await() 方法就像树立起一个栅栏的行为一样，将线程挡住了，当拦住的线程数量达到 parties 的值时，栅栏才会打开，线程才得以通过执行。 // 当线程数量或者请求数量达到 count 时 await 之后的方法才会被执行。上面的示例中 count 的值就为 5。 private int count; /** * Main barrier code, covering the various policies. */ private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { final ReentrantLock lock = this.lock; // 锁住 lock.lock(); try { final Generation g = generation; if (g.broken) throw new BrokenBarrierException(); // 如果线程中断了，抛出异常 if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } // cout减1 int index = --count; // 当 count 数量减为 0 之后说明最后一个线程已经到达栅栏了，也就是达到了可以执行await 方法之后的条件 if (index == 0) { // tripped boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; // 将 count 重置为 parties 属性的初始化值 // 唤醒之前等待的线程 // 下一波执行开始 nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out for (;;) { try { if (!timed) trip.await(); else if (nanos > 0L) nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation &amp;&amp; ! g.broken) { breakBarrier(); throw ie; } else { // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \"belong\" to subsequent execution. Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed &amp;&amp; nanos &lt;= 0L) { breakBarrier(); throw new TimeoutException(); } } } finally { lock.unlock(); } } 总结：CyclicBarrier 内部通过一个 count 变量作为计数器，count 的初始值为 parties 属性的初始化值，每当一个线程到了栅栏这里了，那么就将计数器减一。如果 count 值为 0 了，表示这是这一代最后一个线程到达栅栏，就尝试执行我们构造方法中输入的任务。 Semaphoresynchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 public class SemaphoreDemo { public static void main(String[] args) { Semaphore semaphore = new Semaphore(3);// 模拟三个停车位 for (int i = 0; i &lt; 6; i++) { // 模拟6部汽车 new Thread(() -> { try { semaphore.acquire(); System.out.println(Thread.currentThread().getName() + \" 抢到车位\"); // 停车3s try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \" 停车3s后离开车位\"); } catch (Exception e) { e.printStackTrace(); } finally { semaphore.release(); } }, \"Car \" + i).start(); } } } 阻塞队列 ArrayBlockingQueue:由数组结构组成的有界阻塞队列. LinkedBlockingQueue:由链表结构组成的有界(但大小默认值Integer&gt;MAX_VALUE)阻塞队列. PriorityBlockingQueue:支持优先级排序的无界阻塞队列. DelayQueue:使用优先级队列实现的延迟无界阻塞队列. SynchronousQueue:不存储元素的阻塞队列,也即是单个元素的队列. LinkedTransferQueue:由链表结构组成的无界阻塞队列. LinkedBlockingDuque:由链表结构组成的双向阻塞队列. 抛出异常方法：add remove 不抛异常：offer poll 阻塞 put take 带时间 offer poll 生产者消费者synchronized版本的生产者和消费者，比较繁琐 public class ProdConsumerSynchronized { private final LinkedList&lt;String> lists = new LinkedList&lt;>(); public synchronized void put(String s) { while (lists.size() != 0) { // 用while怕有存在虚拟唤醒线程 // 满了， 不生产了 try { this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } lists.add(s); System.out.println(Thread.currentThread().getName() + \" \" + lists.peekFirst()); this.notifyAll(); // 这里可是通知所有被挂起的线程，包括其他的生产者线程 } public synchronized void get() { while (lists.size() == 0) { try { this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(Thread.currentThread().getName() + \" \" + lists.removeFirst()); this.notifyAll(); // 通知所有被wait挂起的线程 用notify可能就死锁了。 } public static void main(String[] args) { ProdConsumerSynchronized prodConsumerSynchronized = new ProdConsumerSynchronized(); // 启动消费者线程 for (int i = 0; i &lt; 5; i++) { new Thread(prodConsumerSynchronized::get, \"ConsA\" + i).start(); } // 启动生产者线程 for (int i = 0; i &lt; 5; i++) { int tempI = i; new Thread(() -> { prodConsumerSynchronized.put(\"\" + tempI); }, \"ProdA\" + i).start(); } } } ReentrantLock public class ProdConsumerReentrantLock { private LinkedList&lt;String> lists = new LinkedList&lt;>(); private Lock lock = new ReentrantLock(); private Condition prod = lock.newCondition(); private Condition cons = lock.newCondition(); public void put(String s) { lock.lock(); try { // 1. 判断 while (lists.size() != 0) { // 等待不能生产 prod.await(); } // 2.干活 lists.add(s); System.out.println(Thread.currentThread().getName() + \" \" + lists.peekFirst()); // 3. 通知 cons.signalAll(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public void get() { lock.lock(); try { // 1. 判断 while (lists.size() == 0) { // 等待不能消费 cons.await(); } // 2.干活 System.out.println(Thread.currentThread().getName() + \" \" + lists.removeFirst()); // 3. 通知 prod.signalAll(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public static void main(String[] args) { ProdConsumerReentrantLock prodConsumerReentrantLock = new ProdConsumerReentrantLock(); for (int i = 0; i &lt; 5; i++) { int tempI = i; new Thread(() -> { prodConsumerReentrantLock.put(tempI + \"\"); }, \"ProdA\" + i).start(); } for (int i = 0; i &lt; 5; i++) { new Thread(prodConsumerReentrantLock::get, \"ConsA\" + i).start(); } } } BlockingQueue public class ProdConsumerBlockingQueue { private volatile boolean flag = true; private AtomicInteger atomicInteger = new AtomicInteger(); BlockingQueue&lt;String> blockingQueue = null; public ProdConsumerBlockingQueue(BlockingQueue&lt;String> blockingQueue) { this.blockingQueue = blockingQueue; } public void myProd() throws Exception { String data = null; boolean retValue; while (flag) { data = atomicInteger.incrementAndGet() + \"\"; retValue = blockingQueue.offer(data, 2, TimeUnit.SECONDS); if (retValue) { System.out.println(Thread.currentThread().getName() + \" 插入队列\" + data + \" 成功\"); } else { System.out.println(Thread.currentThread().getName() + \" 插入队列\" + data + \" 失败\"); } TimeUnit.SECONDS.sleep(1); } System.out.println(Thread.currentThread().getName() + \" 大老板叫停了，flag=false，生产结束\"); } public void myConsumer() throws Exception { String result = null; while (flag) { result = blockingQueue.poll(2, TimeUnit.SECONDS); if (null == result || result.equalsIgnoreCase(\"\")) { flag = false; System.out.println(Thread.currentThread().getName() + \" 超过2s没有取到蛋糕，消费退出\"); return; } System.out.println(Thread.currentThread().getName() + \" 消费队列\" + result + \"成功\"); } } public void stop() { flag = false; } public static void main(String[] args) { ProdConsumerBlockingQueue prodConsumerBlockingQueue = new ProdConsumerBlockingQueue(new ArrayBlockingQueue&lt;>(10)); new Thread(() -> { System.out.println(Thread.currentThread().getName() + \" 生产线程启动\"); try { prodConsumerBlockingQueue.myProd(); } catch (Exception e) { e.printStackTrace(); } }, \"Prod\").start(); new Thread(() -> { System.out.println(Thread.currentThread().getName() + \" 消费线程启动\"); try { prodConsumerBlockingQueue.myConsumer(); } catch (Exception e) { e.printStackTrace(); } }, \"Consumer\").start(); try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"5s后main叫停，线程结束\"); prodConsumerBlockingQueue.stop(); } } 线程池线程池的好处 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 FixedThreadPoolFixedThreadPool 被称为可重用固定线程数的线程池。通过 Executors 类中的相关源代码来看一下相关实现： /** * 创建一个可重用固定数量线程的线程池 */ public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable>(), threadFactory); } 从上面源代码可以看出新创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 都被设置为 nThreads，这个 nThreads 参数是我们使用的时候自己传递的。 如果当前运行的线程数小于 corePoolSize， 如果再来新任务的话，就创建新的线程来执行任务； 当前运行的线程数等于 corePoolSize 后， 如果再来新任务的话，会将任务加入 LinkedBlockingQueue； 线程池中的线程执行完 手头的任务后，会在循环中反复从 LinkedBlockingQueue 中获取任务来执行； 不推荐使用 FixedThreadPool 使用无界队列 LinkedBlockingQueue（队列的容量为 Intger.MAX_VALUE）作为线程池的工作队列会对线程池带来如下影响 ： 当线程池中的线程数达到 corePoolSize 后，新任务将在无界队列中等待，因此线程池中的线程数不会超过 corePoolSize； 由于使用无界队列时 maximumPoolSize 将是一个无效参数，因为不可能存在任务队列满的情况。所以，通过创建 FixedThreadPool的源码可以看出创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 被设置为同一个值。 由于 1 和 2，使用无界队列时 keepAliveTime 将是一个无效参数； 运行中的 FixedThreadPool（未执行 shutdown()或 shutdownNow()）不会拒绝任务，在任务比较多的时候会导致 OOM（内存溢出）。 SingleThreadExecutor /** *返回只有一个线程的线程池 */ public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable>(), threadFactory)); } 和上面一个差不多，只不过core和max都被设置为1 CachedThreadPool /** * 创建一个线程池，根据需要创建新线程，但会在先前构建的线程可用时重用它。 */ public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable>(), threadFactory); } CachedThreadPool 的corePoolSize 被设置为空（0），maximumPoolSize被设置为 Integer.MAX.VALUE，即它是无界的，这也就意味着如果主线程提交任务的速度高于 maximumPool 中线程处理任务的速度时，CachedThreadPool 会不断创建新的线程。极端情况下，这样会导致耗尽 cpu 和内存资源。 首先执行 SynchronousQueue.offer(Runnable task) 提交任务到任务队列。如果当前 maximumPool 中有闲线程正在执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)，那么主线程执行 offer 操作与空闲线程执行的 poll 操作配对成功，主线程把任务交给空闲线程执行，execute()方法执行完成，否则执行下面的步骤 2； 当初始 maximumPool 为空，或者 maximumPool 中没有空闲线程时，将没有线程执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)。这种情况下，步骤 1 将失败，此时 CachedThreadPool 会创建新线程执行任务，execute 方法执行完成； ScheduledThreadPoolExecutor省略，基本不会用ThreadPoolExecutor（重点） /** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue&lt;Runnable> workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务 ) { if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } ThreadPoolExecutor 3 个最重要的参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，信任就会被存放在队列中。 ThreadPoolExecutor其他常见参数: keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下. 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务。您不会任务请求。但是这种策略会降低对于新任务提交速度，影响程序的整体性能。另外，这个策略喜欢增加队列容量。如果您的应用程序可以承受此延迟并且你不能任务丢弃任何一个任务请求的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。 Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。（这个直接查看 ThreadPoolExecutor 的构造函数源码就可以看出，比较简单的原因，这里就不贴代码了。） Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。 public class ThreadPoolExecutorDemo { public static void main(String[] args) { ExecutorService threadpools = new ThreadPoolExecutor( 3, 5, 1l, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;>(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); //new ThreadPoolExecutor.AbortPolicy(); //new ThreadPoolExecutor.CallerRunsPolicy(); //new ThreadPoolExecutor.DiscardOldestPolicy(); //new ThreadPoolExecutor.DiscardPolicy(); try { for (int i = 0; i &lt; 8; i++) { threadpools.execute(() -> { System.out.println(Thread.currentThread().getName() + \"\\t办理业务\"); }); } } catch (Exception e) { e.printStackTrace(); } finally { threadpools.shutdown(); } } } Java锁机制公平锁/非公平锁公平锁指多个线程按照申请锁的顺序来获取锁。非公平锁指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象（很长时间都没获取到锁-非洲人…），ReentrantLock，了解一下。 可重入锁可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁，典型的synchronized，了解一下 synchronized void setA() throws Exception { Thread.sleep(1000); setB(); // 因为获取了setA()的锁，此时调用setB()将会自动获取setB()的锁，如果不自动获取的话方法B将不会执行 } synchronized void setB() throws Exception { Thread.sleep(1000); } 独享锁/共享锁 独享锁：是指该锁一次只能被一个线程所持有。 共享锁：是该锁可被多个线程所持有。 互斥锁/读写锁上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是其具体的实现 乐观锁/悲观锁 乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待兵法同步的角度。 悲观锁认为对于同一个人数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出现问题。 乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作时没有事情的。 悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁带来大量的性能提升。 悲观锁在Java中的使用，就是利用各种锁。乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子类操作的更新。重量级锁是悲观锁的一种，自旋锁、轻量级锁与偏向锁属于乐观锁 分段锁 分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来哦实现高效的并发操作。 以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7与JDK8中HashMap的实现）的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表；同时又是ReentrantLock（Segment继承了ReentrantLock） 当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。 分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。 偏向锁/轻量级锁/重量级锁 这三种锁是锁的状态，并且是针对Synchronized。在Java5通过引入锁升级的机制来实现高效Synchronized。这三种锁的状态是通过对象监视器在对象头中的字段来表明的。偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁。降低获取锁的代价。 偏向锁的适用场景：始终只有一个线程在执行代码块，在它没有执行完释放锁之前，没有其它线程去执行同步快，在锁无竞争的情况下使用，一旦有了竞争就升级为轻量级锁，升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致stop the word操作；在有锁竞争时，偏向锁会多做很多额外操作，尤其是撤销偏向锁的时候会导致进入安全点，安全点会导致stw，导致性能下降，这种情况下应当禁用。 轻量级锁是指当锁是偏向锁的时候，被另一个线程锁访问，偏向锁就会升级为轻量级锁，其他线程会通过自选的形式尝试获取锁，不会阻塞，提高性能。 重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。 自旋锁 在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。 自旋锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。 自旋锁尽可能的减少线程的阻塞，适用于锁的竞争不激烈，且占用锁时间非常短的代码来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起再唤醒的操作的消耗。 但是如果锁的竞争激烈，或者持有锁的线程需要长时间占用锁执行同步块，这时候就不适用使用自旋锁了，因为自旋锁在获取锁前一直都是占用cpu做无用功，同时有大量线程在竞争一个锁，会导致获取锁的时间很长，线程自旋的消耗大于线程阻塞挂起操作的消耗，其它需要cpu的线程又不能获取到cpu，造成cpu的浪费。 Java锁总结Java锁机制可归为Sychornized锁和Lock锁两类。Synchronized是基于JVM来保证数据同步的，而Lock则是硬件层面，依赖特殊的CPU指令来实现数据同步的。 Synchronized是一个非公平、悲观、独享、互斥、可重入的重量级锁。 ReentrantLock是一个默认非公平但可实现公平的、悲观、独享、互斥、可重入、重量级锁。 ReentrantReadWriteLock是一个默认非公平但可实现公平的、悲观、写独享、读共享、读写、可重入、重量级锁。","categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://dreamcat.ink/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Centos7安装RocketMQ及配置","slug":"Centos7安装RocketMQ及配置","date":"2020-02-21T12:33:02.000Z","updated":"2020-10-29T15:54:04.168Z","comments":true,"path":"2020/02/21/centos7-an-zhuang-rocketmq-ji-pei-zhi/","link":"","permalink":"http://dreamcat.ink/2020/02/21/centos7-an-zhuang-rocketmq-ji-pei-zhi/","excerpt":"","text":"引言 项目中用到消息队列RocketMQ，因此在Centos7安装及配置。。。 下载 rocketmq 选择rocketmq-all-4.6.1-bin-release.zip 因为RocketMQ依赖maven打包，因此需要安装maven 注意：下面所添加的环境统统是.zshrc Maven安装 maven地址 在本文章中采用最新3.6.3 解压tar -zxf apache-maven-3.6.3-bin.tar.gz 修改仓库地址为阿里云，因为默认下载依赖总超时，找到conf中的setting.xml文件 &lt;mirror> &lt;id>alimaven&lt;/id> &lt;name>aliyun maven&lt;/name> &lt;url>http://maven.aliyun.com/nexus/content/groups/public/&lt;/url> &lt;mirrorOf>central&lt;/mirrorOf> &lt;/mirror> 配置环境变量vim .zshrc export M2_HOME=/home/pch/Documents/mf/web/maven3 export PATH=$PATH:$Java_HOME/bin:$M2_HOME/bin 刷新环境变量source .zshrc Rocketmq安装 解压unzip rocketmq-all-4.6.1-bin-release.zip -d ./ 注意在bin目录中：runserver.sh和runbroker.sh 个人情况修改JAVA_OPT=”${JAVA_OPT} -server一行参数 将nameserver地址添加到环境变量中 export NAMESRV_ADDR=127.0.0.1:9876 刷新配置文件source .zshrh 创建logs文件夹，存放启动日志，方便查看 后台运行nameservernohup sh mqnamesrv &gt; ../logs/nameser.log 2&gt;&amp;1&amp; 后台运行brokernohup sh mqbroker &gt; ../logs/broker.log 2&gt;&amp;1&amp; 控制台安装这个控制台属于springboot的项目… 项目链接git clone https://github.com/apache/rocketmq-externals rocketmq-externals里面有所有Apache RocketMq外部项目，有的还在孵化中，我主要是使用rocketmq-console，进入到console项目中，修改resources文件夹下面的配置文件 在rocketmq-externals/rocketmq-console/src/main/resources目录下打开配置文件vim application.properties 修改以下配置 server.port=8090 rocketmq.config.namesrvAddr=127.0.0.1:9876 rocketmq.config.dataPath=/home/pch/Documents/mf/web/rocket-data 开始maven打包mvn clean install -Dmaven.test.skip=true 完成之后在target找到rocketmq-console-ng-1.0.1.jar，后台运行它 nohup java -jar rocketmq-console-ng-1.0.1.jar &gt; rocket-data/console.out 2&gt;&amp;1&amp; localhost:8090即可看到效果","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://dreamcat.ink/tags/MQ/"}]},{"title":"为什么说ArrayList是线程不安全的?","slug":"为什么说ArrayList是线程不安全的","date":"2020-02-14T08:41:53.000Z","updated":"2020-10-29T15:24:13.721Z","comments":true,"path":"2020/02/14/wei-shi-me-shuo-arraylist-shi-xian-cheng-bu-an-quan-de/","link":"","permalink":"http://dreamcat.ink/2020/02/14/wei-shi-me-shuo-arraylist-shi-xian-cheng-bu-an-quan-de/","excerpt":"","text":"引言 面试时相信面试官首先就会问到关于它的知识。一个经常被问到的问题就是：ArrayList是否是线程安全的？那么它为什么是线程不安全的呢？它线程不安全的具体体现又是怎样的呢？我们从源码的角度来看下。 源码分析首先看看该类的属性字段： /** * 列表元素集合数组 * 如果新建ArrayList对象时没有指定大小，那么会将EMPTY_ELEMENTDATA赋值给elementData， * 并在第一次添加元素时，将列表容量设置为DEFAULT_CAPACITY */ transient Object[] elementData; /** * 列表大小，elementData中存储的元素个数 */ private int size; ArrayList的实现主要就是用了一个Object的数组，用来保存所有的元素，以及一个size变量用来保存当前数组中已经添加了多少元素。 再次看add方法的源码 public boolean add(E e) { /** * 添加一个元素时，做了如下两步操作 * 1.判断列表的capacity容量是否足够，是否需要扩容 * 2.真正将元素放在列表的元素数组里面 */ ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } 由此看到add元素时，实际做了两个大的步骤： 判断elementData数组容量是否满足需求 在elementData对应位置上设置值 数组越界 列表大小为9，即size=9 线程A开始进入add方法，这时它获取到size的值为9，调用ensureCapacityInternal方法进行容量判断。 线程B此时也进入add方法，它获取到size的值也为9，也开始调用ensureCapacityInternal方法。 线程A发现需求大小为10，而elementData的大小就为10，可以容纳。于是它不再扩容，返回。 线程B也发现需求大小为10，也可以容纳，返回。 线程A开始进行设置值操作， elementData[size++] = e 操作。此时size变为10。 线程B也开始进行设置值操作，它尝试设置elementData[10] = e，而elementData没有进行过扩容，它的下标最大为9。于是此时会报出一个数组越界的异常ArrayIndexOutOfBoundsException. null值的情况elementData[size++] = e不是一个原子操作： elementData[size] = e; size = size + 1; 逻辑： 列表大小为0，即size=0 线程A开始添加一个元素，值为A。此时它执行第一条操作，将A放在了elementData下标为0的位置上。 接着线程B刚好也要开始添加一个值为B的元素，且走到了第一步操作。此时线程B获取到size的值依然为0，于是它将B也放在了elementData下标为0的位置上。 线程A开始将size的值增加为1 线程B开始将size的值增加为2 这样线程AB执行完毕后，理想中情况为size为2，elementData下标0的位置为A，下标1的位置为B。而实际情况变成了size为2，elementData下标为0的位置变成了B，下标1的位置上什么都没有。并且后续除非使用set方法修改此位置的值，否则将一直为null，因为size为2，添加元素时会从下标为2的位置上开始。 案例 /** * 故障现象 * java.util.ConcurrentModificationException */ public static void notSafe() { List&lt;String> list = new ArrayList&lt;>(); for (int i = 1; i &lt;= 3; i++) { new Thread(() -> { list.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(list); }, \"Thread \" + i).start(); } }","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"Java多线程 CAS原理剖析","slug":"Java多线程-CAS原理剖析","date":"2020-02-13T16:05:57.000Z","updated":"2020-10-29T15:56:46.378Z","comments":true,"path":"2020/02/14/java-duo-xian-cheng-cas-yuan-li-pou-xi/","link":"","permalink":"http://dreamcat.ink/2020/02/14/java-duo-xian-cheng-cas-yuan-li-pou-xi/","excerpt":"","text":"引言 大家都知道，在Java并发种，我们最初接触的应该就是synchronized关键字了，但是synchronized属于重量级锁，很多时候会引起性能问题，volatile也是个不错的选择，但是volatile不能保证原子性，只能在某些场合下使用。像synchronized这种独占锁属于悲观锁，乐观锁最常见的就是CAS。 例子我们在读Concurrent包下的类的源码时，发现无论是ReenterLock内部的AQS，还是各种Atomic开头的原子类，内部都应用到了CAS，最常见的就是我们在并发编程时遇到的i++这种情况。传统的方法肯定是在方法上加上synchronized关键字: public class Test { public volatile int i; public synchronized void add() { i++; } } 但是这种方法在性能上可能会差一点，我们还可以使用AtomicInteger，就可以保证i原子的++了。 public class Test { public AtomicInteger i; public void add() { i.getAndIncrement(); } } 我们来看getAndIncrement的内部： public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); } 再深入到getAndAddInt(): public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; } 现在重点来了，compareAndSwapInt（var1, var2, var5, var5 + var4）其实换成compareAndSwapInt（obj, offset, expect, update）比较清楚，意思就是如果obj内的value和expect相等，就证明没有其他线程改变过这个变量，那么就更新它为update，如果这一步的CAS没有成功，那就采用自旋的方式继续进行CAS操作，取出乍一看这也是两个步骤了啊，其实在JNI里是借助于一个CPU指令完成的。所以还是原子操作。 CAS底层原理我们可以看到compareAndSwapInt实现是在Unsafe_CompareAndSwapInt里面，再深入到Unsafe_CompareAndSwapInt: UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper(\"Unsafe_CompareAndSwapInt\"); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e; UNSAFE_END p是取出的对象，addr是p中offset处的地址，最后调用了Atomic::cmpxchg(x, addr, e), 其中参数x是即将更新的值，参数e是原内存的值。代码中能看到cmpxchg有基于各个平台的实现。 CAS的问题ABA问题CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。这就是CAS的ABA问题。 常见的解决思路是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A-B-A 就会变成1A-2B-3A。 目前在JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 public class ABADemo { static AtomicInteger atomicInteger = new AtomicInteger(100); static AtomicStampedReference&lt;Integer> atomicStampedReference = new AtomicStampedReference&lt;>(100, 1); public static void main(String[] args) { System.out.println(\"=====ABA的问题产生=====\"); new Thread(() -> { atomicInteger.compareAndSet(100, 101); atomicInteger.compareAndSet(101, 100); }, \"t1\").start(); new Thread(() -> { // 保证线程1完成一次ABA问题 try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(atomicInteger.compareAndSet(100, 2020) + \" \" + atomicInteger.get()); }, \"t2\").start(); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"=====解决ABA的问题=====\"); new Thread(() -> { int stamp = atomicStampedReference.getStamp(); // 第一次获取版本号 System.out.println(Thread.currentThread().getName() + \" 第1次版本号\" + stamp); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } atomicStampedReference.compareAndSet(100, 101, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"\\t第2次版本号\" + atomicStampedReference.getStamp()); atomicStampedReference.compareAndSet(101, 100, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"\\t第3次版本号\" + atomicStampedReference.getStamp()); }, \"t3\").start(); new Thread(() -> { int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + \"\\t第1次版本号\" + stamp); try { TimeUnit.SECONDS.sleep(4); } catch (InterruptedException e) { e.printStackTrace(); } boolean result = atomicStampedReference.compareAndSet(100, 2020, stamp, stamp + 1); System.out.println(Thread.currentThread().getName() + \"\\t修改是否成功\" + result + \"\\t当前最新实际版本号：\" + atomicStampedReference.getStamp()); System.out.println(Thread.currentThread().getName() + \"\\t当前最新实际值：\" + atomicStampedReference.getReference()); }, \"t4\").start(); } } 循环时间长开销大上面我们说过如果CAS不成功，则会原地自旋，如果长时间自旋会给CPU带来非常大的执行开销。 参考文章","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://dreamcat.ink/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"深刻理解volatile的一切","slug":"深刻理解volatile的一切","date":"2020-02-12T16:44:43.000Z","updated":"2020-10-30T12:37:34.705Z","comments":true,"path":"2020/02/13/shen-ke-li-jie-volatile-de-yi-qie/","link":"","permalink":"http://dreamcat.ink/2020/02/13/shen-ke-li-jie-volatile-de-yi-qie/","excerpt":"","text":"引言 不得不说，如果谈到volatile只会它的作用：可见性，可序性和不能保证原子性，就太Low了些，因此还得熟悉其中的奥妙才行呀… 可见性volatile的可见性是指当一个变量被volatile修饰后，这个变量就对所有线程均可见。白话点就是说当一个线程修改了一个volatile修饰的变量后，其他线程可以立刻得知这个变量的修改，拿到最这个变量最新的值。 public class VolatileVisibleDemo { // private boolean isReady = true; private volatile boolean isReady = true; void m() { System.out.println(Thread.currentThread().getName() + \" m start...\"); while (isReady) { } System.out.println(Thread.currentThread().getName() + \" m end...\"); } public static void main(String[] args) { VolatileVisibleDemo demo = new VolatileVisibleDemo(); new Thread(() -> demo.m(), \"t1\").start(); try {TimeUnit.SECONDS.sleep(1);} catch (InterruptedException e) {e.printStackTrace();} demo.isReady = false; // 刚才一秒过后开始执行 } } 分析：一开始isReady为true，m方法中的while会一直循环，而主线程开启开线程之后会延迟1s将isReady赋值为false，若不加volatile修饰，则程序一直在运行，若加了volatile修饰，则程序最后会输出t1 m end… 有序性有序性是指程序代码的执行是按照代码的实现顺序来按序执行的；volatile的有序性特性则是指禁止JVM指令重排优化。 public class Singleton { private static Singleton instance = null; //private static volatile Singleton instance = null; private Singleton() { } public static Singleton getInstance() { //第一次判断 if(instance == null) { synchronized (Singleton.class) { if(instance == null) { //初始化，并非原子操作 instance = new Singleton(); } } } return instance; } } 上面的代码是一个很常见的单例模式实现方式，但是上述代码在多线程环境下是有问题的。为什么呢，问题出在instance对象的初始化上，因为instance = new Singleton();这个初始化操作并不是原子的，在JVM上会对应下面的几条指令： memory =allocate(); //1. 分配对象的内存空间 ctorInstance(memory); //2. 初始化对象 instance =memory; //3. 设置instance指向刚分配的内存地址 上面三个指令中，步骤2依赖步骤1，但是步骤3不依赖步骤2，所以JVM可能针对他们进行指令重拍序优化，重排后的指令如下： memory =allocate(); //1. 分配对象的内存空间 instance =memory; //3. 设置instance指向刚分配的内存地址 ctorInstance(memory); //2. 初始化对象 这样优化之后，内存的初始化被放到了instance分配内存地址的后面，这样的话当线程1执行步骤3这段赋值指令后，刚好有另外一个线程2进入getInstance方法判断instance不为null，这个时候线程2拿到的instance对应的内存其实还未初始化，这个时候拿去使用就会导致出错。 所以我们在用这种方式实现单例模式时，会使用volatile关键字修饰instance变量，这是因为volatile关键字除了可以保证变量可见性之外，还具有防止指令重排序的作用。当用volatile修饰instance之后，JVM执行时就不会对上面提到的初始化指令进行重排序优化，这样也就不会出现多线程安全问题了。 不能保证原子性volatile关键字能保证变量的可见性和代码的有序性，但是不能保证变量的原子性，下面我再举一个volatile与原子性的例子： public class VolatileAtomicDemo { public static volatile int count = 0; public static void increase() { count++; } public static void main(String[] args) { Thread[] threads = new Thread[20]; for(int i = 0; i &lt; threads.length; i++) { threads[i] = new Thread(() -> { for(int j = 0; j &lt; 1000; j++) { increase(); } }); threads[i].start(); } //等待所有累加线程结束 while (Thread.activeCount() > 1) { Thread.yield(); } System.out.println(count); } } 上面这段代码创建了20个线程，每个线程对变量count进行1000次自增操作，如果这段代码并发正常的话，结果应该是20000，但实际运行过程中经常会出现小于20000的结果，因为count++这个自增操作不是原子操作。可参照这张图 内存屏障Java的Volatile的特征是任何读都能读到最新值，本质上是JVM通过内存屏障来实现的；为了实现volatile内存语义，JMM会分别限制重排序类型。下面是JMM针对编译器制定的volatile重排序规则表： 是否能重排序 第二个操作 第一个操作 普通读/写 volatile读 volatile写 普通读/写 no volatile读 no no no volatile写 no no 从上表我们可以看出： 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能，为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略： 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 volatile写插入内存指令图： 上图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。 这里比较有意思的是volatile写后面的StoreLoad屏障。这个屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面，是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在这里采取了保守策略：在每个volatile写的后面或在每个volatile读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里我们可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。 volatile读插入内存指令图： 上图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。下面我们通过具体的示例代码来说明： class VolatileBarrierExample { int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() { int i = v1; //第一个volatile读 int j = v2; // 第二个volatile读 a = i + j; //普通写 v1 = i + 1; // 第一个volatile写 v2 = j * 2; //第二个 volatile写 } } 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化： 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器常常会在这里插入一个StoreLoad屏障。 volatile汇编：0x000000011214bb49: mov %rdi,%rax 0x000000011214bb4c: dec %eax 0x000000011214bb4e: mov %eax,0x10(%rsi) 0x000000011214bb51: lock addl $0x0,(%rsp) ;*putfield v1 ; - com.earnfish.VolatileBarrierExample::readAndWrite@21 (line 35) 0x000000011214bb56: imul %edi,%ebx 0x000000011214bb59: mov %ebx,0x14(%rsi) 0x000000011214bb5c: lock addl $0x0,(%rsp) ;*putfield v2 ; - com.earnfish.VolatileBarrierExample::readAndWrite@28 (line 36) 对应的Java： v1 = i - 1; // 第一个volatile写 v2 = j * i; // 第二个volatile写 可见其本质是通过一个lock指令来实现的。那么lock是什么意思呢？ 它的作用是使得本CPU的Cache写入了内存，该写入动作也会引起别的CPU invalidate其Cache。所以通过这样一个空操作，可让前面volatile变量的修改对其他CPU立即可见。 锁住内存 任何读必须在写完成之后再执行 使其它线程这个值的栈缓存失效 参考volatile内存屏障","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://dreamcat.ink/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Java锁机制","slug":"Java多线程-锁机制","date":"2020-02-09T17:22:20.000Z","updated":"2020-10-30T12:43:52.337Z","comments":true,"path":"2020/02/10/java-duo-xian-cheng-suo-ji-zhi/","link":"","permalink":"http://dreamcat.ink/2020/02/10/java-duo-xian-cheng-suo-ji-zhi/","excerpt":"","text":"引言 \u0011Java常见的几种锁，比如： 公平锁/非公平锁 可重入锁 独享锁/共享锁 互斥锁/读写锁 乐观锁/悲观锁 分段锁 偏向锁/轻量级锁/重量级锁 自旋锁 公平锁/非公平锁公平锁指多个线程按照申请锁的顺序来获取锁。非公平锁指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象（很长时间都没获取到锁-非洲人…），ReentrantLock，了解一下。 可重入锁可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁，典型的synchronized，了解一下 synchronized void setA() throws Exception { Thread.sleep(1000); setB(); // 因为获取了setA()的锁，此时调用setB()将会自动获取setB()的锁，如果不自动获取的话方法B将不会执行 } synchronized void setB() throws Exception { Thread.sleep(1000); } 独享锁/共享锁 独享锁：是指该锁一次只能被一个线程所持有。 共享锁：是该锁可被多个线程所持有。 互斥锁/读写锁上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是其具体的实现 乐观锁/悲观锁 乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待兵法同步的角度。 悲观锁认为对于同一个人数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出现问题。 乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作时没有事情的。 悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁带来大量的性能提升。 悲观锁在Java中的使用，就是利用各种锁。乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子类操作的更新。重量级锁是悲观锁的一种，自旋锁、轻量级锁与偏向锁属于乐观锁 分段锁 分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来哦实现高效的并发操作。 ** 以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7与JDK8中HashMap的实现）的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表；同时又是ReentrantLock（Segment继承了ReentrantLock）** 当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。 分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。 偏向锁/轻量级锁/重量级锁 这三种锁是锁的状态，并且是针对Synchronized。在Java5通过引入锁升级的机制来实现高效Synchronized。这三种锁的状态是通过对象监视器在对象头中的字段来表明的。偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁。降低获取锁的代价。 偏向锁的适用场景：始终只有一个线程在执行代码块，在它没有执行完释放锁之前，没有其它线程去执行同步快，在锁无竞争的情况下使用，一旦有了竞争就升级为轻量级锁，升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致stop the word操作；在有锁竞争时，偏向锁会多做很多额外操作，尤其是撤销偏向锁的时候会导致进入安全点，安全点会导致stw，导致性能下降，这种情况下应当禁用。 轻量级锁是指当锁是偏向锁的时候，被另一个线程锁访问，偏向锁就会升级为轻量级锁，其他线程会通过自选的形式尝试获取锁，不会阻塞，提高性能。 重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。 自旋锁 在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。 自旋锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。 自旋锁尽可能的减少线程的阻塞，适用于锁的竞争不激烈，且占用锁时间非常短的代码来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起再唤醒的操作的消耗。 但是如果锁的竞争激烈，或者持有锁的线程需要长时间占用锁执行同步块，这时候就不适用使用自旋锁了，因为自旋锁在获取锁前一直都是占用cpu做无用功，同时有大量线程在竞争一个锁，会导致获取锁的时间很长，线程自旋的消耗大于线程阻塞挂起操作的消耗，其它需要cpu的线程又不能获取到cpu，造成cpu的浪费。 Java锁总结Java锁机制可归为Sychornized锁和Lock锁两类。Synchronized是基于JVM来保证数据同步的，而Lock则是硬件层面，依赖特殊的CPU指令来实现数据同步的。 Synchronized是一个非公平、悲观、独享、互斥、可重入的重量级锁。 ReentrantLock是一个默认非公平但可实现公平的、悲观、独享、互斥、可重入、重量级锁。 ReentrantReadWriteLock是一个默认非公平但可实现公平的、悲观、写独享、读共享、读写、可重入、重量级锁。","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://dreamcat.ink/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Spring-SpringBoot-HelloWorld项目启动流程及细节和原理","slug":"Spring-SpringBoot-HelloWorld项目启动流程及细节和原理","date":"2020-02-03T04:03:34.000Z","updated":"2020-10-29T15:48:51.130Z","comments":true,"path":"2020/02/03/spring-springboot-helloworld-xiang-mu-qi-dong-liu-cheng-ji-xi-jie-he-yuan-li/","link":"","permalink":"http://dreamcat.ink/2020/02/03/spring-springboot-helloworld-xiang-mu-qi-dong-liu-cheng-ji-xi-jie-he-yuan-li/","excerpt":"","text":"引言 \u0011SpringBoot大大简化了Spring项目的配置，虽然很舒服，但是为了面试等，还是要知道里面的细节和原理呀，再此举个HelloWorld的例子 声明 使用Idea分析，并使用Idea创建SpringBoot的工程。 项目结构就不一一详细说了，大家肯定都知道每个文件是干什么的。 这里主要是负责细看其中的原理和细节。 版本：2.2.4.RELEASE SpringApplication实例先看源码： public SpringApplication(ResourceLoader resourceLoader, Class... primarySources) { this.sources = new LinkedHashSet(); // 1. this.bannerMode = Mode.CONSOLE; this.logStartupInfo = true; this.addCommandLineProperties = true; this.headless = true; this.registerShutdownHook = true; this.additionalProfiles = new HashSet(); this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet(Arrays.asList(primarySources)); this.webApplicationType = this.deduceWebApplicationType(); this.setInitializers(this.getSpringFactoriesInstances(ApplicationContextInitializer.class)); // 3. this.setListeners(this.getSpringFactoriesInstances(ApplicationListener.class)); // 4. this.mainApplicationClass = this.deduceMainApplicationClass(); // 5. } com.example.helloworld.HelloworldApplication放入到Set的集合中 判断是否为Web环境：存在（javax.servlet.Servlet &amp;&amp; org.springframework.web.context.ConfigurableWebApplicationContext ）类 创建并初始化ApplicationInitializer列表 （spring.factories） 创建并初始化ApplicationListener列表 （spring.factories） 初始化主类mainApplicatioClass (DemoApplication) 总结：上面就是SpringApplication初始化的代码，new SpringApplication()没做啥事情 ，主要加载了META-INF/spring.factories 下面定义的事件监听器接口实现类 ConfigurableApplicationContext的run方法看源码，都可以找的到 public ConfigurableApplicationContext run(String... args) { StopWatch stopWatch = new StopWatch(); // 1. stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter> exceptionReporters = new ArrayList(); this.configureHeadlessProperty(); SpringApplicationRunListeners listeners = this.getRunListeners(args); // 2. listeners.starting(); // Collection exceptionReporters; try { ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); // ConfigurableEnvironment environment = this.prepareEnvironment(listeners, applicationArguments); // this.configureIgnoreBeanInfo(environment); // Banner printedBanner = this.printBanner(environment); // context = this.createApplicationContext(); // exceptionReporters = this.getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[]{ConfigurableApplicationContext.class}, context); this.prepareContext(context, environment, listeners, applicationArguments, printedBanner);// this.refreshContext(context); // this.afterRefresh(context, applicationArguments); // stopWatch.stop();// if (this.logStartupInfo) { (new StartupInfoLogger(this.mainApplicationClass)).logStarted(this.getApplicationLog(), stopWatch); } listeners.started(context); this.callRunners(context, applicationArguments); } catch (Throwable var10) { this.handleRunFailure(context, var10, exceptionReporters, listeners); throw new IllegalStateException(var10); } try { listeners.running(context); return context; } catch (Throwable var9) { this.handleRunFailure(context, var9, exceptionReporters, (SpringApplicationRunListeners)null); throw new IllegalStateException(var9); } } 创建计时器StopWatch 获取SpringApplicationRunListeners并启动 创建ApplicationArguments 创建并初始化ConfigurableEnvironment 打印Banner 创建ConfigurableApplicationContext 准备ConfigurableApplicationContext 刷新ConfigurableApplicationContext，这个refreshContext()加载了bean，还启动了内置web容器，需要细细的去看看 容器刷新后动作，啥都没做 计时器停止计时 refreshContext()该源码中其实就是Spring源码的refresh()的源码 不过这里的refresh()是在AbstractApplicationContext抽象类上 其他就不提了，关注点在onrefresh()方法上，但是个空方法，毕竟是抽象类，去找其子类继承的它 debug调试可以找到ServletWebServerApplicationContext ServletWebServerApplicationContext先看个类图吧，很吊 onRefresh()-&gt;createWebServer()-&gt;getWebServerFactory()，此时已经加载了个web容器 可以返回刚才的createWebServer()，然后看factory.getWebServer public WebServer getWebServer(ServletContextInitializer... initializers) { //tomcat这位大哥出现了 Tomcat tomcat = new Tomcat(); File baseDir = (this.baseDirectory != null ? this.baseDirectory : createTempDir(\"tomcat\")); tomcat.setBaseDir(baseDir.getAbsolutePath()); Connector connector = new Connector(this.protocol); tomcat.getService().addConnector(connector); customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); configureEngine(tomcat.getEngine()); for (Connector additionalConnector : this.additionalTomcatConnectors) { tomcat.getService().addConnector(additionalConnector); } prepareContext(tomcat.getHost(), initializers); return getTomcatWebServer(tomcat); } 内置的Tomcat就出现了 总结：run() 方法主要调用了spring容器启动方法扫描配置，加载bean到spring容器中；启动的内置Web容器 @SpringBootApplication主要是三个注解 @SpringBootConfiguration:允许在上下文中注册额外的bean或导入其他配置类。 @EnableAutoConfiguration:启用 SpringBoot 的自动配置机制 @ComponentScan: 扫描常用的注解","categories":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"}]},{"title":"Spring-SpringAOP源码分析","slug":"Spring-SpringAOP源码分析","date":"2020-01-31T12:03:40.000Z","updated":"2020-10-30T12:53:26.585Z","comments":true,"path":"2020/01/31/spring-springaop-yuan-ma-fen-xi/","link":"","permalink":"http://dreamcat.ink/2020/01/31/spring-springaop-yuan-ma-fen-xi/","excerpt":"","text":"引言 使用了SpringBoot框架做项目，难道就不好奇它为什么那么方便吗？简化了很多配置，那么首先就先分析Spring底层的源码和原理 Spring源码分析Spring容器的创建，最重要的就是refresh()[创建刷新]，在该方法内部，有着12大步骤流程。 Spring版本：5.2.1.RELEASE public void refresh() throws BeansException, IllegalStateException { synchronized(this.startupShutdownMonitor) { this.prepareRefresh(); // 1. ConfigurableListableBeanFactory beanFactory = this.obtainFreshBeanFactory(); // 2. this.prepareBeanFactory(beanFactory); // 3. try { this.postProcessBeanFactory(beanFactory); // 4. this.invokeBeanFactoryPostProcessors(beanFactory); // 5. this.registerBeanPostProcessors(beanFactory); // 6. this.initMessageSource(); // 7. this.initApplicationEventMulticaster(); // 8. this.onRefresh(); // 9. this.registerListeners(); // 10. this.finishBeanFactoryInitialization(beanFactory); // 11. this.finishRefresh(); // 12. } catch (BeansException var9) { if (this.logger.isWarnEnabled()) { this.logger.warn(\"Exception encountered during context initialization - cancelling refresh attempt: \" + var9); } this.destroyBeans(); this.cancelRefresh(var9); throw var9; } finally { this.resetCommonCaches(); } } } prepareRefresh()刷新前的预处理如一些属性设置，环境，容器的一些早起事件等，主要执行的三个方法： initPropertySource()初始化一些属性设置，子类自定义个性化的属性设置方法； getEnvironment().validateRequiredProperties()检验属性的合法等 earlyApplicationEvents= new LinkedHashSet&lt;ApplicationEvent&gt;();保存容器中的一些早起的事件； obtainFreshBeanFactory()获取BeanFactory，主要也是二个方法： refreshBeanFactory();负责刷新创建BeanFactory 创建了一个默认的this.beanFactory = new DefaultListableBeanFactory(); 设置id； getBeanFactory();回刚才GenericApplicationContext创建的BeanFactory对象； prepareBeanFactory(beanFactory)BeanFactory的预准备工作（BeanFactory进行一些设置）； 设置BeanFactory的类加载器、支持表达式解析器… 添加部分BeanPostProcessor【ApplicationContextAwareProcessor】 设置忽略的自动装配的接口EnvironmentAware、EmbeddedValueResolverAware、xxx； 注册可以解析的自动装配；我们能直接在任何组件中自动注入：BeanFactory、ResourceLoader、ApplicationEventPublisher、ApplicationContext 添加BeanPostProcessor【ApplicationListenerDetector】 添加编译时的AspectJ； 给BeanFactory中注册一些能用的组件；environment【ConfigurableEnvironment】、systemProperties【Map&lt;String, Object&gt;】、systemEnvironment【Map&lt;String, Object&gt; postProcessBeanFactory(beanFactory)BeanFactory准备工作完成后进行的后置处理工作； 子类通过重写这个方法来在BeanFactory创建并预准备完成以后做进一步的设置 invokeBeanFactoryPostProcessors(beanFactory)执行BeanFactoryPostProcessor的方法，BeanFactoryPostProcessor：BeanFactory的后置处理器。在BeanFactory标准初始化之后执行的； 分别是两个接口：两个接口：BeanFactoryPostProcessor、BeanDefinitionRegistryPostProcessor 先执行BeanDefinitionRegistryPostProcessor(注册bean的一堆定义信息) 获取所有的BeanDefinitionRegistryPostProcessor； 看先执行实现了PriorityOrdered优先级接口的BeanDefinitionRegistryPostProcessor、postProcessor.postProcessBeanDefinitionRegistry(registry) 在执行实现了Ordered顺序接口的BeanDefinitionRegistryPostProcessor；postProcessor.postProcessBeanDefinitionRegistry(registry) 最后执行没有实现任何优先级或者是顺序接口的BeanDefinitionRegistryPostProcessors；postProcessor.postProcessBeanDefinitionRegistry(registry) 再执行BeanFactoryPostProcessor的方法 获取所有的BeanFactoryPostProcessor 看先执行实现了PriorityOrdered优先级接口的BeanFactoryPostProcessor、postProcessor.postProcessBeanFactory() 在执行实现了Ordered顺序接口的BeanFactoryPostProcessor；postProcessor.postProcessBeanFactory() 最后执行没有实现任何优先级或者是顺序接口的BeanFactoryPostProcessor；postProcessor.postProcessBeanFactory() registerBeanPostProcessors(beanFactory)注册BeanPostProcessor（Bean的后置处理器）【 intercept bean creation】 不同接口类型的BeanPostProcessor；在Bean创建前后的执行时机是不一样的： BeanPostProcessor DestructionAwareBeanPostProcessor InstantiationAwareBeanPostProcessor SmartInstantiationAwareBeanPostProcessor MergedBeanDefinitionPostProcessor【internalPostProcessors】 获取所有的 BeanPostProcessor;后置处理器都默认可以通过PriorityOrdered、Ordered接口来执行优先级 先注册PriorityOrdered优先级接口的BeanPostProcessor；把每一个BeanPostProcessor；添加到BeanFactory中 再注册Ordered接口的 最后注册没有实现任何优先级接口的 最终注册MergedBeanDefinitionPostProcessor； 注册一个ApplicationListenerDetector；来在Bean创建完成后检查是否是ApplicationListener，如果是applicationContext.addApplicationListener((ApplicationListener&lt;?&gt;) bean); initMessageSource()初始化MessageSource组件（做国际化功能；消息绑定，消息解析）； 获取BeanFactory 看容器中是否有id为messageSource的，类型是MessageSource的组件;MessageSource：取出国际化配置文件中的某个key的值；能按照区域信息获取； 如果有赋值给messageSource 如果没有自己创建一个DelegatingMessageSource； 把创建好的MessageSource注册在容器中，以后获取国际化配置文件的值的时候，可以自动注入MessageSource； beanFactory.registerSingleton(MESSAGE_SOURCE_BEAN_NAME, this.messageSource); MessageSource.getMessage(String code, Object[] args, String defaultMessage, Locale locale); initApplicationEventMulticaster()初始化事件派发器； 获取BeanFactory 从BeanFactory中获取applicationEventMulticaster的ApplicationEventMulticaster； 如果上一步没有配置；创建一个SimpleApplicationEventMulticaster 将创建的ApplicationEventMulticaster添加到BeanFactory中，以后其他组件直接自动注入 onRefresh()留给子容器（子类） 子类重写这个方法，在容器刷新的时候可以自定义逻辑； registerListeners()给容器中将所有项目里面的ApplicationListener注册进来； 从容器中拿到所有的ApplicationListener 将每个监听器添加到事件派发器中；getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName); 派发之前步骤产生的事件； finishBeanFactoryInitialization(beanFactory)初始化所有剩下的单实例bean； 获取容器中的所有Bean，依次进行初始化和创建对象 获取Bean的定义信息；RootBeanDefinition 判断Bean是非抽象的，是单实例的，是非懒加载； 判断是否是FactoryBean；是否是实现FactoryBean接口的Bean； 不是工厂Bean。利用getBean(beanName);创建对象 getBean(beanName)；–&gt;ioc.getBean(); doGetBean(name, null, null, false); 先获取缓存中保存的单实例Bean。如果能获取到说明这个Bean之前被创建过（所有创建过的单实例Bean都会被缓存起来）private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;String, Object&gt;(256); 缓存中获取不到，开始Bean的创建对象流程； 标记当前bean已经被创建 获取Bean的定义信息； 【获取当前Bean依赖的其他Bean;如果有按照getBean()把依赖的Bean先创建出来；】 启动单实例Bean的创建流程； createBean(beanName, mbd, args); Object bean = resolveBeforeInstantiation(beanName, mbdToUse);让BeanPostProcessor先拦截返回代理对象；【InstantiationAwareBeanPostProcessor】：提前执行；先触发：postProcessBeforeInstantiation()；如果有返回值：触发postProcessAfterInitialization()； 如果前面的InstantiationAwareBeanPostProcessor没有返回代理对象；调用4） Object beanInstance = doCreateBean(beanName, mbdToUse, args);创建Bean 【创建Bean实例】；createBeanInstance(beanName, mbd, args);利用工厂方法或者对象的构造器创建出Bean实例； applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName);调用MergedBeanDefinitionPostProcessor的postProcessMergedBeanDefinition(mbd, beanType, beanName); 【Bean属性赋值】populateBean(beanName, mbd, instanceWrapper);赋值之前： 拿到InstantiationAwareBeanPostProcessor后置处理器；postProcessAfterInstantiation(); 拿到InstantiationAwareBeanPostProcessor后置处理器；postProcessPropertyValues(); 应用Bean属性的值；为属性利用setter方法等进行赋值；applyPropertyValues(beanName, mbd, bw, pvs); 【Bean初始化】initializeBean(beanName, exposedObject, mbd); 【执行Aware接口方法】invokeAwareMethods(beanName, bean);执行xxxAware接口的方法 BeanNameAware\\BeanClassLoaderAware\\BeanFactoryAware 【执行后置处理器初始化之前】applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); 【执行初始化方法】invokeInitMethods(beanName, wrappedBean, mbd); 是否是InitializingBean接口的实现；执行接口规定的初始化； 是否自定义初始化方法； 【执行后置处理器初始化之后】applyBeanPostProcessorsAfterInitialization–&gt;BeanPostProcessor.postProcessAfterInitialization()； 注册Bean的销毁方法； 将创建的Bean添加到缓存中singletonObjects；ioc容器就是这些Map；很多的Map里面保存了单实例Bean，环境信息。。。。； 所有Bean都利用getBean创建完成以后；检查所有的Bean是否是SmartInitializingSingleton接口的；如果是；就执行afterSingletonsInstantiated()； finishRefresh()完成BeanFactory的初始化创建工作；IOC容器就创建完成； initLifecycleProcessor();初始化和生命周期有关的后置处理器；LifecycleProcessor 默认从容器中找是否有lifecycleProcessor的组件【LifecycleProcessor】；如果没有new DefaultLifecycleProcessor();加入到容器； 写一个LifecycleProcessor的实现类，可以在BeanFactory–&gt;void onRefresh();void onClose(); getLifecycleProcessor().onRefresh();拿到前面定义的生命周期处理器（BeanFactory）；回调onRefresh()； publishEvent(new ContextRefreshedEvent(this));发布容器刷新完成事件； liveBeansView.registerApplicationContext(this); Spring源码总结 Spring容器在启动的时候，先会保存所有注册进来的Bean的定义信息； xml注册bean； 注解注册Bean；@Service、@Component、@Bean、xxx Spring容器会合适的时机创建这些Bean 用到这个bean的时候；利用getBean创建bean；创建好以后保存在容器中； 统一创建剩下所有的bean的时候；finishBeanFactoryInitialization()； 后置处理器；BeanPostProcessor 每一个bean创建完成，都会使用各种后置处理器进行处理；来增强bean的功能；比如 AutowiredAnnotationBeanPostProcessor:处理自动注入 AnnotationAwareAspectJAutoProxyCreator:来做AOP功能； xxx 事件驱动模型； ApplicationListener；事件监听； ApplicationEventMulticaster；事件派发： SpringAOP源码分析实际上，Spring容器过程当中，如果开启了AOP功能，那么会创建一个后置器[AnnotationAwareAspectJAutoProxyCreator]，看到UML图就明白了它的特点了。 AOP具体流程，就不赘述了，毕竟其中有很多Spring容器创建的很多步骤，直接看总结即可包括了整个流程了 总结 @EnableAspectJAutoProxy 开启AOP功能 @EnableAspectJAutoProxy 会给容器中注册一个组件 AnnotationAwareAspectJAutoProxyCreator AnnotationAwareAspectJAutoProxyCreator是一个后置处理器； 容器的创建流程： registerBeanPostProcessors（）注册后置处理器；创建AnnotationAwareAspectJAutoProxyCreator对象（Spring源码） finishBeanFactoryInitialization（）初始化剩下的单实例bean（Spring源码） 创建业务逻辑组件和切面组件 AnnotationAwareAspectJAutoProxyCreator拦截组件的创建过程 组件创建完之后，判断组件是否需要增强；是-&gt;切面的通知方法，包装成增强器（Advisor）;给业务逻辑组件创建一个代理对象（cglib）； 执行目标方法： 代理对象执行目标方法 CglibAopProxy.intercept()； 得到目标方法的拦截器链（增强器包装成拦截器MethodInterceptor） 利用拦截器的链式机制，依次进入每一个拦截器进行执行； 效果： 正常执行：前置通知-》目标方法-》后置通知-》返回通知 出现异常：前置通知-》目标方法-》后置通知-》异常通知","categories":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"}]},{"title":"github表情图标","slug":"github表情图标","date":"2020-01-20T05:15:16.000Z","updated":"2020-10-29T15:52:42.146Z","comments":true,"path":"2020/01/20/github-biao-qing-tu-biao/","link":"","permalink":"http://dreamcat.ink/2020/01/20/github-biao-qing-tu-biao/","excerpt":"","text":"引言 github的一些表情图标的含义，方便做文档，从而显得有规范，高逼格一点。 放一个emoji网站：emoji参考 emoji emoji代码 commit提交说明 :art: (调色板) :art: 改进代码结构/代码格式 :zap: (闪电):racehorse: (赛马) :zap: :racehorse: 提升性能 :fire: (火焰) :fire: 移除代码或文件 :bug: (bug) :bug: 修复 bug :ambulance: (急救车) :ambulance: 重要补丁 :sparkles: (火花) :sparkles: 引入新功能 :memo: (备忘录) :memo: 撰写文档 :rocket: (火箭) :rocket: 部署功能 :lipstick: (口红) :lipstick: 更新 UI 和样式文件 :tada: (庆祝) :tada: 初次提交 :white_check_mark: (白色复选框) :white_check_mark: 增加测试 :lock: (锁) :lock: 修复安全问题 :apple: (苹果) :apple: 修复 macOS 下的问题 :penguin: (企鹅) :penguin: 修复 Linux 下的问题 :checkered_flag: (旗帜) :checked_flag: 修复 Windows 下的问题 :bookmark: (书签) :bookmark: 发行/版本标签 :rotating_light: (警车灯) :rotating_light: 移除 linter 警告 :construction: (施工) :construction: 工作进行中 :green_heart: (绿心) :green_heart: 修复 CI 构建问题 :arrow_down: (下降箭头) :arrow_down: 降级依赖 :arrow_up: (上升箭头) :arrow_up: 升级依赖 :construction_worker: (工人) :construction_worker: 添加 CI 构建系统 :chart_with_upwards_trend: (上升趋势图) :chart_with_upwards_trend: 添加分析或跟踪代码 :hammer: (锤子) :hammer: 重大重构 :heavy_minus_sign: (减号) :heavy_minus_sign: 减少一个依赖 :whale: (鲸鱼) :whale: Docker 相关工作 :heavy_plus_sign: (加号) :heavy_plug_sign: 增加一个依赖 :wrench: (扳手) :wrench: 修改配置文件 :globe_with_meridians: (地球) :globe_with_meridians: 国际化与本地化 :pencil2: (铅笔) :pencil2: 修复 typo","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"github","slug":"github","permalink":"http://dreamcat.ink/tags/github/"}]},{"title":"Spring-Springboot和Dubbo整合教程","slug":"Spring-Springboot和Dubbo整合教程","date":"2019-12-13T02:59:36.000Z","updated":"2020-10-29T15:48:44.652Z","comments":true,"path":"2019/12/13/spring-springboot-he-dubbo-zheng-he-jiao-cheng/","link":"","permalink":"http://dreamcat.ink/2019/12/13/spring-springboot-he-dubbo-zheng-he-jiao-cheng/","excerpt":"","text":"引言 最近在做微服务的项目，基于Springboot、Dubbo等架构。环境等就不在这篇文章中介绍了，该文章介绍如何快速整合以及测试。 前提 前提安装Zookeeper、Dubbo和Tomcat等 项目结构 dream-parent：所有的子模块的pom包的版本号由perent管理。 dream-commons：公共的配置或者util等 user-service：用户服务提供者 dream-user：用户消费者 user-api：用户管理服务接口 user-provide：实现并发布 bean和mapper就不解释了 services是实现api的服务 POM包管理parent&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;parent> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-parent&lt;/artifactId> &lt;version>2.1.6.RELEASE&lt;/version> &lt;relativePath/> &lt;/parent> &lt;groupId>com.dream.parent&lt;/groupId> &lt;artifactId>dream-parent&lt;/artifactId> &lt;version>1.0-SANPSHOT&lt;/version> &lt;packaging>pom&lt;/packaging> &lt;properties> &lt;project.build.sourceEncoding>UTF-8&lt;/project.build.sourceEncoding> &lt;project.reporting.outputEncoding>UTF-8&lt;/project.reporting.outputEncoding> &lt;java.version>1.8&lt;/java.version> &lt;lombok.version>1.18.8&lt;/lombok.version> &lt;fastjson.version>1.2.46&lt;/fastjson.version> &lt;mybatis-plus.version>3.3.0&lt;/mybatis-plus.version> &lt;velocity.version>2.1&lt;/velocity.version> &lt;druid.version>1.1.10&lt;/druid.version> &lt;dubbo-spring.version>0.2.0&lt;/dubbo-spring.version> &lt;/properties> &lt;dependencyManagement> &lt;dependencies> &lt;!--lombok--> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;version>${lombok.version}&lt;/version> &lt;/dependency> &lt;!--fastjson--> &lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>fastjson&lt;/artifactId> &lt;version>${fastjson.version}&lt;/version> &lt;/dependency> &lt;!--dubbo-spring--> &lt;dependency> &lt;groupId>com.alibaba.boot&lt;/groupId> &lt;artifactId>dubbo-spring-boot-starter&lt;/artifactId> &lt;version>${dubbo-spring.version}&lt;/version> &lt;/dependency> &lt;!--mybatis-plus--> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId> &lt;version>${mybatis-plus.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-generator&lt;/artifactId> &lt;version>${mybatis-plus.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.velocity&lt;/groupId> &lt;artifactId>velocity-engine-core&lt;/artifactId> &lt;version>${velocity.version}&lt;/version> &lt;/dependency> &lt;!--后期要用上--> &lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>druid&lt;/artifactId> &lt;version>${druid.version}&lt;/version> &lt;/dependency> &lt;/dependencies> &lt;/dependencyManagement> &lt;build> &lt;plugins> &lt;plugin> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-maven-plugin&lt;/artifactId> &lt;/plugin> &lt;/plugins> &lt;/build> &lt;/project> user-service&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;groupId>com.dream.user&lt;/groupId> &lt;artifactId>user-service&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;modules> &lt;module>user-api&lt;/module> &lt;module>user-provider&lt;/module> &lt;/modules> &lt;packaging>pom&lt;/packaging> &lt;/project> user-api&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;groupId>com.dream.user&lt;/groupId> &lt;artifactId>user-api&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;packaging>jar&lt;/packaging> &lt;parent> &lt;groupId>com.dream.parent&lt;/groupId> &lt;artifactId>dream-parent&lt;/artifactId> &lt;version>1.0-SANPSHOT&lt;/version> &lt;/parent> &lt;dependencies> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId> &lt;/dependency> &lt;/dependencies> &lt;/project> user-provider&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;parent> &lt;groupId>com.dream.parent&lt;/groupId> &lt;artifactId>dream-parent&lt;/artifactId> &lt;version>1.0-SANPSHOT&lt;/version> &lt;relativePath/> &lt;!-- lookup parent from repository --> &lt;/parent> &lt;groupId>com.dream.user&lt;/groupId> &lt;artifactId>user-provider&lt;/artifactId> &lt;version>0.0.1-SNAPSHOT&lt;/version> &lt;name>user-provider&lt;/name> &lt;description>Demo project for Spring Boot&lt;/description> &lt;properties> &lt;java.version>1.8&lt;/java.version> &lt;/properties> &lt;dependencies> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-test&lt;/artifactId> &lt;!--&lt;scope>test&lt;/scope>--> &lt;!--&lt;exclusions>--> &lt;!--&lt;exclusion>--> &lt;!--&lt;groupId>org.junit.vintage&lt;/groupId>--> &lt;!--&lt;artifactId>junit-vintage-engine&lt;/artifactId>--> &lt;!--&lt;/exclusion>--> &lt;!--&lt;/exclusions>--> &lt;/dependency> &lt;dependency> &lt;groupId>com.dream.user&lt;/groupId> &lt;artifactId>user-api&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-jdbc&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>com.alibaba.boot&lt;/groupId> &lt;artifactId>dubbo-spring-boot-starter&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;/dependency> &lt;/dependencies> &lt;build> &lt;plugins> &lt;plugin> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-maven-plugin&lt;/artifactId> &lt;/plugin> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-compiler-plugin&lt;/artifactId> &lt;configuration> &lt;source>6&lt;/source> &lt;target>6&lt;/target> &lt;/configuration> &lt;/plugin> &lt;/plugins> &lt;/build> &lt;/project> dream-user&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;parent> &lt;groupId>com.dream.parent&lt;/groupId> &lt;artifactId>dream-parent&lt;/artifactId> &lt;version>1.0-SANPSHOT&lt;/version> &lt;/parent> &lt;groupId>com.dream.user&lt;/groupId> &lt;artifactId>dream-user&lt;/artifactId> &lt;version>0.0.1-SNAPSHOT&lt;/version> &lt;name>dream-user&lt;/name> &lt;description>Demo project for Spring Boot&lt;/description> &lt;properties> &lt;java.version>1.8&lt;/java.version> &lt;/properties> &lt;dependencies> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-test&lt;/artifactId> &lt;scope>test&lt;/scope> &lt;exclusions> &lt;exclusion> &lt;groupId>org.junit.vintage&lt;/groupId> &lt;artifactId>junit-vintage-engine&lt;/artifactId> &lt;/exclusion> &lt;/exclusions> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>com.alibaba.boot&lt;/groupId> &lt;artifactId>dubbo-spring-boot-starter&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-jdbc&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>com.dream.user&lt;/groupId> &lt;artifactId>user-api&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;/dependency> &lt;/dependencies> &lt;build> &lt;plugins> &lt;plugin> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-maven-plugin&lt;/artifactId> &lt;/plugin> &lt;/plugins> &lt;/build> &lt;/project> 测试User表 sql表语句 CREATE TABLE `user` ( `id` int(10) NOT NULL AUTO_INCREMENT COMMENT '编号', `name` varchar(16) NOT NULL COMMENT '姓名', `gender` varchar(4) NOT NULL COMMENT '性别', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8; INSERT INTO `user` (`id`, `name`, `gender`) VALUES (1, 'Maifeng', '男'), (2, 'Liumeng', '女'), (3, 'haha', '男'), (4, 'lala', '男'), (5, 'biangbiang', '男'), (6, 'huhu', '女'); User模型和UserService接口 存放在bean和api中 User模型/** * @program dshop * @description: 用户实体 * @author: mf * @create: 2019/12/12 16:47 */ package com.dream.user.dal.bean; import com.baomidou.mybatisplus.extension.activerecord.Model; import lombok.Data; @Data public class User extends Model&lt;User> { private Integer id; private String name; private String gender; } IUserService接口package com.dream.user; public interface IUserService { String Hello(); } user-service提供者yml配置文件server: port: 8081 spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/rsd?serverTimeZone=UTC username: root password: 123 dubbo: application: name: dshop-user-service # 应用名称 registry: # 注册中心 address: 192.168.1.106:2181 protocol: zookeeper protocol: port: 28800 UserMapperpackage com.dream.user.dal.mapper; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.dream.user.dal.bean.User; import org.springframework.stereotype.Component; @Component public interface UserMapper extends BaseMapper&lt;User>{ } UserServiceImpl/** * @program dream * @description: 用户服务 * @author: mf * @create: 2019/12/14 01:22 */ package com.dream.user.services; import com.alibaba.dubbo.config.annotation.Service; import com.dream.user.IUserService; import org.springframework.stereotype.Component; @Component @Service public class UserServiceImpl implements IUserService { @Override public String Hello() { return \"Hello Dubbo...\"; } } UserProviderApplication启动文件package com.dream.user; import com.alibaba.dubbo.config.spring.context.annotation.EnableDubbo; import org.mybatis.spring.annotation.MapperScan; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication @EnableDubbo @MapperScan(\"com.dream.user.dal.mapper\") public class UserProviderApplication { public static void main(String[] args) { SpringApplication.run(UserProviderApplication.class, args); } } UserMapperTest测试文件/** * @program dream * @description: UserMapper测试 * @author: mf * @create: 2019/12/14 01:02 */ package com.dream.user; import com.baomidou.mybatisplus.core.conditions.query.QueryWrapper; import com.dream.user.dal.bean.User; import com.dream.user.dal.mapper.UserMapper; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; import java.util.List; @RunWith(SpringRunner.class) @SpringBootTest public class UserMapperTest { @Autowired private UserMapper userMapper; @Test public void testAllUsers() { List&lt;User> users = userMapper.selectList(new QueryWrapper&lt;User>()); for (User user : users) { System.out.println(user); } } } UserServiceTest/** * @program dream * @description: 用户服务测试 * @author: mf * @create: 2019/12/14 01:26 */ package com.dream.user; import com.alibaba.dubbo.config.annotation.Reference; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; @RunWith(SpringRunner.class) @SpringBootTest public class UserServiceTest { @Reference private IUserService userService; @Test public void testHello() { System.out.println(userService.Hello()); } } dream-user消费者UserController/** * @program dream * @description: 用户消费者 * @author: mf * @create: 2019/12/14 20:42 */ package com.dream.user.controller; import com.alibaba.dubbo.config.annotation.Reference; import com.dream.user.IUserService; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class UserController { @Reference private IUserService userService; @GetMapping(\"/test\") public String hello() { return userService.Hello(); } } DreamUserApplication启动文件package com.dream.user; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class DreamUserApplication { public static void main(String[] args) { SpringApplication.run(DreamUserApplication.class, args); } } UserControllerTest/** * @program dream * @description: 用户消费测试 * @author: mf * @create: 2019/12/14 20:56 */ package com.dream.user; import com.alibaba.dubbo.config.annotation.Reference; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; @RunWith(SpringRunner.class) @SpringBootTest public class UserControllerTest { @Reference private IUserService userService; @Test public void test() { System.out.println(userService.Hello()); } } ymlserver: port: 8082 dubbo: application: name: dshop-user-web registry: protocol: zookeeper address: 192.168.1.106:2181 spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/rsd?serverTimeZone=UTC username: root password: 123 启动UserProviderApplication服务 访问`localhost:8082/test","categories":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"}]},{"title":"windows用virtualbox安装centos系统","slug":"windows用virtualbox安装centos系统","date":"2019-12-10T12:52:35.000Z","updated":"2020-10-29T15:45:37.294Z","comments":true,"path":"2019/12/10/windows-yong-virtualbox-an-zhuang-centos-xi-tong/","link":"","permalink":"http://dreamcat.ink/2019/12/10/windows-yong-virtualbox-an-zhuang-centos-xi-tong/","excerpt":"","text":"引言 最近频繁使用虚拟机，并且经常使用centos7，所以记录一下使用virtualbox安装centos7 前提 virtualbox下载地址 centos7阿里云站点 直接推荐这篇博客地址 安装xshell和xftp 学生免费版本地址 下载之后，一路安装即可 不过我这边都有下载地址xshell,xftp 安装ssh yum install -y openssl openssh-server 修改配置文件vim /etc/ssh/sshd_config 将端口22的#号去掉，将ListenAddress的#号去掉即可 启动ssh服务：systemctl start sshd.service 设置开机启动：systemctl enable sshd.service 用xshell连接ssh 新建会话 主机：填IP地址 端口号：22 用户登陆：填centos的root和密码 连接测试即可","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"虚拟机","slug":"虚拟机","permalink":"http://dreamcat.ink/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}]},{"title":"Centos7配置Dubbo和Zookeeper","slug":"Centos7配置Dubbo和Zookeeper","date":"2019-12-10T10:12:08.000Z","updated":"2020-10-29T15:53:58.179Z","comments":true,"path":"2019/12/10/centos7-pei-zhi-dubbo-he-zookeeper/","link":"","permalink":"http://dreamcat.ink/2019/12/10/centos7-pei-zhi-dubbo-he-zookeeper/","excerpt":"","text":"引言 最近看了看分布式架构，尝试了一下dubbo，注册中心是hiZookeeper，所以先在centos下配置环境吧。 前提 系统：Centos7 安装[Java][http://dreamcat.ink/2019/07/08/tool-notes/java3/]环境 下载地址 选择Linux 64位tar.gz 我一般将它放进/usr/local/myapps，myapps是存放自己的软件等。 解压tar -xzvf xxx，xxx指的刚才下载的jdk 添加java的环境变量，打开vim /etc/profile 在蓝色字体下面添加 JAVA_HOME=/usr/local/myapps/jdk1.8.0_221 # 这是我的java存放的地址 PATH=$JAVA_HOME/bin:$PATH export JAVA_HOME PATH 更新source /etc/profile 查看是否添加成功javac即可 安装Zookeeper 下载地址 选择3.4.14版本下载即可，放进和刚才的myapps下。 解压：tar -zxf zookeeper-3.4.14.tar.gz 将配置文件cp -r zoo_sample.cfg zoo.cfg 启动zookeeper./zkServer.sh start 安装DubboDubbo的源码github 我提前编译好的dubbo.war. dubbo-monitor-simple 下载2.5.x版本的zip 前提是系统装了maven环境，可以编译生成war。 解压分别进入dubbo-admin和Dubbo-simple-&gt;Dubbo-monitor-simple，执行mvn package 分别在target文件下找到了对应的一个war包和tar.gz包 解压 tar -zxf dubbo-monitor-simple-2.0.0-assembly.tar.gz 进入配置文件修改vim dubbo.properties 将注册中心#去掉，dubbo.registry.address=zookeeper://127.0.0.1:2181 将multicast注册中心加# 如果是低配置云服务器，记得修改vim start.sh的内存大小，512m，原来2g 然后启动dubbo./start.sh start 记得防火墙设置，可以加端口，也可以直接关掉防火墙 加端口：如果在/etc/sysconfig/iptables，那默认是firewall防火墙，所以卸载 systemctl stop firewalld systemctl mask firewalld yum install -y iptables yum install iptables-services 开启服务systemctl start iptables.service 设置防火墙启动systemctl enable iptables.service 然后执行vim /etc/sysconfig/iptables模仿22端口添加即可 记得添加2181，8080 安装tomcat由于dubbo.war在tomcat容器中运行，所以先下载tomcat，其实tomcat不用安装，下载解压即可。 下载地址 选择左边栏中的tomcat8 选择core中的tar.gz下载 然后放在myapps路径下：/usr/local/myapps下 解压：tar -zxf apache-tomcat-8.5.47 修改端口cd conf vim server.xml将8080改为8088，因为不改的话会和dubbo冲突 将webapps下的ROOT目录下的文件全部删除rm -rf ./*即可 将dubbo.war存放在刚才的ROOT目录下 解压：unzip dubbo.war -d apache-tomcat-8.5.47/webapps/ROOT/ 如果提示没有unzip的话，安装yum install -y unzip zip 去bin目录下启动./startup.sh 记得添加防火墙 访问对应的ip加端口：8088之后，guest账户和密码guest，root账户和密码是root","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"http://dreamcat.ink/tags/Dubbo/"}]},{"title":"Java多线程-线程池","slug":"Java多线程-线程池","date":"2019-11-27T07:50:00.000Z","updated":"2020-10-29T15:52:14.505Z","comments":true,"path":"2019/11/27/java-duo-xian-cheng-xian-cheng-chi/","link":"","permalink":"http://dreamcat.ink/2019/11/27/java-duo-xian-cheng-xian-cheng-chi/","excerpt":"","text":"引言 JavaGuide :一份涵盖大部分Java程序员所需要掌握的核心知识。star:45159，替他宣传一下子 这位大佬，总结的真好！！！我引用这位大佬的文章，因为方便自己学习和打印… 使用线程池的好处 池化技术相比大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。 这里借用《Java 并发编程的艺术》提到的来说一下使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 Executor框架简介Executor 框架是 Java5 之后引进的，在 Java 5 之后，通过 Executor 来启动线程比使用 Thread 的 start 方法更好，除了更易管理，效率更好（用线程池实现，节约开销）外，还有关键的一点：有助于避免 this 逃逸问题。 补充：this 逃逸是指在构造函数返回之前其他线程就持有该对象的引用. 调用尚未构造完全的对象的方法可能引发令人疑惑的错误。 Executor 框架不仅包括了线程池的管理，还提供了线程工厂、队列以及拒绝策略等，Executor 框架让并发编程变得更加简单。 Executor 框架结构(主要由三大部分组成)任务(Runnable /Callable)执行任务需要实现的 Runnable 接口 或 Callable接口。Runnable 接口或 Callable 接口 实现类都可以被 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行。 任务的执行(Executor)任务执行机制的核心接口 Executor ，以及继承自 Executor 接口的 ExecutorService 接口。ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 这两个关键类实现了 ExecutorService 接口。 这里提了很多底层的类关系，但是，实际上我们需要更多关注的是 ThreadPoolExecutor 这个类，这个类在我们实际使用线程池的过程中，使用频率还是非常高的。 ThreadPoolExecutor 类描述: //AbstractExecutorService实现了ExecutorService接口 public class ThreadPoolExecutor extends AbstractExecutorService ScheduledThreadPoolExecutor 类描述: //ScheduledExecutorService实现了ExecutorService接口 public class ScheduledThreadPoolExecutor extends ThreadPoolExecutor implements ScheduledExecutorService 异步计算的结果(Future)Future 接口以及 Future 接口的实现类 FutureTask 类都可以代表异步计算的结果。 当我们把 Runnable接口 或 Callable 接口 的实现类提交给 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行。（调用 submit() 方法时会返回一个 FutureTask 对象） Executor 框架的使用示意图 主线程首先要创建实现 Runnable 或者 Callable 接口的任务对象。 把创建完成的实现 Runnable/Callable接口的 对象直接交给 ExecutorService 执行: ExecutorService.execute（Runnable command））或者也可以把 Runnable 对象或Callable 对象提交给 ExecutorService 执行（ExecutorService.submit（Runnable task）或 ExecutorService.submit（Callable &lt;T&gt; task））。 如果执行 ExecutorService.submit（…），ExecutorService 将返回一个实现Future接口的对象（我们刚刚也提到过了执行 execute()方法和 submit()方法的区别，submit()会返回一个 FutureTask 对象）。由于 FutureTask 实现了 Runnable，我们也可以创建 FutureTask，然后直接交给 ExecutorService 执行。 最后，主线程可以执行 FutureTask.get()方法来等待任务执行完成。主线程也可以执行 FutureTask.cancel（boolean mayInterruptIfRunning）来取消此任务的执行。 (重要)ThreadPoolExecutor 类简单介绍线程池实现类 ThreadPoolExecutor 是 Executor 框架最核心的类。 ThreadPoolExecutor 类分析ThreadPoolExecutor 类中提供的四个构造方法。我们来看最长的那个，其余三个都是在这个构造方法的基础上产生（其他几个构造方法说白点都是给定某些默认参数的构造方法比如默认制定拒绝策略是什么），这里就不贴代码讲了，比较简单。 /** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue&lt;Runnable> workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务 ) { if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 下面这些对创建 非常重要，在后面使用线程池的过程中你一定会用到！所以，务必拿着小本本记清楚。 ThreadPoolExecutor 3 个最重要的参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，信任就会被存放在队列中。 ThreadPoolExecutor其他常见参数: keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下。 下面这张图可以加深你对线程池中各个参数的相互关系的理解（图片来源：《Java性能调优实战》）： ThreadPoolExecutor 饱和策略定义: 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务。您不会任务请求。但是这种策略会降低对于新任务提交速度，影响程序的整体性能。另外，这个策略喜欢增加队列容量。如果您的应用程序可以承受此延迟并且你不能任务丢弃任何一个任务请求的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。 Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。（这个直接查看 ThreadPoolExecutor 的构造函数源码就可以看出，比较简单的原因，这里就不贴代码了。） 推荐使用 ThreadPoolExecutor 构造函数创建线程池在《阿里巴巴 Java 开发手册》“并发处理”这一章节，明确指出线程资源必须通过线程池提供，不允许在应用中自行显示创建线程。 为什么呢？ 使用线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源开销，解决资源不足的问题。如果不使用线程池，有可能会造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。 另外《阿里巴巴 Java 开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 构造函数的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险 Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。 方式一：通过ThreadPoolExecutor构造函数实现（推荐） 方式二：通过 Executor 框架的工具类 Executors 来实现 我们可以创建三种类型的 ThreadPoolExecutor： FixedThreadPool SingleThreadExecutor CachedThreadPool (重要)ThreadPoolExecutor 使用示例我们上面讲解了 Executor框架以及 ThreadPoolExecutor 类，下面让我们实战一下，来通过写一个 ThreadPoolExecutor 的小 Demo 来回顾上面的内容。 示例代码:Runnable+ThreadPoolExecutor首先创建一个 Runnable 接口的实现类（当然也可以是 Callable 接口，我们上面也说了两者的区别。） MyRunnable.java import java.util.Date; /** * 这是一个简单的Runnable类，需要大约5秒钟来执行其任务。 * @author shuang.kou */ public class MyRunnable implements Runnable { private String command; public MyRunnable(String s) { this.command = s; } @Override public void run() { System.out.println(Thread.currentThread().getName() + \" Start. Time = \" + new Date()); processCommand(); System.out.println(Thread.currentThread().getName() + \" End. Time = \" + new Date()); } private void processCommand() { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } } @Override public String toString() { return this.command; } } 编写测试程序，我们这里以阿里巴巴推荐的使用 ThreadPoolExecutor 构造函数自定义参数的方式来创建线程池。 ThreadPoolExecutorDemo.java import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;>(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i &lt; 10; i++) { //创建WorkerThread对象（WorkerThread类实现了Runnable 接口） Runnable worker = new MyRunnable(\"\" + i); //执行Runnable executor.execute(worker); } //终止线程池 executor.shutdown(); while (!executor.isTerminated()) { } System.out.println(\"Finished all threads\"); } } 可以看到我们上面的代码指定了： corePoolSize: 核心线程数为 5。 maximumPoolSize ：最大线程数 10 keepAliveTime : 等待时间为 1L。 unit: 等待时间的单位为 TimeUnit.SECONDS。 workQueue：任务队列为 ArrayBlockingQueue，并且容量为 100; handler:饱和策略为 CallerRunsPolicy。 pool-1-thread-2 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-5 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-4 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-1 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-3 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-5 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-3 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-2 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-4 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-1 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-2 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-1 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-4 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-3 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-5 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-2 End. Time = Tue Nov 12 20:59:54 CST 2019 pool-1-thread-3 End. Time = Tue Nov 12 20:59:54 CST 2019 pool-1-thread-4 End. Time = Tue Nov 12 20:59:54 CST 2019 pool-1-thread-5 End. Time = Tue Nov 12 20:59:54 CST 2019 pool-1-thread-1 End. Time = Tue Nov 12 20:59:54 CST 2019线程池原理分析我们通过代码输出结果可以看出：线程池每次会同时执行 5 个任务，这 5 个任务执行完之后，剩余的 5 个任务才会被执行。 大家可以先通过上面讲解的内容，分析一下到底是咋回事？（自己独立思考一会） 现在，我们就分析上面的输出内容来简单分析一下线程池原理。 为了搞懂线程池的原理，我们需要首先分析一下 execute方法。在 5.1 节中的 Demo 中我们使用 executor.execute(worker)来提交一个任务到线程池中去，这个方法非常重要，下面我们来看看它的源码： // 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount) private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) { return c &amp; CAPACITY; } private final BlockingQueue&lt;Runnable> workQueue; public void execute(Runnable command) { // 如果任务为null，则抛出异常。 if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息 int c = ctl.get(); // 下面会涉及到 3 步 操作 // 1.首先判断当前线程池中之行的任务数量是否小于 corePoolSize // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 2.如果当前之行的任务数量大于等于 corePoolSize 的时候就会走到这里 // 通过 isRunning 方法判断线程池状态，线程池处于 RUNNING 状态才会被并且队列可以加入任务，该任务才会被加入进去 if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); // 再次获取线程池状态，如果线程池状态不是 RUNNING 状态就需要从任务队列中移除任务，并尝试判断线程是否全部执行完毕。同时执行拒绝策略。 if (!isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 如果当前线程池为空就新创建一个线程并执行。 else if (workerCountOf(recheck) == 0) addWorker(null, false); } //3. 通过addWorker(command, false)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 //如果addWorker(command, false)执行失败，则通过reject()执行相应的拒绝策略的内容。 else if (!addWorker(command, false)) reject(command); } 通过下图可以更好的对上面这 3 步做一个展示，下图是我为了省事直接从网上找到，原地址不明。 没搞懂的话，也没关系，可以看看我的分析： 我们在代码中模拟了 10 个任务，我们配置的核心线程数为 5 、等待队列容量为 100 ，所以每次只可能存在 5 个任务同时执行，剩下的 5 个任务会被放到等待队列中去。当前的 5 个任务之行完成后，才会之行剩下的 5 个任务。 几个常见的对比Runnable vs CallableRunnable自 Java 1.0 以来一直存在，但Callable仅在 Java 1.5 中引入,目的就是为了来处理Runnable不支持的用例。Runnable 接口不会返回结果或抛出检查异常，但是Callable 接口可以。所以，如果任务不需要返回结果或抛出异常推荐使用 Runnable 接口，这样代码看起来会更加简洁。 工具类 Executors 可以实现 Runnable 对象和 Callable 对象之间的相互转换。（Executors.callable（Runnable task）或 Executors.callable（Runnable task，Object resule））。 Runnable.java @FunctionalInterface public interface Runnable { /** * 被线程执行，没有返回值也无法抛出异常 */ public abstract void run(); } Callable.java @FunctionalInterface public interface Callable&lt;V> { /** * 计算结果，或在无法这样做时抛出异常。 * @return 计算得出的结果 * @throws 如果无法计算结果，则抛出异常 */ V call() throws Exception; } execute() vs submit() execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 我们以AbstractExecutorService接口中的一个 submit 方法为例子来看看源代码： public Future&lt;?> submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture&lt;Void> ftask = newTaskFor(task, null); execute(ftask); return ftask; } 上面方法调用的 newTaskFor 方法返回了一个 FutureTask 对象。 protected &lt;T> RunnableFuture&lt;T> newTaskFor(Runnable runnable, T value) { return new FutureTask&lt;T>(runnable, value); } 我们再来看看execute()方法： public void execute(Runnable command) { ... } shutdown()VSshutdownNow() shutdown（） :关闭线程池，线程池的状态变为 SHUTDOWN。线程池不再接受新任务了，但是队列里的任务得执行完毕。 shutdownNow（） :关闭线程池，线程的状态变为 STOP。线程池会终止当前正在运行的任务，并停止处理排队的任务并返回正在等待执行的 List。 isTerminated() VS isShutdown() isShutDown 当调用 shutdown() 方法后返回为 true。 isTerminated 当调用 shutdown() 方法后，并且所有提交的任务完成后返回为 true Callable+ThreadPoolExecutor示例代码MyCallable.java import java.util.concurrent.Callable; public class MyCallable implements Callable&lt;String> { @Override public String call() throws Exception { Thread.sleep(1000); //返回执行当前 Callable 的线程名字 return Thread.currentThread().getName(); } } CallableDemo.java import java.util.ArrayList; import java.util.Date; import java.util.List; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.Callable; import java.util.concurrent.ExecutionException; import java.util.concurrent.Future; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class CallableDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;>(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); List&lt;Future&lt;String>> futureList = new ArrayList&lt;>(); Callable&lt;String> callable = new MyCallable(); for (int i = 0; i &lt; 10; i++) { //提交任务到线程池 Future&lt;String> future = executor.submit(callable); //将返回值 future 添加到 list，我们可以通过 future 获得 执行 Callable 得到的返回值 futureList.add(future); } for (Future&lt;String> fut : futureList) { try { System.out.println(new Date() + \"::\" + fut.get()); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); } } //关闭线程池 executor.shutdown(); } } Wed Nov 13 13:40:41 CST 2019::pool-1-thread-1 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-2 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-3 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-4 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-5 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-3 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-2 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-1 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-4 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-5几种常见的线程池详解FixedThreadPool介绍FixedThreadPool 被称为可重用固定线程数的线程池。通过 Executors 类中的相关源代码来看一下相关实现： /** * 创建一个可重用固定数量线程的线程池 */ public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable>(), threadFactory); } 从上面源代码可以看出新创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 都被设置为 nThreads，这个 nThreads 参数是我们使用的时候自己传递的。 执行任务过程介绍FixedThreadPool 的 execute() 方法运行示意图（该图片来源：《Java 并发编程的艺术》）： 上图说明： 如果当前运行的线程数小于 corePoolSize， 如果再来新任务的话，就创建新的线程来执行任务； 当前运行的线程数等于 corePoolSize 后， 如果再来新任务的话，会将任务加入 LinkedBlockingQueue； 线程池中的线程执行完 手头的任务后，会在循环中反复从 LinkedBlockingQueue 中获取任务来执行； 为什么不推荐使用FixedThreadPool？FixedThreadPool 使用无界队列 LinkedBlockingQueue（队列的容量为 Intger.MAX_VALUE）作为线程池的工作队列会对线程池带来如下影响 ： 当线程池中的线程数达到 corePoolSize 后，新任务将在无界队列中等待，因此线程池中的线程数不会超过 corePoolSize； 由于使用无界队列时 maximumPoolSize 将是一个无效参数，因为不可能存在任务队列满的情况。所以，通过创建 FixedThreadPool的源码可以看出创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 被设置为同一个值。 由于 1 和 2，使用无界队列时 keepAliveTime 将是一个无效参数； 运行中的 FixedThreadPool（未执行 shutdown()或 shutdownNow()）不会拒绝任务，在任务比较多的时候会导致 OOM（内存溢出）。 SingleThreadExecutor 详解介绍SingleThreadExecutor 是只有一个线程的线程池。下面看看SingleThreadExecutor 的实现： /** *返回只有一个线程的线程池 */ public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable>(), threadFactory)); } public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable>())); } 从上面源代码可以看出新创建的 SingleThreadExecutor 的 corePoolSize 和 maximumPoolSize 都被设置为 1.其他参数和 FixedThreadPool 相同。 执行任务过程介绍SingleThreadExecutor 的运行示意图（该图片来源：《Java 并发编程的艺术》）： 上图说明; 如果当前运行的线程数少于 corePoolSize，则创建一个新的线程执行任务； 当前线程池中有一个运行的线程后，将任务加入 LinkedBlockingQueue 线程执行完当前的任务后，会在循环中反复从LinkedBlockingQueue 中获取任务来执行； CachedThreadPool详解介绍CachedThreadPool 是一个会根据需要创建新线程的线程池。下面通过源码来看看 CachedThreadPool 的实现： /** * 创建一个线程池，根据需要创建新线程，但会在先前构建的线程可用时重用它。 */ public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable>(), threadFactory); } public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable>()); } CachedThreadPool 的corePoolSize 被设置为空（0），maximumPoolSize被设置为 Integer.MAX.VALUE，即它是无界的，这也就意味着如果主线程提交任务的速度高于 maximumPool 中线程处理任务的速度时，CachedThreadPool 会不断创建新的线程。极端情况下，这样会导致耗尽 cpu 和内存资源。 执行任务过程介绍CachedThreadPool 的 execute()方法的执行示意图（该图片来源：《Java 并发编程的艺术》）： 上图说明： 首先执行 SynchronousQueue.offer(Runnable task) 提交任务到任务队列。如果当前 maximumPool 中有闲线程正在执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)，那么主线程执行 offer 操作与空闲线程执行的 poll 操作配对成功，主线程把任务交给空闲线程执行，execute()方法执行完成，否则执行下面的步骤 2； 当初始 maximumPool 为空，或者 maximumPool 中没有空闲线程时，将没有线程执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)。这种情况下，步骤 1 将失败，此时 CachedThreadPool 会创建新线程执行任务，execute 方法执行完成； 为什么不推荐使用CachedThreadPool？CachedThreadPool允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。 ScheduledThreadPoolExecutor 详解ScheduledThreadPoolExecutor 主要用来在给定的延迟后运行任务，或者定期执行任务。 这个在实际项目中基本不会被用到，所以对这部分大家只需要简单了解一下它的思想。关于如何在Spring Boot 中 实现定时任务 简介ScheduledThreadPoolExecutor 使用的任务队列 DelayQueue 封装了一个 PriorityQueue，PriorityQueue 会对队列中的任务进行排序，执行所需时间短的放在前面先被执行(ScheduledFutureTask 的 time 变量小的先执行)，如果执行所需时间相同则先提交的任务将被先执行(ScheduledFutureTask 的 squenceNumber 变量小的先执行)。 ScheduledThreadPoolExecutor 和 Timer 的比较： Timer 对系统时钟的变化敏感，ScheduledThreadPoolExecutor不是； Timer 只有一个执行线程，因此长时间运行的任务可以延迟其他任务。 ScheduledThreadPoolExecutor 可以配置任意数量的线程。 此外，如果你想（通过提供 ThreadFactory），你可以完全控制创建的线程; 在TimerTask 中抛出的运行时异常会杀死一个线程，从而导致 Timer 死机:-( …即计划任务将不再运行。ScheduledThreadExecutor 不仅捕获运行时异常，还允许您在需要时处理它们（通过重写 afterExecute 方法ThreadPoolExecutor）。抛出异常的任务将被取消，但其他任务将继续运行。 综上，在 JDK1.5 之后，你没有理由再使用 Timer 进行任务调度了。 备注： Quartz 是一个由 java 编写的任务调度库，由 OpenSymphony 组织开源出来。在实际项目开发中使用 Quartz 的还是居多，比较推荐使用 Quartz。因为 Quartz 理论上能够同时对上万个任务进行调度，拥有丰富的功能特性，包括任务调度、任务持久化、可集群化、插件等等。 运行机制 ScheduledThreadPoolExecutor 的执行主要分为两大部分： 当调用 ScheduledThreadPoolExecutor 的 scheduleAtFixedRate() 方法或者scheduleWirhFixedDelay() 方法时，会向 ScheduledThreadPoolExecutor 的 DelayQueue 添加一个实现了 RunnableScheduledFuture 接口的 ScheduledFutureTask 。 线程池中的线程从 DelayQueue 中获取 ScheduledFutureTask，然后执行任务。 ScheduledThreadPoolExecutor 为了实现周期性的执行任务，对 ThreadPoolExecutor做了如下修改： 使用 DelayQueue 作为任务队列； 获取任务的方不同 执行周期任务后，增加了额外的处理 ScheduledThreadPoolExecutor 执行周期任务的步骤 线程 1 从 DelayQueue 中获取已到期的 ScheduledFutureTask（DelayQueue.take()）。到期任务是指 ScheduledFutureTask的 time 大于等于当前系统的时间； 线程 1 执行这个 ScheduledFutureTask； 线程 1 修改 ScheduledFutureTask 的 time 变量为下次将要被执行的时间； 线程 1 把这个修改 time 之后的 ScheduledFutureTask 放回 DelayQueue 中（DelayQueue.add())。 线程池大小确定线程池数量的确定一直是困扰着程序员的一个难题，大部分程序员在设定线程池大小的时候就是随心而定。我们并没有考虑过这样大小的配置是否会带来什么问题，我自己就是这大部分程序员中的一个代表。 由于笔主对如何确定线程池大小也没有什么实际经验，所以，这部分内容参考了网上很多文章/书籍。 首先，可以肯定的一点是线程池大小设置过大或者过小都会有问题。合适的才是最好，貌似在 95 % 的场景下都是合适的。 如果阅读过我的上一篇关于线程池的文章的话，你一定知道： 如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的！ CPU 根本没有得到充分利用。 但是，如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率。 上下文切换： 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 有一个简单并且适用面比较广的公式： CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://dreamcat.ink/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Java多线程-AQS","slug":"Java多线程-AQS","date":"2019-11-27T06:49:50.000Z","updated":"2020-10-29T15:52:09.259Z","comments":true,"path":"2019/11/27/java-duo-xian-cheng-aqs/","link":"","permalink":"http://dreamcat.ink/2019/11/27/java-duo-xian-cheng-aqs/","excerpt":"","text":"引言 JavaGuide :一份涵盖大部分Java程序员所需要掌握的核心知识。star:45159，替他宣传一下子 这位大佬，总结的真好！！！我引用这位大佬的文章，因为方便自己学习和打印… AQS 简单介绍AQS的全称为（AbstractQueuedSynchronizer），这个类在java.util.concurrent.locks包下面。 AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。 AQS原理概述AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node）来实现锁的分配。 AQS使用一个int成员变量来表示同步状态，通过内置的FIFO队列来完成获取资源线程的排队工作。AQS使用CAS对该同步状态进行原子操作实现对其值的修改。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过protected类型的getState，setState，compareAndSetState进行操作 //返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } AQS对资源的共享方式AQS定义两种资源共享方式 Exclusive（独占）：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的 Share（共享）：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 ReentrantReadWriteLock 可以看成是组合式，因为ReentrantReadWriteLock也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在上层已经帮我们实现好了。 AQS底层使用了模板方法模式这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用，下面简单的给大家介绍一下模板方法模式，模板方法模式是一个很容易理解的设计模式之一。 模板方法模式是基于”继承“的，主要是为了在不改变模板结构的前提下在子类中重新定义模板中的内容以实现复用代码。举个很简单的例子假如我们要去一个地方的步骤是：购票buyTicket()-&gt;安检securityCheck()-&gt;乘坐某某工具回家ride()-&gt;到达目的地arrive()。我们可能乘坐不同的交通工具回家比如飞机或者火车，所以除了ride()方法，其他方法的实现几乎相同。我们可以定义一个包含了这些方法的抽象类，然后用户根据自己的需要继承该抽象类然后修改 ride()方法。 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS类中的其他方法都是final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS(Compare and Swap)减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 Semaphore(信号量)-允许多个线程同时访问synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 示例代码如下： public class SemaphoreExample1 { // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); // 一次只能允许执行的线程数量。 final Semaphore semaphore = new Semaphore(20); for (int i = 0; i &lt; threadCount; i++) { final int threadnum = i; threadPool.execute(() -> {// Lambda 表达式的运用 try { semaphore.acquire();// 获取一个许可，所以可运行线程数量为20/1=20 test(threadnum); semaphore.release();// 释放一个许可 } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } }); } threadPool.shutdown(); System.out.println(\"finish\"); } public static void test(int threadnum) throws InterruptedException { Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println(\"threadnum:\" + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 } } 执行 acquire 方法阻塞，直到有一个许可证可以获得然后拿走一个许可证；每个 release 方法增加一个许可证，这可能会释放一个阻塞的acquire方法。然而，其实并没有实际的许可证这个对象，Semaphore只是维持了一个可获得许可证的数量。 Semaphore经常用于限制获取某种资源的线程数量。 除了 acquire方法之外，另一个比较常用的与之对应的方法是tryAcquire方法，该方法如果获取不到许可就立即返回false。 Semaphore 有两种模式，公平模式和非公平模式。 公平模式： 调用acquire的顺序就是获取许可证的顺序，遵循FIFO； 非公平模式： 抢占式的。 Semaphore 对应的两个构造方法如下： public Semaphore(int permits) { sync = new NonfairSync(permits); } public Semaphore(int permits, boolean fair) { sync = fair ? new FairSync(permits) : new NonfairSync(permits); } 这两个构造方法，都必须提供许可的数量，第二个构造方法可以指定是公平模式还是非公平模式，默认非公平模式。 CountDownLatch （倒计时器）CountDownLatch是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程的操作执行完后再执行。在Java并发中，countdownlatch的概念是一个常见的面试题，所以一定要确保你很好的理解了它。 CountDownLatch 的三种典型用法①某一线程在开始运行前等待n个线程执行完毕。将 CountDownLatch 的计数器初始化为n ：new CountDownLatch(n)，每当一个任务线程执行完毕，就将计数器减1 countdownlatch.countDown()，当计数器的值变为0时，在CountDownLatch上 await() 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。 ②实现多个线程开始执行任务的最大并行性。注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 ：new CountDownLatch(1)，多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为0，多个线程同时被唤醒。 ③死锁检测：一个非常方便的使用场景是，你可以使用n个线程访问共享资源，在每次测试阶段的线程数目是不同的，并尝试产生死锁。 public class CountDownLatchExample1 { // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); final CountDownLatch countDownLatch = new CountDownLatch(threadCount); for (int i = 0; i &lt; threadCount; i++) { final int threadnum = i; threadPool.execute(() -> {// Lambda 表达式的运用 try { test(threadnum); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } finally { countDownLatch.countDown();// 表示一个请求已经被完成 } }); } countDownLatch.await(); threadPool.shutdown(); System.out.println(\"finish\"); } public static void test(int threadnum) throws InterruptedException { Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println(\"threadnum:\" + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 } } 上面的代码中，我们定义了请求的数量为550，当这550个请求被处理完成之后，才会执行System.out.println(&quot;finish&quot;);。 与CountDownLatch的第一次交互是主线程等待其他线程。主线程必须在启动其他线程后立即调用CountDownLatch.await()方法。这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。 其他N个线程必须引用闭锁对象，因为他们需要通知CountDownLatch对象，他们已经完成了各自的任务。这种通知机制是通过 CountDownLatch.countDown()方法来完成的；每调用一次这个方法，在构造函数中初始化的count值就减1。所以当N个线程都调 用了这个方法，count的值等于0，然后主线程就能通过await()方法，恢复执行自己的任务。 CountDownLatch 的不足CountDownLatch是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当CountDownLatch使用完毕后，它不能再次被使用。 CountDownLatch相常见面试题：解释一下CountDownLatch概念？ CountDownLatch 和CyclicBarrier的不同之处？ 给出一些CountDownLatch使用的例子？ CountDownLatch 类中主要的方法？ CyclicBarrier(循环栅栏)CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。 CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 CyclicBarrier 的应用场景CyclicBarrier 可以用于多线程计算数据，最后合并计算结果的应用场景。比如我们用一个Excel保存了用户所有银行流水，每个Sheet保存一个帐户近一年的每笔银行流水，现在需要统计用户的日均银行流水，先用多线程处理每个sheet里的银行流水，都执行完之后，得到每个sheet的日均银行流水，最后，再用barrierAction用这些线程的计算结果，计算出整个Excel的日均银行流水。 public class CyclicBarrierExample2 { // 请求的数量 private static final int threadCount = 550; // 需要同步的线程数量 private static final CyclicBarrier cyclicBarrier = new CyclicBarrier(5); public static void main(String[] args) throws InterruptedException { // 创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for (int i = 0; i &lt; threadCount; i++) { final int threadNum = i; Thread.sleep(1000); threadPool.execute(() -> { try { test(threadNum); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (BrokenBarrierException e) { // TODO Auto-generated catch block e.printStackTrace(); } }); } threadPool.shutdown(); } public static void test(int threadnum) throws InterruptedException, BrokenBarrierException { System.out.println(\"threadnum:\" + threadnum + \"is ready\"); try { /**等待60秒，保证子线程完全执行结束*/ cyclicBarrier.await(60, TimeUnit.SECONDS); } catch (Exception e) { System.out.println(\"-----CyclicBarrierException------\"); } System.out.println(\"threadnum:\" + threadnum + \"is finish\"); } } threadnum:0is ready threadnum:1is ready threadnum:2is ready threadnum:3is ready threadnum:4is ready threadnum:4is finish threadnum:0is finish threadnum:1is finish threadnum:2is finish threadnum:3is finish threadnum:5is ready threadnum:6is ready threadnum:7is ready threadnum:8is ready threadnum:9is ready threadnum:9is finish threadnum:5is finish threadnum:8is finish threadnum:7is finish threadnum:6is finish ......可以看到当线程数量也就是请求数量达到我们定义的 5 个的时候， await方法之后的方法才被执行。 另外，CyclicBarrier还提供一个更高级的构造函数CyclicBarrier(int parties, Runnable barrierAction)，用于在线程到达屏障时，优先执行barrierAction，方便处理更复杂的业务场景。示例代码如下： public class CyclicBarrierExample3 { // 请求的数量 private static final int threadCount = 550; // 需要同步的线程数量 private static final CyclicBarrier cyclicBarrier = new CyclicBarrier(5, () -> { System.out.println(\"------当线程数达到之后，优先执行------\"); }); public static void main(String[] args) throws InterruptedException { // 创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for (int i = 0; i &lt; threadCount; i++) { final int threadNum = i; Thread.sleep(1000); threadPool.execute(() -> { try { test(threadNum); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (BrokenBarrierException e) { // TODO Auto-generated catch block e.printStackTrace(); } }); } threadPool.shutdown(); } public static void test(int threadnum) throws InterruptedException, BrokenBarrierException { System.out.println(\"threadnum:\" + threadnum + \"is ready\"); cyclicBarrier.await(); System.out.println(\"threadnum:\" + threadnum + \"is finish\"); } } threadnum:0is ready threadnum:1is ready threadnum:2is ready threadnum:3is ready threadnum:4is ready ------当线程数达到之后，优先执行------ threadnum:4is finish threadnum:0is finish threadnum:2is finish threadnum:1is finish threadnum:3is finish threadnum:5is ready threadnum:6is ready threadnum:7is ready threadnum:8is ready threadnum:9is ready ------当线程数达到之后，优先执行------ threadnum:9is finish threadnum:5is finish threadnum:6is finish threadnum:8is finish threadnum:7is finish ......CyclicBarrier和CountDownLatch的区别CountDownLatch是计数器，只能使用一次，而CyclicBarrier的计数器提供reset功能，可以多次使用。 对于CountDownLatch来说，重点是“一个线程（多个线程）等待”，而其他的N个线程在完成“某件事情”之后，可以终止，也可以等待。而对于CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。 CountDownLatch是计数器，线程完成一个记录一个，只不过计数不是递增而是递减，而CyclicBarrier更像是一个阀门，需要所有线程都到达，阀门才能打开，然后继续执行。","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://dreamcat.ink/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Java多线程-Atomic原子类","slug":"Java多线程-Atomic原子类","date":"2019-11-27T02:34:41.000Z","updated":"2020-10-29T15:52:02.105Z","comments":true,"path":"2019/11/27/java-duo-xian-cheng-atomic-yuan-zi-lei/","link":"","permalink":"http://dreamcat.ink/2019/11/27/java-duo-xian-cheng-atomic-yuan-zi-lei/","excerpt":"","text":"引言 JavaGuide :一份涵盖大部分Java程序员所需要掌握的核心知识。star:45159，替他宣传一下子 这位大佬，总结的真好！！！我引用这位大佬的文章，因为方便自己学习和打印… Atomic 原子类介绍Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。 所以，所谓原子类说简单点就是具有原子/原子操作特征的类。 JUC包中的原子类分为4类 基本类型 使用原子的方式更新基本类型 AtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类 数组类型 使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整型数组原子类 AtomicLongArray：长整型数组原子类 AtomicReferenceArray ：引用类型数组原子类 引用类型 AtomicReference：引用类型原子类 AtomicReferenceFieldUpdater：原子更新引用类型里的字段 AtomicMarkableReference ：原子更新带有标记位的引用类型 对象的属性修改类型 AtomicIntegerFieldUpdater:原子更新整型字段的更新器 AtomicLongFieldUpdater：原子更新长整型字段的更新器 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicMarkableReference：原子更新带有标记的引用类型。该类将 boolean 标记与引用关联起来，也可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 CAS ABA 问题 描述: 第一个线程取到了变量 x 的值 A，然后巴拉巴拉干别的事，总之就是只拿到了变量 x 的值 A。这段时间内第二个线程也取到了变量 x 的值 A，然后把变量 x 的值改为 B，然后巴拉巴拉干别的事，最后又把变量 x 的值变为 A （相当于还原了）。在这之后第一个线程终于进行了变量 x 的操作，但是此时变量 x 的值还是 A，所以 compareAndSet 操作是成功。 例子描述(可能不太合适，但好理解): 年初，现金为零，然后通过正常劳动赚了三百万，之后正常消费了（比如买房子）三百万。年末，虽然现金零收入（可能变成其他形式了），但是赚了钱是事实，还是得交税的！ 代码例子（以AtomicInteger为例） import java.util.concurrent.atomic.AtomicInteger; public class AtomicIntegerDefectDemo { public static void main(String[] args) { defectOfABA(); } static void defectOfABA() { final AtomicInteger atomicInteger = new AtomicInteger(1); Thread coreThread = new Thread( () -> { final int currentValue = atomicInteger.get(); System.out.println(Thread.currentThread().getName() + \" ------ currentValue=\" + currentValue); // 这段目的：模拟处理其他业务花费的时间 try { Thread.sleep(300); } catch (InterruptedException e) { e.printStackTrace(); } boolean casResult = atomicInteger.compareAndSet(1, 2); System.out.println(Thread.currentThread().getName() + \" ------ currentValue=\" + currentValue + \", finalValue=\" + atomicInteger.get() + \", compareAndSet Result=\" + casResult); } ); coreThread.start(); // 这段目的：为了让 coreThread 线程先跑起来 try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } Thread amateurThread = new Thread( () -> { int currentValue = atomicInteger.get(); boolean casResult = atomicInteger.compareAndSet(1, 2); System.out.println(Thread.currentThread().getName() + \" ------ currentValue=\" + currentValue + \", finalValue=\" + atomicInteger.get() + \", compareAndSet Result=\" + casResult); currentValue = atomicInteger.get(); casResult = atomicInteger.compareAndSet(2, 1); System.out.println(Thread.currentThread().getName() + \" ------ currentValue=\" + currentValue + \", finalValue=\" + atomicInteger.get() + \", compareAndSet Result=\" + casResult); } ); amateurThread.start(); } } 输出内容如下： Thread-0 ------ currentValue=1 Thread-1 ------ currentValue=1, finalValue=2, compareAndSet Result=true Thread-1 ------ currentValue=2, finalValue=1, compareAndSet Result=true Thread-0 ------ currentValue=1, finalValue=2, compareAndSet Result=true下面我们来详细介绍一下这些原子类。 基本类型原子类基本类型原子类介绍使用原子的方式更新基本类型 AtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类 上面三个类提供的方法几乎相同，所以我们这里以 AtomicInteger 为例子来介绍。 AtomicInteger 类常用方法 public final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 基本数据类型原子类的优势①多线程环境不使用原子类保证线程安全（基本数据类型） class Test { private volatile int count = 0; //若要线程安全执行执行count++，需要加锁 public synchronized void increment() { count++; } public int getCount() { return count; } } ②多线程环境使用原子类保证线程安全（基本数据类型） class Test2 { private AtomicInteger count = new AtomicInteger(); public void increment() { count.incrementAndGet(); } //使用AtomicInteger之后，不需要加锁，也可以实现线程安全。 public int getCount() { return count.get(); } } AtomicInteger 线程安全原理简单分析AtomicInteger 类的部分源码： // setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用） private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\"value\")); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 其他数组类型原子类、引用类型原子类和对象的属性修改类型原子类就不介绍了","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://dreamcat.ink/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"HashMap & ConcurrentHashMap面试必问","slug":"HashMap-ConcurrentHashMap面试必问","date":"2019-11-17T04:21:38.000Z","updated":"2020-10-29T15:24:19.348Z","comments":true,"path":"2019/11/17/hashmap-concurrenthashmap-mian-shi-bi-wen/","link":"","permalink":"http://dreamcat.ink/2019/11/17/hashmap-concurrenthashmap-mian-shi-bi-wen/","excerpt":"","text":"HashMap众所周知，HashMap的底层结构是数组和链表组成的，不过在jdk1.7和jdk1.8中具体实现略有不同。 jdk1.7先看图 再看看1.7的实现 介绍成员变量： 初始化桶大小，因为底层是数组，所以这是数组默认的大小。 桶最大值。 默认的负载因子（0.75） table真正存放数据的数组。 map存放数量的大小 桶大小，可在构造函数时显式指定。 负载因子，可在构造函数时显式指定。 负载因子源代码 public HashMap() { this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR); // 桶和负载因子 } public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity > MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; init(); } 给定的默认容量为16，负载因子为0.75. Map在使用过程中不断的往里面存放数据，当数量达到了16 * 0.75 = 12就需要将当前16的容量进行扩容，而扩容这个过程涉及到rehash（重新哈希）、复制数据等操作，所有非常消耗性能。 因此通常建议能提前预估HashMap的大小最好，尽量的减少扩容带来的额外性能损耗。 Entrytransient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE; 如何定义呢？ Entry是Hashmap中的一个内部类，从他的成员变量很容易看出： key就是写入时的键 value自然就是值 开始的时候就提到HashMap是由数组和链表组成，所以这个next就是用于实现链表结构 hash存放的是当前key的hashcode putpublic V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); // 判断数组是否需要初始化 } if (key == null) return putForNullKey(value); // 判断key是否为空 int hash = hash(key); // 计算hashcode int i = indexFor(hash, table.length); // 计算桶 for (Entry&lt;K,V> e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { // 遍历判断链表中的key和hashcode是否相等，等就替换 V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); // 没有就添加新的呗 桶其实就是数组，在数组内添加元素 return null; } 判断当前数组是否需要初始化 如果key为空，则put一个空值进去 根据key计算hashcode 根据计算的hashcode定位index的桶 如果桶是一个链表，则需要遍历判断里面的hashcode、key是否和传入的key相等，如果相等则进行覆盖，并返回原来的值 如果桶是空的，说明当前位置没有数据存入，此时新增一个Entry对象写入当前位置。 void addEntry(int hash, K key, V value, int bucketIndex) { if ((size >= threshold) &amp;&amp; (null != table[bucketIndex])) {// 是否扩容 resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex); } void createEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V> e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;>(hash, key, value, e); size++; } 当调用addEntry写入Entry时需要判断是否需要扩容 如果需要就进行两倍扩充，并将当前的key重新hash并定位。 而在createEntry中会将当前位置的桶传入到新建的桶中，如果当前桶油值就会在位置形成链表。 getpublic V get(Object key) { if (key == null) // 判断key是否为空 return getForNullKey(); // 为空，就返回空值 Entry&lt;K,V> entry = getEntry(key); // get entry return null == entry ? null : entry.getValue(); } final Entry&lt;K,V> getEntry(Object key) { if (size == 0) { return null; } int hash = (key == null) ? 0 : hash(key); //根据key和hashcode for (Entry&lt;K,V> e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } return null; } 首先根据key计算hashcode，然后定位具体的桶 判断该位置是否为链表 不是链接就根据key和hashcode是否相等来返回值 为链表则需要遍历直到key和hashcode相等就返回值 啥都没得，就返回null jdk1.8不知道 1.7 的实现大家看出需要优化的点没有？ 其实一个很明显的地方就是链表 当 Hash 冲突严重时，在桶上形成的链表会变的越来越长，这样在查询时的效率就会越来越低；时间复杂度为 O(N)。 看看成员变量 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /** * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; static final int TREEIFY_THRESHOLD = 8; transient Node&lt;K,V>[] table; /** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */ transient Set&lt;Map.Entry&lt;K,V>> entrySet; /** * The number of key-value mappings contained in this map. */ transient int size; TREEIFY_THRESHOLD 用于判断是否需要将链表转换为红黑树的阈值。 HashEntry 修改为 Node。 Node 的核心组成其实也是和 1.7 中的 HashEntry 一样，存放的都是 key value hashcode next 等数据。 put 判断当前桶是否为空，空的就需要初始化（resize中会判断是否进行初始化） 根据当前key的hashcode定位到具体的桶中并判断是否为空，为空则表明没有Hash冲突，就直接在当前位置创建一个新桶 如果当前桶油值（Hash冲突），那么就要比较当前桶中的key、key的hashcode与写入的key是否相等，相等就赋值给e，在第8步的时候会统一进行赋值及返回 如果当前桶为红黑树，那就要按照红黑树的方式写入数据 如果是个链表，就需要将当前的key、value封装称一个新节点写入到当前桶的后面形成链表。 接着判断当前链表的大小是否大于预设的阈值，大于就要转换成为红黑树 如果在遍历过程中找到key相同时直接退出遍历。 如果e != null就相当于存在相同的key，那就需要将值覆盖。 最后判断是否需要进行扩容。 getpublic V get(Object key) { Node&lt;K,V> e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node&lt;K,V> getNode(int hash, Object key) { Node&lt;K,V>[] tab; Node&lt;K,V> first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) > 0 &amp;&amp; // 如果桶为空，则直接返回null (first = tab[(n - 1) &amp; hash]) != null) { if (first.hash == hash &amp;&amp; // always check first node // 否则判断桶的第一个位置（有可能是链表、红黑树）的key是否为查询的key，是就直接返回value ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { // 如果第一个不匹配，则判断它的下一个是红黑树还是链表 if (first instanceof TreeNode) // 红黑树就按照树的查找方式返回值 return ((TreeNode&lt;K,V>)first).getTreeNode(hash, key); do { if (e.hash == hash &amp;&amp; // 不然就按照链表的方式遍历匹配返回值 ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } 首先将key hash之后取得所定位的桶 如果桶为空，则直接返回null 否则判断桶的第一个位置（有可能是链表、红黑树）的key是否为查询的key，是就直接返回value 如果第一个不匹配，则判断它的下一个是红黑树还是链表 红黑树就按照树的查找方式返回值 不然就按照链表的方式遍历匹配返回值 从这两个核心方法（get/put）可以看出 1.8 中对大链表做了优化，修改为红黑树之后查询效率直接提高到了 O(logn)。 问题但是 HashMap 原有的问题也都存在，比如在并发场景下使用时容易出现死循环。 final HashMap&lt;String, String> map = new HashMap&lt;String, String>(); for (int i = 0; i &lt; 1000; i++) { new Thread(new Runnable() { @Override public void run() { map.put(UUID.randomUUID().toString(), \"\"); } }).start(); } HashMap扩容的时候会调用resize()方法，就是这里的并发操作容易在一个桶上形成环形链表 这样当获取一个不存在的key时，计算出的index正好是环形链表的下标就会出现死循环。 但是1.7的头插法造成的问题，1.8改变了插入顺序，就解决了这个问题，但是为了内存可见性等安全性，还是需要ConCurrentHashMap 遍历方式还有一个值得注意的是 HashMap 的遍历方式，通常有以下几种： Iterator&lt;Map.Entry&lt;String, Integer>> entryIterator = map.entrySet().iterator(); while (entryIterator.hasNext()) { Map.Entry&lt;String, Integer> next = entryIterator.next(); System.out.println(\"key=\" + next.getKey() + \" value=\" + next.getValue()); } Iterator&lt;String> iterator = map.keySet().iterator(); while (iterator.hasNext()){ String key = iterator.next(); System.out.println(\"key=\" + key + \" value=\" + map.get(key)); } 建议使用第一种，同时可以把key value取出。 第二种还需要通过key取一次key，效率较低。 ConCurrentHashMapjdk1.7 Segment数组 HashEntry组成 和HashMap一样，仍然是数组加链表 /** * Segment 数组，存放数据时首先需要定位到具体的 Segment 中。 */ final Segment&lt;K,V>[] segments; transient Set&lt;K> keySet; transient Set&lt;Map.Entry&lt;K,V>> entrySet; Segment 是 ConcurrentHashMap 的一个内部类，主要的组成如下： static final class Segment&lt;K,V> extends ReentrantLock implements Serializable { private static final long serialVersionUID = 2249069246763182397L; // 和 HashMap 中的 HashEntry 作用一样，真正存放数据的桶 transient volatile HashEntry&lt;K,V>[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor; } 唯一的区别就是其中的核心数据如 value ，以及链表都是 volatile 修饰的，保证了获取时的可见性。 ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。 不会像HashTable那样不管是put还是get操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。 每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 putpublic V put(K key, V value) { Segment&lt;K,V> s; if (value == null) throw new NullPointerException(); int hash = hash(key); int j = (hash >>> segmentShift) &amp; segmentMask; if ((s = (Segment&lt;K,V>)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); return s.put(key, hash, value, false); } 通过key定位到Segment，之后在对应的Segment中进行具体的put final V put(K key, int hash, V value, boolean onlyIfAbsent) { HashEntry&lt;K,V> node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try { HashEntry&lt;K,V>[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V> first = entryAt(tab, index); for (HashEntry&lt;K,V> e = first;;) { if (e != null) { K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { e.value = value; ++modCount; } break; } e = e.next; } else { if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V>(hash, key, value, first); int c = count + 1; if (c > threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; } } } finally { unlock(); } return oldValue; } 虽然HashEntry中的value是用volatile关键字修饰的，但是并不能保证并发的原子性，所以put操作仍然需要加锁处理。 首先第一步的时候会尝试获取锁，如果获取失败肯定就是其他线程存在竞争，则利用 scanAndLockForPut() 自旋获取锁。 尝试获取自旋锁 如果重试的次数达到了MAX_SCAN_RETRIES 则改为阻塞锁获取，保证能获取成功。 将当前的Segment中的table通过key的hashcode定位到HashEntry 遍历该HashEntry，如果不为空则判断传入的key和当前遍历的key是否相等，相等则覆盖旧的value 不为空则需要新建一个HashEntry并加入到Segment中，同时会先判断是否需要扩容 最后会解除在1中所获取当前Segment的锁。 get方法public V get(Object key) { Segment&lt;K,V> s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V>[] tab; int h = hash(key); long u = (((h >>> segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; if ((s = (Segment&lt;K,V>)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) { for (HashEntry&lt;K,V> e = (HashEntry&lt;K,V>) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) { K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; } } return null; } 只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。 由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。 ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。 jdk1.8那就是查询遍历链表效率太低。 其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性 也将 1.7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。 其中的 val next 都用了 volatile 修饰，保证了可见性。 put 根据key计算出hashcode 判断是否需要进行初始化 f即为当前key定位出的Node，如果为空表示当前位置可以写入数据，利用CAS尝试写入，失败则自旋保证成功。 如果当前位置的hashcode == MOVED == -1，则需要进行扩容 如果都不满足，则利用synchronized锁写入数据 如果数量大于TREEIFY_THRESHOLD 则要转换为红黑树。 get 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 就不满足那就按照链表的方式遍历获取值。 1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。 总结套路： 谈谈你理解的 HashMap，讲讲其中的 get put 过程。 1.8 做了什么优化？ 是线程安全的嘛？ 不安全会导致哪些问题？ 如何解决？有没有线程安全的并发容器？ ConcurrentHashMap 是如何实现的？ 1.7、1.8 实现有何不同？为什么这么做？","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"linux-基础","slug":"linux-基础","date":"2019-11-12T07:23:30.000Z","updated":"2020-10-29T15:49:29.359Z","comments":true,"path":"2019/11/12/linux-ji-chu/","link":"","permalink":"http://dreamcat.ink/2019/11/12/linux-ji-chu/","excerpt":"","text":"引言linux-基础 常用操作以及概念快捷键 Tab: 命令和文件名补全； Ctrl+C: 中断正在运行的程序； Ctrl+D: 结束键盘输入(End Of File，EOF) 求助–help指令的基本用法与选项介绍。 manman 是 manual 的缩写，将指令的具体信息显示出来。 infoinfo 与 man 类似，但是 info 将文档分成一个个页面，每个页面可以进行跳转。 doc/usr/share/doc 存放着软件的一整套说明文件。 关机who在关机前需要先使用 who 命令查看有没有其它用户在线。 sync为了加快对磁盘文件的读写速度，位于内存中的文件数据不会立即同步到磁盘上，因此关机之前需要先进行 sync 同步操作。 shutdown## shutdown [-krhc] 时间 [信息] -k : 不会关机，只是发送警告信息，通知所有在线的用户 -r : 将系统的服务停掉后就重新启动 -h : 将系统的服务停掉后就立即关机 -c : 取消已经在进行的 shutdown 指令内容 PATH可以在环境变量 PATH 中声明可执行文件的路径，路径之间用 : 分隔。 /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/dmtsai/.local/bin:/home/dmtsai/bin sudosudo 允许一般用户使用 root 可执行的命令，不过只有在 /etc/sudoers 配置文件中添加的用户才能使用该指令. 包管理工具RPM 和 DPKG 为最常见的两类软件包管理工具: RPM 全称为 Redhat Package Manager，最早由 Red Hat 公司制定实施，随后被 GNU 开源操作系统接受并成为很多 Linux 系统 (RHEL) 的既定软件标准。 与 RPM 进行竞争的是基于 Debian 操作系统 (Ubuntu) 的 DEB 软件包管理工具 DPKG，全称为 Debian Package，功能方面与 RPM 相似。 YUM 基于 RPM，具有依赖管理功能，并具有软件升级的功能。 发行版Linux 发行版是 Linux 内核及各种应用软件的集成版本。 基于的包管理工具 商业发行版 社区发行版 RPM Red Hat Fedora / CentOS DPKG Ubuntu Debian VIM 一般指令模式(Command mode): VIM 的默认模式，可以用于移动游标查看内容； 编辑模式(Insert mode): 按下 “i” 等按键之后进入，可以对文本进行编辑； 指令列模式(Bottom-line mode): 按下 “:” 按键之后进入，用于保存退出等操作。 在指令列模式下，有以下命令用于离开或者保存文件。 命令 作用 :w 写入磁盘 :w! 当文件为只读时，强制写入磁盘。到底能不能写入，与用户对该文件的权限有关 :q 离开 :q! 强制离开不保存 :wq 写入磁盘后离开 :wq! 强制写入磁盘后离开 磁盘磁盘接口IDEIDE(ATA)全称 Advanced Technology Attachment，接口速度最大为 133MB/s，因为并口线的抗干扰性太差，且排线占用空间较大，不利电脑内部散热，已逐渐被 SATA 所取代。 SATASATA 全称 Serial ATA，也就是使用串口的 ATA 接口，抗干扰性强，且对数据线的长度要求比 ATA 低很多，支持热插拔等功能。SATA-II 的接口速度为 300MiB/s，而新的 SATA-III 标准可达到 600MiB/s 的传输速度。SATA 的数据线也比 ATA 的细得多，有利于机箱内的空气流通，整理线材也比较方便。 SCSISCSI 全称是 Small Computer System Interface(小型机系统接口)，经历多代的发展，从早期的 SCSI-II 到目前的 Ultra320 SCSI 以及 Fiber-Channel(光纤通道)，接口型式也多种多样。SCSI 硬盘广为工作站级个人电脑以及服务器所使用，因此会使用较为先进的技术，如碟片转速 15000rpm 的高转速，且传输时 CPU 占用率较低，但是单价也比相同容量的 ATA 及 SATA 硬盘更加昂贵。 SASSAS(Serial Attached SCSI)是新一代的 SCSI 技术，和 SATA 硬盘相同，都是采取序列式技术以获得更高的传输速度，可达到 6Gb/s。此外也透过缩小连接线改善系统内部空间等。 分区分区表磁盘分区表主要有两种格式，一种是限制较多的 MBR 分区表，一种是较新且限制较少的 GPT 分区表。 MBRMBR 中，第一个扇区最重要，里面有主要开机记录(Master boot record, MBR)及分区表(partition table)，其中主要开机记录占 446 bytes，分区表占 64 bytes。 分区表只有 64 bytes，最多只能存储 4 个分区，这 4 个分区为主分区(Primary)和扩展分区(Extended)。其中扩展分区只有一个，它使用其它扇区用记录额外的分区表，因此通过扩展分区可以分出更多分区，这些分区称为逻辑分区。 Linux 也把分区当成文件，分区文件的命名方式为: 磁盘文件名 + 编号，例如 /dev/sda1。注意，逻辑分区的编号从 5 开始。 GPT不同的磁盘有不同的扇区大小，例如 512 bytes 和最新磁盘的 4 k。GPT 为了兼容所有磁盘，在定义扇区上使用逻辑区块地址(Logical Block Address, LBA)，LBA 默认大小为 512 bytes。 GPT 第 1 个区块记录了主要开机记录(MBR)，紧接着是 33 个区块记录分区信息，并把最后的 33 个区块用于对分区信息进行备份。这 33 个区块第一个为 GPT 表头纪录，这个部份纪录了分区表本身的位置与大小和备份分区的位置，同时放置了分区表的校验码 (CRC32)，操作系统可以根据这个校验码来判断 GPT 是否正确。若有错误，可以使用备份分区进行恢复。 GPT 没有扩展分区概念，都是主分区，每个 LAB 可以分 4 个分区，因此总共可以分 4 * 32 = 128 个分区。 MBR 不支持 2.2 TB 以上的硬盘，GPT 则最多支持到 233 TB = 8 ZB。 开机检测程序BIOSBIOS(Basic Input/Output System，基本输入输出系统)，它是一个固件(嵌入在硬件中的软件)，BIOS 程序存放在断电后内容不会丢失的只读内存中。 BIOS 是开机的时候计算机执行的第一个程序，这个程序知道可以开机的磁盘，并读取磁盘第一个扇区的主要开机记录(MBR)，由主要开机记录(MBR)执行其中的开机管理程序，这个开机管理程序会加载操作系统的核心文件。 主要开机记录(MBR)中的开机管理程序提供以下功能: 选单、载入核心文件以及转交其它开机管理程序。转交这个功能可以用来实现了多重引导，只需要将另一个操作系统的开机管理程序安装在其它分区的启动扇区上，在启动开机管理程序时，就可以通过选单选择启动当前的操作系统或者转交给其它开机管理程序从而启动另一个操作系统。 下图中，第一扇区的主要开机记录(MBR)中的开机管理程序提供了两个选单: M1、M2，M1 指向了 Windows 操作系统，而 M2 指向其它分区的启动扇区，里面包含了另外一个开机管理程序，提供了一个指向 Linux 的选单。 安装多重引导，最好先安装 Windows 再安装 Linux。因为安装 Windows 时会覆盖掉主要开机记录(MBR)，而 Linux 可以选择将开机管理程序安装在主要开机记录(MBR)或者其它分区的启动扇区，并且可以设置开机管理程序的选单。 UEFIBIOS 不可以读取 GPT 分区表，而 UEFI 可以。 文件系统分区与文件系统对分区进行格式化是为了在分区上建立文件系统。一个分区通常只能格式化为一个文件系统，但是磁盘阵列等技术可以将一个分区格式化为多个文件系统。 组成最主要的几个组成部分如下: inode: 一个文件占用一个 inode，记录文件的属性，同时记录此文件的内容所在的 block 编号； block: 记录文件的内容，文件太大时，会占用多个 block。 superblock: 记录文件系统的整体信息，包括 inode 和 block 的总量、使用量、剩余量，以及文件系统的格式与相关信息等； block bitmap: 记录 block 是否被使用的位域。 文件读取对于 Ext2 文件系统，当要读取一个文件的内容时，先在 inode 中去查找文件内容所在的所有 block，然后把所有 block 的内容读出来。 磁盘碎片指一个文件内容所在的 block 过于分散。 Block在 Ext2 文件系统中所支持的 block 大小有 1K，2K 及 4K 三种，不同的大小限制了单个文件和文件系统的最大大小。 一个 block 只能被一个文件所使用，未使用的部分直接浪费了。因此如果需要存储大量的小文件，那么最好选用比较小的 block。 inodeinode 具体包含以下信息: 权限 (read/write/excute)； 拥有者与群组 (owner/group)； 容量； 建立或状态改变的时间 (ctime)； 最近一次的读取时间 (atime)； 最近修改的时间 (mtime)； 定义文件特性的旗标 (flag)，如 SetUID…； 该文件真正内容的指向 (pointer)。 inode 具有以下特点: 每个 inode 大小均固定为 128 bytes (新的 ext4 与 xfs 可设定到 256 bytes)； 每个文件都仅会占用一个 inode。 inode 中记录了文件内容所在的 block 编号，但是每个 block 非常小，一个大文件随便都需要几十万的 block。而一个 inode 大小有限，无法直接引用这么多 block 编号。因此引入了间接、双间接、三间接引用。间接引用是指，让 inode 记录的引用 block 块记录引用信息。 目录建立一个目录时，会分配一个 inode 与至少一个 block。block 记录的内容是目录下所有文件的 inode 编号以及文件名。 可以看出文件的 inode 本身不记录文件名，文件名记录在目录中，因此新增文件、删除文件、更改文件名这些操作与目录的 w 权限有关。 日志如果突然断电，那么文件系统会发生错误，例如断电前只修改了 block bitmap，而还没有将数据真正写入 block 中。 ext3/ext4 文件系统引入了日志功能，可以利用日志来修复文件系统。 挂载挂载利用目录作为文件系统的进入点，也就是说，进入目录之后就可以读取文件系统的数据。 目录配置为了使不同 Linux 发行版本的目录结构保持一致性，Filesystem Hierarchy Standard (FHS) 规定了 Linux 的目录结构。最基础的三个目录如下: / (root, 根目录) /usr (unix software resource): 所有系统默认软件都会安装到这个目录； /var (variable): 存放系统或程序运行过程中的数据文件。 文件文件属性用户分为三种: 文件拥有者、群组以及其它人，对不同的用户有不同的文件权限。 使用 ls 查看一个文件时，会显示一个文件的信息，例如 drwxr-xr-x. 3 root root 17 May 6 00:14 .config，对这个信息的解释如下: drwxr-xr-x: 文件类型以及权限，第 1 位为文件类型字段，后 9 位为文件权限字段 3: 链接数 root: 文件拥有者 root: 所属群组 17: 文件大小 May 6 00:14: 文件最后被修改的时间 .config: 文件名 常见的文件类型及其含义有: d: 目录 -: 文件 l: 链接文件 9 位的文件权限字段中，每 3 个为一组，共 3 组，每一组分别代表对文件拥有者、所属群组以及其它人的文件权限。一组权限中的 3 位分别为 r、w、x 权限，表示可读、可写、可执行。 文件时间有以下三种: modification time (mtime): 文件的内容更新就会更新； status time (ctime): 文件的状态(权限、属性)更新就会更新； access time (atime): 读取文件时就会更新。 文件与目录的基本操作ls列出文件或者目录的信息，目录的信息就是其中包含的文件。 ## ls [-aAdfFhilnrRSt] file|dir -a : 列出全部的文件 -d : 仅列出目录本身 -l : 以长数据串行列出，包含文件的属性与权限等等数据 cd更新当前目录 cd [相对路径或者绝对路径] mkdir创建目录 ## mkdir [-mp] 目录名称 -m : 配置目录权限 -p : 递归创建目录 rmdir删除目录，目录必须为空 rmdir [-p] 目录名称 -p : 递归删除目录 touch更新文件时间或者建立新文件 ## touch [-acdmt] filename -a : 更新 atime -c : 更新 ctime，若该文件不存在则不建立新文件 -m : 更新 mtime -d : 后面可以接更新日期而不使用当前日期，也可以使用 --date=\"日期或时间\" -t : 后面可以接更新时间而不使用当前时间，格式为[YYYYMMDDhhmm] cp复制文件 如果源文件有两个以上，则目的文件一定要是目录才行。 cp [-adfilprsu] source destination -a : 相当于 -dr --preserve=all 的意思，至于 dr 请参考下列说明 -d : 若来源文件为链接文件，则复制链接文件属性而非文件本身 -i : 若目标文件已经存在时，在覆盖前会先询问 -p : 连同文件的属性一起复制过去 -r : 递归持续复制 -u : destination 比 source 旧才更新 destination，或 destination 不存在的情况下才复制 --preserve=all : 除了 -p 的权限相关参数外，还加入 SELinux 的属性, links, xattr 等也复制了 rm删除文件 ## rm [-fir] 文件或目录 -r : 递归删除 mv移动文件 ## mv [-fiu] source destination ## mv [options] source1 source2 source3 .... directory -f : force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖 修改权限可以将一组权限用数字来表示，此时一组权限的 3 个位当做二进制数字的位，从左到右每个位的权值为 4、2、1，即每个权限对应的数字权值为 r : 4、w : 2、x : 1。 ## chmod [-R] xyz dirname/filename 示例: 将 .bashrc 文件的权限修改为 -rwxr-xr–。 ## chmod 754 .bashrc 也可以使用符号来设定权限。 ## chmod [ugoa] [+-=] [rwx] dirname/filename - u: 拥有者 - g: 所属群组 - o: 其他人 - a: 所有人 - +: 添加权限 - -: 移除权限 - =: 设定权限 示例: 为 .bashrc 文件的所有用户添加写权限。 ## chmod a+w .bashrc 文件默认权限 文件默认权限: 文件默认没有可执行权限，因此为 666，也就是 -rw-rw-rw- 。 目录默认权限: 目录必须要能够进入，也就是必须拥有可执行权限，因此为 777 ，也就是 drwxrwxrwx。 可以通过 umask 设置或者查看文件的默认权限，通常以掩码的形式来表示，例如 002 表示其它用户的权限去除了一个 2 的权限，也就是写权限，因此建立新文件时默认的权限为 -rw-rw-r–。 目录的权限文件名不是存储在一个文件的内容中，而是存储在一个文件所在的目录中。因此，拥有文件的 w 权限并不能对文件名进行修改。 目录存储文件列表，一个目录的权限也就是对其文件列表的权限。因此，目录的 r 权限表示可以读取文件列表；w 权限表示可以修改文件列表，具体来说，就是添加删除文件，对文件名进行修改；x 权限可以让该目录成为工作目录，x 权限是 r 和 w 权限的基础，如果不能使一个目录成为工作目录，也就没办法读取文件列表以及对文件列表进行修改了。 链接## ln [-sf] source_filename dist_filename -s : 默认是 hard link，加 -s 为 symbolic link -f : 如果目标文件存在时，先删除目标文件 实体链接在目录下创建一个条目，记录着文件名与 inode 编号，这个 inode 就是源文件的 inode。 删除任意一个条目，文件还是存在，只要引用数量不为 0。 有以下限制: 不能跨越文件系统、不能对目录进行链接。 ## ln /etc/crontab . ## ll -i /etc/crontab crontab 34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 crontab 34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab 符号链接符号链接文件保存着源文件所在的绝对路径，在读取时会定位到源文件上，可以理解为 Windows 的快捷方式。 当源文件被删除了，链接文件就打不开了。 可以为目录建立链接。 ## ll -i /etc/crontab /root/crontab2 34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab 53745909 lrwxrwxrwx. 1 root root 12 Jun 23 22:31 /root/crontab2 -> /etc/crontab 获取文件内容cat取得文件内容。 ## cat [-AbEnTv] filename -n : 打印出行号，连同空白行也会有行号，-b 不会 tac是 cat 的反向操作，从最后一行开始打印。 more和 cat 不同的是它可以一页一页查看文件内容，比较适合大文件的查看。 less和 more 类似，但是多了一个向前翻页的功能。 head取得文件前几行。 ## head [-n number] filename -n : 后面接数字，代表显示几行的意思 tail是head的反向操作，之水取得是后几行 od以字符或者十六进制的形式显示二进制文件。 指令与文件搜索which指令搜索 ## which [-a] command -a : 将所有指令列出，而不是只列第一个 whereis文件搜索。速度比较快，因为它只搜索几个特定的目录。 ## whereis [-bmsu] dirname/filename locate文件搜索。可以用关键字或者正则表达式进行搜索。 locate 使用 /var/lib/mlocate/ 这个数据库来进行搜索，它存储在内存中，并且每天更新一次，所以无法用 locate 搜索新建的文件。可以使用 updatedb 来立即更新数据库。 ## locate [-ir] keyword -r: 正则表达式 find文件搜索。可以使用文件的属性和权限进行搜索。 ## find [basedir] [option] example: find . -name \"shadow*\" 压缩与打包压缩文件名gzipgzip 是 Linux 使用最广的压缩指令，可以解开 compress、zip 与 gzip 所压缩的文件。 经过 gzip 压缩过，源文件就不存在了。 有 9 个不同的压缩等级可以使用。 可以使用 zcat、zmore、zless 来读取压缩文件的内容。 $ gzip [-cdtv#] filename -c : 将压缩的数据输出到屏幕上 -d : 解压缩 -t : 检验压缩文件是否出错 -v : 显示压缩比等信息 -## : ## 为数字的意思，代表压缩等级，数字越大压缩比越高，默认为 6 bzip2提供比 gzip 更高的压缩比。 查看命令: bzcat、bzmore、bzless、bzgrep。 xz提供比 bzip2 更佳的压缩比。 可以看到，gzip、bzip2、xz 的压缩比不断优化。不过要注意的是，压缩比越高，压缩的时间也越长。 查看命令: xzcat、xzmore、xzless、xzgrep。 打包压缩指令只能对一个文件进行压缩，而打包能够将多个文件打包成一个大文件。tar 不仅可以用于打包，也可以使用 gip、bzip2、xz 将打包文件进行压缩。 $ tar [-z|-j|-J] [cv] [-f 新建的 tar 文件] filename... ==打包压缩 $ tar [-z|-j|-J] [tv] [-f 已有的 tar 文件] ==查看 $ tar [-z|-j|-J] [xv] [-f 已有的 tar 文件] [-C 目录] ==解压缩 -z : 使用 zip； -j : 使用 bzip2； -J : 使用 xz； -c : 新建打包文件； -t : 查看打包文件里面有哪些文件； -x : 解打包或解压缩的功能； -v : 在压缩/解压缩的过程中，显示正在处理的文件名； -f : filename: 要处理的文件； -C 目录 : 在特定目录解压缩。 使用方式 命令 打包压缩 tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称 查 看 tar -jtv -f filename.tar.bz2 解压缩 tar -jxv -f filename.tar.bz2 -C 要解压缩的目录 bash可以通过 Shell 请求内核提供服务，Bash 正是 Shell 的一种。 特性 命令历史：记录使用过的命令 命令与文件补全：快捷键：tab shell scripts 通配符: 例如 ls -l /usr/bin/X* 列出 /usr/bin 下面所有以 X 开头的文件 变量操作对一个变量赋值直接使用 =。 对变量取用需要在变量前加上 $ ，也可以用 ${} 的形式； 输出变量使用 echo 命令。 $ x=abc $ echo $x $ echo ${x} 变量内容如果有空格，必须使用双引号或者单引号。 双引号内的特殊字符可以保留原本特性，例如 x=”lang is $LANG”，则 x 的值为 lang is zh_TW.UTF-8； 单引号内的特殊字符就是特殊字符本身，例如 x=’lang is $LANG’，则 x 的值为 lang is $LANG。 可以使用 指令 或者 $(指令) 的方式将指令的执行结果赋值给变量。例如 version=$(uname -r)，则 version 的值为 4.15.0-22-generic。 可以使用 指令 或者 $(指令) 的方式将指令的执行结果赋值给变量。例如 version=$(uname -r)，则 version 的值为 4.15.0-22-generic。 Bash 的变量可以声明为数组和整数数字。注意数字类型没有浮点数。如果不进行声明，默认是字符串类型。变量的声明使用 declare 命令: $ declare [-aixr] variable -a : 定义为数组类型 -i : 定义为整数类型 -x : 定义为环境变量 -r : 定义为 readonly 类型 对数组操作 $ array[1]=a $ array[2]=b $ echo ${array[1]} 正则g/re/p(globally search a regular expression and print)，使用正则表示式进行全局查找并打印。 $ grep [-acinv] [--color=auto] 搜寻字符串 filename -c : 统计个数 -i : 忽略大小写 -n : 输出行号 -v : 反向选择，也就是显示出没有 搜寻字符串 内容的那一行 --color=auto : 找到的关键字加颜色显示 示例: 把含有 the 字符串的行提取出来(注意默认会有 –color=auto 选项，因此以下内容在 Linux 中有颜色显示 the 字符串) $ grep -n 'the' regular_express.txt 8:I can't finish the test. 12:the symbol '*' is represented as start. 15:You are the best is mean you are the no. 1. 16:The world Happy is the same with \"glad\". 18:google is the best tools for search keyword 进程管理查看进程ps查看某个时间点的进程信息 示例一: 查看自己的进程ps -l 示例二: 查看系统所有进程ps aux 示例三: 查看特定的进程ps aux | grep python top实时显示进程信息 示例: 两秒钟刷新一次top -d 2 pstree查看进程树 示例: 查看所有进程树pstree -A netstat查看占用端口的进程 示例: 查看特定端口的进程netstat -anp | grep port 孤儿进程一个父进程退出，而它的一个或多个子进程还在运行，那么这些子进程将成为孤儿进程。 孤儿进程将被init进程（进程号为1）所收养，并有init进程对它们完成状态收集工作。 由于孤儿进程会被init进程收养，所以孤儿进程不会对系统造成危害。 僵尸进程一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过wait()或者waitpid()获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用wait()或waitpid()，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程。 僵尸进程通过 ps 命令显示出来的状态为 Z(zombie)。 系统所能使用的进程号是有限的，如果产生大量僵尸进程，将因为没有可用的进程号而导致系统不能产生新的进程。 要消灭系统中大量的僵尸进程，只需要将其父进程杀死，此时僵尸进程就会变成孤儿进程，从而被 init 所收养，这样 init 就会释放所有的僵尸进程所占有的资源，从而结束僵尸进程。","categories":[{"name":"linux","slug":"linux","permalink":"http://dreamcat.ink/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://dreamcat.ink/tags/linux/"}]},{"title":"Java集合- WeakHashMap源码解析","slug":"Java集合-WeakHashMap源码解析","date":"2019-10-31T11:15:30.000Z","updated":"2020-10-29T15:49:54.346Z","comments":true,"path":"2019/10/31/java-ji-he-weakhashmap-yuan-ma-jie-xi/","link":"","permalink":"http://dreamcat.ink/2019/10/31/java-ji-he-weakhashmap-yuan-ma-jie-xi/","excerpt":"","text":"引言Java的集合框架，TreeSet &amp; TreeMap源码解析等… 总体介绍WeakHashMap，从名字可以看出它是某种 *Map。它的特殊之处在于 *WeakHashMap 里的entry可能会被GC自动删除，即使程序员没有调用remove()或者clear()方法。 更直观的说，当使用 WeakHashMap 时，即使没有显示的添加或删除任何元素，也可能发生如下情况： 调用两次size()方法返回不同的值； 两次调用isEmpty()方法，第一次返回false，第二次返回true； 两次调用containsKey()方法，第一次返回true，第二次返回false，尽管两次使用的是同一个key； 两次调用get()方法，第一次返回一个value，第二次返回null，尽管两次使用的是同一个对象。 其实不然，WeekHashMap\\ 的这个特点特别适用于需要缓存的场景。在缓存场景下，由于内存是有限的，不能缓存所有对象；对象缓存命中可以提高系统效率，但缓存MISS也不会造成错误，因为可以通过计算重新得到。 要明白 WeekHashMap 的工作原理，还需要引入一个概念：弱引用（WeakReference）。我们都知道Java中内存是通过GC自动管理的，GC会在程序运行过程中自动判断哪些对象是可以被回收的，并在合适的时机进行内存释放。GC判断某个对象是否可被回收的依据是，是否有有效的引用指向该对象。如果没有有效引用指向该对象（基本意味着不存在访问该对象的方式），那么该对象就是可回收的。这里的“有效引用”并不包括弱引用。也就是说，虽然弱引用可以用来访问对象，但进行垃圾回收时弱引用并不会被考虑在内，仅有弱引用指向的对象仍然会被GC回收。 WeakHashMap 内部是通过弱引用来管理entry的，弱引用的特性对应到 WeakHashMap 上意味着什么呢？将一对key, value放入到 *WeakHashMap* 里并不能避免该key值被GC回收，除非在 *WeakHashMap* 之外还有对该key的强引用。 WeakHashSet不过Java Collections工具类给出了解决方案，Collections.newSetFromMap(Map map)方法可以将任何 Map包装成一个Set。通过如下方式可以快速得到一个 Weak HashSet： // 将WeakHashMap包装成一个Set Set&lt;Object> weakHashSet = Collections.newSetFromMap( new WeakHashMap&lt;Object, Boolean>()); 不出你所料，newSetFromMap()方法只是对传入的 Map做了简单包装： // Collections.newSetFromMap()用于将任何Map包装成一个Set public static &lt;E> Set&lt;E> newSetFromMap(Map&lt;E, Boolean> map) { return new SetFromMap&lt;>(map); } private static class SetFromMap&lt;E> extends AbstractSet&lt;E> implements Set&lt;E>, Serializable { private final Map&lt;E, Boolean> m; // The backing map private transient Set&lt;E> s; // Its keySet SetFromMap(Map&lt;E, Boolean> map) { if (!map.isEmpty()) throw new IllegalArgumentException(\"Map is non-empty\"); m = map; s = map.keySet(); } public void clear() { m.clear(); } public int size() { return m.size(); } public boolean isEmpty() { return m.isEmpty(); } public boolean contains(Object o) { return m.containsKey(o); } public boolean remove(Object o) { return m.remove(o) != null; } public boolean add(E e) { return m.put(e, Boolean.TRUE) == null; } public Iterator&lt;E> iterator() { return s.iterator(); } public Object[] toArray() { return s.toArray(); } public &lt;T> T[] toArray(T[] a) { return s.toArray(a); } public String toString() { return s.toString(); } public int hashCode() { return s.hashCode(); } public boolean equals(Object o) { return o == this || s.equals(o); } public boolean containsAll(Collection&lt;?> c) {return s.containsAll(c);} public boolean removeAll(Collection&lt;?> c) {return s.removeAll(c);} public boolean retainAll(Collection&lt;?> c) {return s.retainAll(c);} // addAll is the only inherited implementation ...... }","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"Java集合- LinkedHashSet & Map源码解析","slug":"Java集合-LinkedHashSet-Map源码解析","date":"2019-10-31T10:48:58.000Z","updated":"2020-10-29T15:51:46.693Z","comments":true,"path":"2019/10/31/java-ji-he-linkedhashset-map-yuan-ma-jie-xi/","link":"","permalink":"http://dreamcat.ink/2019/10/31/java-ji-he-linkedhashset-map-yuan-ma-jie-xi/","excerpt":"","text":"引言Java的集合框架，TreeSet &amp; TreeMap源码解析等… 总体介绍LinkedHashMap实现了Map接口，即允许放入key为null的元素，也允许插入value为null的元素。从名字上可以看出该容器是linked list和HashMap的混合体，也就是说它同时满足HashMap和linked list的某些特性。可将*LinkedHashMap*看作采用*linked list*增强的*HashMap*。 事实上LinkedHashMap是HashMap的直接子类，二者唯一的区别是*LinkedHashMap*在*HashMap*的基础上，采用双向链表（doubly-linked list）的形式将所有entry连接起来，这样是为保证元素的迭代顺序跟插入顺序相同。上图给出了LinkedHashMap的结构图，主体部分跟HashMap完全一样，多了header指向双向链表的头部（是一个哑元），该双向链表的迭代顺序就是entry的插入顺序。 除了可以保迭代历顺序，这种结构还有一个好处：迭代*LinkedHashMap*时不需要像*HashMap*那样遍历整个table，而只需要直接遍历header指向的双向链表即可，也就是说LinkedHashMap的迭代时间就只跟entry的个数相关，而跟table的大小无关。 有两个参数可以影响LinkedHashMap的性能：初始容量（inital capacity）和负载系数（load factor）。初始容量指定了初始table的大小，负载系数用来指定自动扩容的临界值。当entry的数量超过capacity*load_factor时，容器将自动扩容并重新哈希。对于插入元素较多的场景，将初始容量设大可以减少重新哈希的次数。 将对象放入到LinkedHashMap或LinkedHashSet中时，有两个方法需要特别关心：hashCode()和equals()。*hashCode()方法决定了对象会被放到哪个bucket里，当多个对象的哈希值冲突时，equals()方法决定了这些对象是否是“同一个对象”**。所以，如果要将自定义的对象放入到LinkedHashMap或LinkedHashSet中，需要@Override*hashCode()和equals()方法。 通过如下方式可以得到一个跟源Map 迭代顺序一样的LinkedHashMap： void foo(Map m) { Map copy = new LinkedHashMap(m); ... } 出于性能原因，LinkedHashMap是非同步的（not synchronized），如果需要在多线程环境使用，需要程序员手动同步；或者通过如下方式将LinkedHashMap包装成（wrapped）同步的： Map m = Collections.synchronizedMap(new LinkedHashMap(...)); 方法解析put()put(K key, V value)方法是将指定的key, value对添加到map里。该方法首先会对map做一次查找，看是否包含该元组，如果已经包含则直接返回，查找过程类似于get()方法；如果没有找到，则会通过addEntry(int hash, K key, V value, int bucketIndex)方法插入新的entry。 从table的角度看，新的entry需要插入到对应的bucket里，当有哈希冲突时，采用头插法将新的entry插入到冲突链表的头部。 从header的角度看，新的entry需要插入到双向链表的尾部。 // LinkedHashMap.addEntry() void addEntry(int hash, K key, V value, int bucketIndex) { if ((size >= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length);// 自动扩容，并重新哈希 hash = (null != key) ? hash(key) : 0; bucketIndex = hash &amp; (table.length-1);// hash%table.length } // 1.在冲突链表头部插入新的entry HashMap.Entry&lt;K,V> old = table[bucketIndex]; Entry&lt;K,V> e = new Entry&lt;>(hash, key, value, old); table[bucketIndex] = e; // 2.在双向链表的尾部插入新的entry e.addBefore(header); size++; } 上述代码中用到了addBefore()方法将新entry e插入到双向链表头引用header的前面，这样e就成为双向链表中的最后一个元素。addBefore()的代码如下： // LinkedHashMap.Entry.addBefor()，将this插入到existingEntry的前面 private void addBefore(Entry&lt;K,V> existingEntry) { after = existingEntry; before = existingEntry.before; before.after = this; after.before = this; } 上述代码只是简单修改相关entry的引用而已。 remove()remove(Object key)的作用是删除key值对应的entry，该方法的具体逻辑是在removeEntryForKey(Object key)里实现的。removeEntryForKey()方法会首先找到key值对应的entry，然后删除该entry（修改链表的相应引用）。查找过程跟get()方法类似。 从table的角度看，需要将该entry从对应的bucket里删除，如果对应的冲突链表不空，需要修改冲突链表的相应引用。 从header的角度来看，需要将该entry从双向链表中删除，同时修改链表中前面以及后面元素的相应引用。 // LinkedHashMap.removeEntryForKey()，删除key值对应的entry final Entry&lt;K,V> removeEntryForKey(Object key) { ...... int hash = (key == null) ? 0 : hash(key); int i = indexFor(hash, table.length);// hash&amp;(table.length-1) Entry&lt;K,V> prev = table[i];// 得到冲突链表 Entry&lt;K,V> e = prev; while (e != null) {// 遍历冲突链表 Entry&lt;K,V> next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) {// 找到要删除的entry modCount++; size--; // 1. 将e从对应bucket的冲突链表中删除 if (prev == e) table[i] = next; else prev.next = next; // 2. 将e从双向链表中删除 e.before.after = e.after; e.after.before = e.before; return e; } prev = e; e = next; } return e; } LinkedHashSet前面已经说过LinkedHashSet是对LinkedHashMap的简单包装，对LinkedHashSet的函数调用都会转换成合适的LinkedHashMap方法，因此LinkedHashSet的实现非常简单，这里不再赘述。 public class LinkedHashSet&lt;E> extends HashSet&lt;E> implements Set&lt;E>, Cloneable, java.io.Serializable { ...... // LinkedHashSet里面有一个LinkedHashMap public LinkedHashSet(int initialCapacity, float loadFactor) { map = new LinkedHashMap&lt;>(initialCapacity, loadFactor); } ...... public boolean add(E e) {//简单的方法转换 return map.put(e, PRESENT)==null; } ...... } LinkedHashMap经典用法LinkedHashMap除了可以保证迭代顺序外，还有一个非常有用的用法：可以轻松实现一个采用了FIFO替换策略的缓存。具体说来，LinkedHashMap有一个子类方法protected boolean removeEldestEntry(Map.Entry eldest)，该方法的作用是告诉Map是否要删除“最老”的Entry，所谓最老就是当前Map中最早插入的Entry，如果该方法返回true，最老的那个元素就会被删除。在每次插入新元素的之后LinkedHashMap会自动询问removeEldestEntry()是否要删除最老的元素。这样只需要在子类中重载该方法，当元素个数超过一定数量时让removeEldestEntry()返回true，就能够实现一个固定大小的FIFO策略的缓存。示例代码如下： /** 一个固定大小的FIFO替换策略的缓存 */ class FIFOCache&lt;K, V> extends LinkedHashMap&lt;K, V>{ private final int cacheSize; public FIFOCache(int cacheSize){ this.cacheSize = cacheSize; } // 当Entry个数超过cacheSize时，删除最老的Entry @Override protected boolean removeEldestEntry(Map.Entry&lt;K,V> eldest) { return size() > cacheSize; } }","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"Java集合- TreeSet & TreeMap源码解析","slug":"Java集合-TreeSet-TreeMap源码解析","date":"2019-10-31T01:53:49.000Z","updated":"2020-10-30T12:43:44.240Z","comments":true,"path":"2019/10/31/java-ji-he-treeset-treemap-yuan-ma-jie-xi/","link":"","permalink":"http://dreamcat.ink/2019/10/31/java-ji-he-treeset-treemap-yuan-ma-jie-xi/","excerpt":"","text":"引言Java的集合框架，TreeSet &amp; TreeMap源码解析等… 概述之所以把TreeSet和TreeMap放在一起讲解，是因为二者在Java里有着相同的实现，前者仅仅是对后者做了一层包装，也就是说TreeSet\\里面有一个*TreeMap*（适配器模式）。因此本文将重点分析TreeMap。 Java TreeMap实现了SortedMap接口，也就是说会按照key的大小顺序对Map中的元素进行排序，key大小的评判可以通过其本身的自然顺序（natural ordering），也可以通过构造时传入的比较器（Comparator） TreeMap底层通过红黑树（Red-Black tree）实现，也就意味着containsKey(), get(), put(), remove()都有着log(n)的时间复杂度。其具体算法实现参照了《算法导论》。 出于性能原因，TreeMap是非同步的（not synchronized），如果需要在多线程环境使用，需要程序员手动同步；或者通过如下方式将TreeMap包装成（wrapped）同步的： SortedMap m = Collections.synchronizedSortedMap(new TreeMap(...)); 红黑树是一种近似平衡的二叉查找树，它能够确保任何一个节点的左右子树的高度差不会超过二者中较低那个的一陪。具体来说，红黑树是满足如下条件的二叉查找树（binary search tree）： 每个节点要么是红色，要么是黑色。 根节点必须是黑色 红色节点不能连续（也即是，红色节点的孩子和父亲都不能是红色）。 对于每个节点，从该点至null（树尾端）的任何路径，都含有相同个数的黑色节点。 在树的结构发生改变时（插入或者删除操作），往往会破坏上述条件3或条件4，需要通过调整使得查找树重新满足红黑树的约束条件。 预备知识前文说到当查找树的结构发生改变时，红黑树的约束条件可能被破坏，需要通过调整使得查找树重新满足红黑树的约束条件。调整可以分为两类：一类是颜色调整，即改变某个节点的颜色；另一类是结构调整，集改变检索树的结构关系。结构调整过程包含两个基本操作：左旋（Rotate Left），右旋（RotateRight）。 左旋左旋的过程是将x的右子树绕x逆时针旋转，使得x的右子树成为x的父亲，同时修改相关节点的引用。旋转之后，二叉查找树的属性仍然满足。 TreeMap中左旋代码如下： //Rotate Left private void rotateLeft(Entry&lt;K,V> p) { if (p != null) { Entry&lt;K,V> r = p.right; p.right = r.left; if (r.left != null) r.left.parent = p; r.parent = p.parent; if (p.parent == null) root = r; else if (p.parent.left == p) p.parent.left = r; else p.parent.right = r; r.left = p; p.parent = r; } } 右旋右旋的过程是将x的左子树绕x顺时针旋转，使得x的左子树成为x的父亲，同时修改相关节点的引用。旋转之后，二叉查找树的属性仍然满足。 TreeMap中右旋代码如下： //Rotate Right private void rotateRight(Entry&lt;K,V> p) { if (p != null) { Entry&lt;K,V> l = p.left; p.left = l.right; if (l.right != null) l.right.parent = p; l.parent = p.parent; if (p.parent == null) root = l; else if (p.parent.right == p) p.parent.right = l; else p.parent.left = l; l.right = p; p.parent = l; } } 寻找节点后继对于一棵二叉查找树，给定节点t，其后继（树中比大于t的最小的那个元素）可以通过如下方式找到： t的右子树不空，则t的后继是其右子树中最小的那个元素。 t的右孩子为空，则t的后继是其第一个向左走的祖先。 TreeMap中寻找节点后继的代码如下： // 寻找节点后继函数successor() static &lt;K,V> TreeMap.Entry&lt;K,V> successor(Entry&lt;K,V> t) { if (t == null) return null; else if (t.right != null) {// 1. t的右子树不空，则t的后继是其右子树中最小的那个元素 Entry&lt;K,V> p = t.right; while (p.left != null) p = p.left; return p; } else {// 2. t的右孩子为空，则t的后继是其第一个向左走的祖先 Entry&lt;K,V> p = t.parent; Entry&lt;K,V> ch = t; while (p != null &amp;&amp; ch == p.right) { ch = p; p = p.parent; } return p; } } 方法解析get()get(Object key)方法根据指定的key值返回对应的value，该方法调用了getEntry(Object key)得到相应的entry，然后返回entry.value。因此getEntry()是算法的核心。算法思想是根据key的自然顺序（或者比较器顺序）对二叉查找树进行查找，直到找到满足k.compareTo(p.key) == 0的entry。 //getEntry()方法 final Entry&lt;K,V> getEntry(Object key) { ...... if (key == null)//不允许key值为null throw new NullPointerException(); Comparable&lt;? super K> k = (Comparable&lt;? super K>) key;//使用元素的自然顺序 Entry&lt;K,V> p = root; while (p != null) { int cmp = k.compareTo(p.key); if (cmp &lt; 0)//向左找 p = p.left; else if (cmp > 0)//向右找 p = p.right; else return p; } return null; } put()put(K key, V value)方法是将指定的key, value对添加到map里。该方法首先会对map做一次查找，看是否包含该元组，如果已经包含则直接返回，查找过程类似于getEntry()方法；如果没有找到则会在红黑树中插入新的entry，如果插入之后破坏了红黑树的约束条件，还需要进行调整（旋转，改变某些节点的颜色）。 public V put(K key, V value) { ...... int cmp; Entry&lt;K,V> parent; if (key == null) throw new NullPointerException(); Comparable&lt;? super K> k = (Comparable&lt;? super K>) key;//使用元素的自然顺序 do { parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left;//向左找 else if (cmp > 0) t = t.right;//向右找 else return t.setValue(value); } while (t != null); Entry&lt;K,V> e = new Entry&lt;>(key, value, parent);//创建并插入新的entry if (cmp &lt; 0) parent.left = e; else parent.right = e; fixAfterInsertion(e);//调整 size++; return null; } 上述代码的插入部分并不难理解：首先在红黑树上找到合适的位置，然后创建新的entry并插入（当然，新插入的节点一定是树的叶子）。难点是调整函数fixAfterInsertion()，前面已经说过，调整往往需要1.改变某些节点的颜色，2.对某些节点进行旋转。 调整函数fixAfterInsertion()的具体代码如下，其中用到了上文中提到的rotateLeft()和rotateRight()函数。通过代码我们能够看到，情况2其实是落在情况3内的。情况4～情况6跟前三种情况是对称的，因此图解中并没有画出后三种情况，读者可以参考代码自行理解。 //红黑树调整函数fixAfterInsertion() private void fixAfterInsertion(Entry&lt;K,V> x) { x.color = RED; while (x != null &amp;&amp; x != root &amp;&amp; x.parent.color == RED) { if (parentOf(x) == leftOf(parentOf(parentOf(x)))) { Entry&lt;K,V> y = rightOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { setColor(parentOf(x), BLACK); // 情况1 setColor(y, BLACK); // 情况1 setColor(parentOf(parentOf(x)), RED); // 情况1 x = parentOf(parentOf(x)); // 情况1 } else { if (x == rightOf(parentOf(x))) { x = parentOf(x); // 情况2 rotateLeft(x); // 情况2 } setColor(parentOf(x), BLACK); // 情况3 setColor(parentOf(parentOf(x)), RED); // 情况3 rotateRight(parentOf(parentOf(x))); // 情况3 } } else { Entry&lt;K,V> y = leftOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { setColor(parentOf(x), BLACK); // 情况4 setColor(y, BLACK); // 情况4 setColor(parentOf(parentOf(x)), RED); // 情况4 x = parentOf(parentOf(x)); // 情况4 } else { if (x == leftOf(parentOf(x))) { x = parentOf(x); // 情况5 rotateRight(x); // 情况5 } setColor(parentOf(x), BLACK); // 情况6 setColor(parentOf(parentOf(x)), RED); // 情况6 rotateLeft(parentOf(parentOf(x))); // 情况6 } } } root.color = BLACK; } remove()remove(Object key)的作用是删除key值对应的entry，该方法首先通过上文中提到的getEntry(Object key)方法找到key值对应的entry，然后调用deleteEntry(Entry entry)删除对应的entry。由于删除操作会改变红黑树的结构，有可能破坏红黑树的约束条件，因此有可能要进行调整。 getEntry()函数前面已经讲解过，这里重点放deleteEntry()上，该函数删除指定的entry并在红黑树的约束被破坏时进行调用fixAfterDeletion(Entry x)进行调整。 由于红黑树是一棵增强版的二叉查找树，红黑树的删除操作跟普通二叉查找树的删除操作也就非常相似，唯一的区别是红黑树在节点删除之后可能需要进行调整。现在考虑一棵普通二叉查找树的删除过程，可以简单分为两种情况： 删除点p的左右子树都为空，或者只有一棵子树非空。 删除点p的左右子树都非空。 对于上述情况1，处理起来比较简单，直接将p删除（左右子树都为空时），或者用非空子树替代p（只有一棵子树非空时）；对于情况2，可以用p的后继s（树中大于x的最小的那个元素）代替p，然后使用情况1删除s（此时s一定满足情况1.可以画画看）。 基于以上逻辑，红黑树的节点删除函数deleteEntry()代码如下： // 红黑树entry删除函数deleteEntry() private void deleteEntry(Entry&lt;K,V> p) { modCount++; size--; if (p.left != null &amp;&amp; p.right != null) {// 2. 删除点p的左右子树都非空。 Entry&lt;K,V> s = successor(p);// 后继 p.key = s.key; p.value = s.value; p = s; } Entry&lt;K,V> replacement = (p.left != null ? p.left : p.right); if (replacement != null) {// 1. 删除点p只有一棵子树非空。 replacement.parent = p.parent; if (p.parent == null) root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else p.parent.right = replacement; p.left = p.right = p.parent = null; if (p.color == BLACK) fixAfterDeletion(replacement);// 调整 } else if (p.parent == null) { root = null; } else { // 1. 删除点p的左右子树都为空 if (p.color == BLACK) fixAfterDeletion(p);// 调整 if (p.parent != null) { if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; } } } 上述代码中占据大量代码行的，是用来修改父子节点间引用关系的代码，其逻辑并不难理解。下面着重讲解删除后调整函数fixAfterDeletion()。首先请思考一下，删除了哪些点才会导致调整？只有删除点是BLACK的时候，才会触发调整函数，因为删除RED节点不会破坏红黑树的任何约束，而删除BLACK节点会破坏规则4。 跟上文中讲过的fixAfterInsertion()函数一样，这里也要分成若干种情况。记住，无论有多少情况，具体的调整操作只有两种：1.改变某些节点的颜色，2.对某些节点进行旋转。 上述图解的总体思想是：将情况1首先转换成情况2，或者转换成情况3和情况4。当然，该图解并不意味着调整过程一定是从情况1开始。通过后续代码我们还会发现几个有趣的规则：a).如果是由情况1之后紧接着进入的情况2，那么情况2之后一定会退出循环（因为x为红色）；b).一旦进入情况3和情况4，一定会退出循环（因为x为root）。 删除后调整函数fixAfterDeletion()的具体代码如下，其中用到了上文中提到的rotateLeft()和rotateRight()函数。通过代码我们能够看到，情况3其实是落在情况4内的。情况5～情况8跟前四种情况是对称的，因此图解中并没有画出后四种情况，读者可以参考代码自行理解。 private void fixAfterDeletion(Entry&lt;K,V> x) { while (x != root &amp;&amp; colorOf(x) == BLACK) { if (x == leftOf(parentOf(x))) { Entry&lt;K,V> sib = rightOf(parentOf(x)); if (colorOf(sib) == RED) { setColor(sib, BLACK); // 情况1 setColor(parentOf(x), RED); // 情况1 rotateLeft(parentOf(x)); // 情况1 sib = rightOf(parentOf(x)); // 情况1 } if (colorOf(leftOf(sib)) == BLACK &amp;&amp; colorOf(rightOf(sib)) == BLACK) { setColor(sib, RED); // 情况2 x = parentOf(x); // 情况2 } else { if (colorOf(rightOf(sib)) == BLACK) { setColor(leftOf(sib), BLACK); // 情况3 setColor(sib, RED); // 情况3 rotateRight(sib); // 情况3 sib = rightOf(parentOf(x)); // 情况3 } setColor(sib, colorOf(parentOf(x))); // 情况4 setColor(parentOf(x), BLACK); // 情况4 setColor(rightOf(sib), BLACK); // 情况4 rotateLeft(parentOf(x)); // 情况4 x = root; // 情况4 } } else { // 跟前四种情况对称 Entry&lt;K,V> sib = leftOf(parentOf(x)); if (colorOf(sib) == RED) { setColor(sib, BLACK); // 情况5 setColor(parentOf(x), RED); // 情况5 rotateRight(parentOf(x)); // 情况5 sib = leftOf(parentOf(x)); // 情况5 } if (colorOf(rightOf(sib)) == BLACK &amp;&amp; colorOf(leftOf(sib)) == BLACK) { setColor(sib, RED); // 情况6 x = parentOf(x); // 情况6 } else { if (colorOf(leftOf(sib)) == BLACK) { setColor(rightOf(sib), BLACK); // 情况7 setColor(sib, RED); // 情况7 rotateLeft(sib); // 情况7 sib = leftOf(parentOf(x)); // 情况7 } setColor(sib, colorOf(parentOf(x))); // 情况8 setColor(parentOf(x), BLACK); // 情况8 setColor(leftOf(sib), BLACK); // 情况8 rotateRight(parentOf(x)); // 情况8 x = root; // 情况8 } } } setColor(x, BLACK); } TreeSet前面已经说过TreeSet是对TreeMap的简单包装，对TreeSet的函数调用都会转换成合适的TreeMap方法，因此TreeSet的实现非常简单。这里不再赘述。 // TreeSet是对TreeMap的简单包装 public class TreeSet&lt;E> extends AbstractSet&lt;E> implements NavigableSet&lt;E>, Cloneable, java.io.Serializable { ...... private transient NavigableMap&lt;E,Object> m; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); public TreeSet() { this.m = new TreeMap&lt;E,Object>();// TreeSet里面有一个TreeMap } ...... public boolean add(E e) { return m.put(e, PRESENT)==null; } ...... }","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"Java集合- HashSet & HashMap源码解析","slug":"Java集合-HashSet-HashMap源码解析","date":"2019-10-31T01:38:23.000Z","updated":"2020-10-29T15:51:54.479Z","comments":true,"path":"2019/10/31/java-ji-he-hashset-hashmap-yuan-ma-jie-xi/","link":"","permalink":"http://dreamcat.ink/2019/10/31/java-ji-he-hashset-hashmap-yuan-ma-jie-xi/","excerpt":"","text":"引言Java的集合框架，HashSet HashMap源码解析等… 概述之所以把HashSet和HashMap放在一起讲解，是因为二者在Java里有着相同的实现，前者仅仅是对后者做了一层包装，也就是说HashSet\\里面有一个*HashMap*（适配器模式）。因此本文将重点分析HashMap。 HashMap实现了Map接口，即允许放入key为null的元素，也允许插入value为null的元素；除该类未实现同步外，其余跟Hashtable大致相同；跟TreeMap不同，该容器不保证元素顺序，根据需要该容器可能会对元素重新哈希，元素的顺序也会被重新打散，因此不同时间迭代同一个HashMap的顺序可能会不同。 根据对冲突的处理方式不同，哈希表有两种实现方式，一种开放地址方式（Open addressing），另一种是冲突链表方式（Separate chaining with linked lists）。Java *HashMap*采用的是冲突链表方式。 从上图容易看出，如果选择合适的哈希函数，put()和get()方法可以在常数时间内完成。但在对HashMap进行迭代时，需要遍历整个table以及后面跟的冲突链表。因此对于迭代比较频繁的场景，不宜将HashMap的初始大小设的过大。 有两个参数可以影响HashMap的性能：初始容量（inital capacity）和负载系数（load factor）。初始容量指定了初始table的大小，负载系数用来指定自动扩容的临界值。当entry的数量超过capacity*load_factor时，容器将自动扩容并重新哈希。对于插入元素较多的场景，将初始容量设大可以减少重新哈希的次数。 将对象放入到HashMap或HashSet中时，有两个方法需要特别关心：hashCode()和equals()。*hashCode()方法决定了对象会被放到哪个bucket里，当多个对象的哈希值冲突时，equals()方法决定了这些对象是否是“同一个对象”**。所以，如果要将自定义的对象放入到HashMap或HashSet中，需要@Override*hashCode()和equals()方法。 HashMap实现get()get(Object key)方法根据指定的key值返回对应的value，该方法调用了getEntry(Object key)得到相应的entry，然后返回entry.getValue()。因此getEntry()是算法的核心。 算法思想是首先通过hash()函数得到对应bucket的下标，然后依次遍历冲突链表，通过key.equals(k)方法来判断是否是要找的那个entry。 上图中hash(k)&amp;(table.length-1)等价于hash(k)%table.length，原因是HashMap要求table.length必须是2的指数，因此table.length-1就是二进制低位全是1，跟hash(k)相与会将哈希值的高位全抹掉，剩下的就是余数了。 //getEntry()方法 final Entry&lt;K,V> getEntry(Object key) { ...... int hash = (key == null) ? 0 : hash(key); for (Entry&lt;K,V> e = table[hash&amp;(table.length-1)];//得到冲突链表 e != null; e = e.next) {//依次遍历冲突链表中的每个entry Object k; //依据equals()方法判断是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } return null; } put()put(K key, V value)方法是将指定的key, value对添加到map里。该方法首先会对map做一次查找，看是否包含该元组，如果已经包含则直接返回，查找过程类似于getEntry()方法；如果没有找到，则会通过addEntry(int hash, K key, V value, int bucketIndex)方法插入新的entry，插入方式为头插法。 //addEntry() void addEntry(int hash, K key, V value, int bucketIndex) { if ((size >= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length);//自动扩容，并重新哈希 hash = (null != key) ? hash(key) : 0; bucketIndex = hash &amp; (table.length-1);//hash%table.length } //在冲突链表头部插入新的entry Entry&lt;K,V> e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;>(hash, key, value, e); size++; } remove()remove(Object key)的作用是删除key值对应的entry，该方法的具体逻辑是在removeEntryForKey(Object key)里实现的。removeEntryForKey()方法会首先找到key值对应的entry，然后删除该entry（修改链表的相应引用）。查找过程跟getEntry()过程类似。 //removeEntryForKey() final Entry&lt;K,V> removeEntryForKey(Object key) { ...... int hash = (key == null) ? 0 : hash(key); int i = indexFor(hash, table.length);//hash&amp;(table.length-1) Entry&lt;K,V> prev = table[i];//得到冲突链表 Entry&lt;K,V> e = prev; while (e != null) {//遍历冲突链表 Entry&lt;K,V> next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) {//找到要删除的entry modCount++; size--; if (prev == e) table[i] = next;//删除的是冲突链表的第一个entry else prev.next = next; return e; } prev = e; e = next; } return e; } HashSet前面已经说过HashSet是对HashMap的简单包装，对HashSet的函数调用都会转换成合适的HashMap方法，因此HashSet的实现非常简单，只有不到300行代码。这里不再赘述。 //HashSet是对HashMap的简单包装 public class HashSet&lt;E> { ...... private transient HashMap&lt;E,Object> map;//HashSet里面有一个HashMap // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); public HashSet() { map = new HashMap&lt;>(); } ...... public boolean add(E e) {//简单的方法转换 return map.put(e, PRESENT)==null; } ...... }","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"Java集合- PriorityQueue源码解析","slug":"Java集合-PriorityQueue源码解析","date":"2019-10-30T08:06:18.000Z","updated":"2020-10-29T15:51:40.841Z","comments":true,"path":"2019/10/30/java-ji-he-priorityqueue-yuan-ma-jie-xi/","link":"","permalink":"http://dreamcat.ink/2019/10/30/java-ji-he-priorityqueue-yuan-ma-jie-xi/","excerpt":"","text":"引言Java的集合框架，PriorityQueue源码解析等… 概述前面以Java ArrayDeque为例讲解了Stack和Queue，其实还有一种特殊的队列叫做PriorityQueue，即优先队列。优先队列的作用是能保证每次取出的元素都是队列中权值最小的（Java的优先队列每次取最小元素，C++的优先队列每次取最大元素）。这里牵涉到了大小关系，元素大小的评判可以通过元素本身的自然顺序（*natural ordering*），也可以通过构造时传入的比较器（Comparator，类似于C++的仿函数）。 Java中PriorityQueue实现了Queue接口，不允许放入null元素；其通过堆实现，具体说是通过完全二叉树（complete binary tree）实现的小顶堆（任意一个非叶子节点的权值，都不大于其左右子节点的权值），也就意味着可以通过数组来作为PriorityQueue的底层实现。 上图中我们给每个元素按照层序遍历的方式进行了编号，如果你足够细心，会发现父节点和子节点的编号是有联系的，更确切的说父子节点的编号之间有如下关系： leftNo = parentNo*2+1 rightNo = parentNo*2+2 parentNo = (nodeNo-1)/2通过上述三个公式，可以轻易计算出某个节点的父节点以及子节点的下标。这也就是为什么可以直接用数组来存储堆的原因。 PriorityQueue的peek()和element操作是常数时间，add(), offer(), 无参数的remove()以及poll()方法的时间复杂度都是log(N)。 方法解析add()和offer()add(E e)和offer(E e)的语义相同，都是向优先队列中插入元素，只是Queue接口规定二者对插入失败时的处理不同，前者在插入失败时抛出异常，后则则会返回false。对于PriorityQueue这两个方法其实没什么差别。 //offer(E e) public boolean offer(E e) { if (e == null)//不允许放入null元素 throw new NullPointerException(); modCount++; int i = size; if (i >= queue.length) grow(i + 1);//自动扩容 size = i + 1; if (i == 0)//队列原来为空，这是插入的第一个元素 queue[0] = e; else siftUp(i, e);//调整 return true; } 上述代码中，扩容函数grow()类似于ArrayList里的grow()函数，就是再申请一个更大的数组，并将原数组的元素复制过去，这里不再赘述。需要注意的是siftUp(int k, E x)方法，该方法用于插入元素x并维持堆的特性。 //siftUp() private void siftUp(int k, E x) { while (k > 0) { int parent = (k - 1) >>> 1;//parentNo = (nodeNo-1)/2 Object e = queue[parent]; if (comparator.compare(x, (E) e) >= 0)//调用比较器的比较方法 break; queue[k] = e; k = parent; } queue[k] = x; } 新加入的元素x可能会破坏小顶堆的性质，因此需要进行调整。调整的过程为：从k指定的位置开始，将x逐层与当前点的parent进行比较并交换，直到满足x &gt;= queue[parent]为止。注意这里的比较可以是元素的自然顺序，也可以是依靠比较器的顺序。 element()和peek()element()和peek()的语义完全相同，都是获取但不删除队首元素，也就是队列中权值最小的那个元素，二者唯一的区别是当方法失败时前者抛出异常，后者返回null。根据小顶堆的性质，堆顶那个元素就是全局最小的那个；由于堆用数组表示，根据下标关系，0下标处的那个元素既是堆顶元素。所以直接返回数组0下标处的那个元素即可。 //peek() public E peek() { if (size == 0) return null; return (E) queue[0];//0下标处的那个元素就是最小的那个 } remove()和poll()remove()和poll()方法的语义也完全相同，都是获取并删除队首元素，区别是当方法失败时前者抛出异常，后者返回null。由于删除操作会改变队列的结构，为维护小顶堆的性质，需要进行必要的调整。 public E poll() { if (size == 0) return null; int s = --size; modCount++; E result = (E) queue[0];//0下标处的那个元素就是最小的那个 E x = (E) queue[s]; queue[s] = null; if (s != 0) siftDown(0, x);//调整 return result; } 上述代码首先记录0下标处的元素，并用最后一个元素替换0下标位置的元素，之后调用siftDown()方法对堆进行调整，最后返回原来0下标处的那个元素（也就是最小的那个元素）。重点是siftDown(int k, E x)方法，该方法的作用是从k指定的位置开始，将x逐层向下与当前点的左右孩子中较小的那个交换，直到x小于或等于左右孩子中的任何一个为止。 //siftDown() private void siftDown(int k, E x) { int half = size >>> 1; while (k &lt; half) { //首先找到左右孩子中较小的那个，记录到c里，并用child记录其下标 int child = (k &lt;&lt; 1) + 1;//leftNo = parentNo*2+1 Object c = queue[child]; int right = child + 1; if (right &lt; size &amp;&amp; comparator.compare((E) c, (E) queue[right]) > 0) c = queue[child = right]; if (comparator.compare(x, (E) c) &lt;= 0) break; queue[k] = c;//然后用c取代原来的值 k = child; } queue[k] = x; } remove(Object o)remove(Object o)方法用于删除队列中跟o相等的某一个元素（如果有多个相等，只删除一个），该方法不是Queue接口内的方法，而是Collection接口的方法。由于删除操作会改变队列结构，所以要进行调整；又由于删除元素的位置可能是任意的，所以调整过程比其它函数稍加繁琐。具体来说，remove(Object o)可以分为2种情况：1. 删除的是最后一个元素。直接删除即可，不需要调整。2. 删除的不是最后一个元素，从删除点开始以最后一个元素为参照调用一次siftDown()即可。此处不再赘述。 //remove(Object o) public boolean remove(Object o) { //通过遍历数组的方式找到第一个满足o.equals(queue[i])元素的下标 int i = indexOf(o); if (i == -1) return false; int s = --size; if (s == i) //情况1 queue[i] = null; else { E moved = (E) queue[s]; queue[s] = null; siftDown(i, moved);//情况2 ...... } return true; }","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"Java集合-Stack & Queue源码解析","slug":"Java集合-Stack-Queue源码解析","date":"2019-10-30T02:15:02.000Z","updated":"2020-10-29T15:24:28.142Z","comments":true,"path":"2019/10/30/java-ji-he-stack-queue-yuan-ma-jie-xi/","link":"","permalink":"http://dreamcat.ink/2019/10/30/java-ji-he-stack-queue-yuan-ma-jie-xi/","excerpt":"引言Java的集合框架，Stack &amp; Queue源码解析等…","text":"引言Java的集合框架，Stack &amp; Queue源码解析等… Stack &amp; Queue概述Java里有一个叫做Stack的类，却没有叫做Queue的类（它是个接口名字）。当需要使用栈时，Java已不推荐使用Stack，而是推荐使用更高效的ArrayDeque；既然Queue只是一个接口，当需要使用队列时也就首选ArrayDeque了（次选是LinkedList）。 QueueQueue接口继承自Collection接口，除了最基本的Collection的方法之外，它还支持额外的insertion, extraction和inspection操作。这里有两组格式，共6个方法，一组是抛出异常的实现；另外一组是返回值的实现（没有则返回null）。 Throws exception Returns special value Insert add(e) offer(e) Remove remove() poll() Examine element() peek() DequeDeque是”double ended queue”, 表示双向的队列，英文读作”deck”. Deque 继承自 Queue接口，除了支持Queue的方法之外，还支持insert, remove和examine操作，由于Deque是双向的，所以可以对队列的头和尾都进行操作，它同时也支持两组格式，一组是抛出异常的实现；另外一组是返回值的实现（没有则返回null）。 Deque的含义是“double ended queue”，即双端队列，它既可以当作栈使用，也可以当作队列使用。下表列出了Deque与Queue相对应的接口： 当把Deque当做FIFO的queue来使用时，元素是从deque的尾部添加，从头部进行删除的； 所以deque的部分方法是和queue是等同的。具体如下： Queue Method Equivalent Deque Method 说明 add(e) addLast(e) 向队尾插入元素，失败则抛出异常 offer(e) offerLast(e) 向队尾插入元素，失败则返回false remove() removeFirst() 获取并删除队首元素，失败则抛出异常 poll() pollFirst() 获取并删除队首元素，失败则返回null element() getFirst() 获取但不删除队首元素，失败则抛出异常 peek() peekFirst() 获取但不删除队首元素，失败则返回null 下表列出了Deque与Stack对应的接口： Stack Method Equivalent Deque Method 说明 push(e) addFirst(e) 向栈顶插入元素，失败则抛出异常 无 offerFirst(e) 向栈顶插入元素，失败则返回false pop() removeFirst() 获取并删除栈顶元素，失败则抛出异常 无 pollFirst() 获取并删除栈顶元素，失败则返回null peek() peekFirst() 获取但不删除栈顶元素，失败则抛出异常 无 peekFirst() 获取但不删除栈顶元素，失败则返回null ArrayDeque和LinkedList是Deque的两个通用实现。 由于官方更推荐使用AarryDeque用作栈和队列。从名字可以看出ArrayDeque底层通过数组实现，为了满足可以同时在数组两端插入或删除元素的需求，该数组还必须是循环的，即循环数组（circular array），也就是说数组的任何一点都可能被看作起点或者终点。ArrayDeque是非线程安全的（not thread-safe），当多个线程同时使用的时候，需要程序员手动同步；另外，该容器不允许放入null元素。 上图中我们看到，head指向首端第一个有效元素，tail指向尾端第一个可以插入元素的空位。因为是循环数组，所以head不一定总等于0，tail也不一定总是比head大。 方法解析addFirst()addFirst(E e)的作用是在Deque的首端插入元素，也就是在head的前面插入元素，在空间足够且下标没有越界的情况下，只需要将elements[--head] = e即可。 实际需要考虑：1.空间是否够用，以及2.下标是否越界的问题。上图中，如果head为0之后接着调用addFirst()，虽然空余空间还够用，但head为-1，下标越界了。下列代码很好的解决了这两个问题。 //addFirst(E e) public void addFirst(E e) { if (e == null)//不允许放入null throw new NullPointerException(); elements[head = (head - 1) &amp; (elements.length - 1)] = e;//2.下标是否越界 if (head == tail)//1.空间是否够用 doubleCapacity();//扩容 } 上述代码我们看到，空间问题是在插入之后解决的，因为tail总是指向下一个可插入的空位，也就意味着elements数组至少有一个空位，所以插入元素的时候不用考虑空间问题。 下标越界的处理解决起来非常简单，head = (head - 1) &amp; (elements.length - 1)就可以了，这段代码相当于取余，同时解决了head为负值的情况。因为elements.length必需是2的指数倍，elements - 1就是二进制低位全1，跟head - 1相与之后就起到了取模的作用，如果head - 1为负数（其实只可能是-1），则相当于对其取相对于elements.length的补码。 下面再说说扩容函数doubleCapacity()，其逻辑是申请一个更大的数组（原数组的两倍），然后将原数组复制过去。过程如下图所示： 图中我们看到，复制分两次进行，第一次复制head右边的元素，第二次复制head左边的元素。 //doubleCapacity() private void doubleCapacity() { assert head == tail; int p = head; int n = elements.length; int r = n - p; // head右边元素的个数 int newCapacity = n &lt;&lt; 1;//原空间的2倍 if (newCapacity &lt; 0) throw new IllegalStateException(\"Sorry, deque too big\"); Object[] a = new Object[newCapacity]; System.arraycopy(elements, p, a, 0, r);//复制右半部分，对应上图中绿色部分 System.arraycopy(elements, 0, a, r, p);//复制左半部分，对应上图中灰色部分 elements = (E[])a; head = 0; tail = n; } addLast()addLast(E e)的作用是在Deque的尾端插入元素，也就是在tail的位置插入元素，由于tail总是指向下一个可以插入的空位，因此只需要elements[tail] = e;即可。插入完成后再检查空间，如果空间已经用光，则调用doubleCapacity()进行扩容。 public void addLast(E e) { if (e == null)//不允许放入null throw new NullPointerException(); elements[tail] = e;//赋值 if ( (tail = (tail + 1) &amp; (elements.length - 1)) == head)//下标越界处理 doubleCapacity();//扩容 } pollFirst()pollFirst()的作用是删除并返回Deque首端元素，也即是head位置处的元素。如果容器不空，只需要直接返回elements[head]即可，当然还需要处理下标的问题。由于ArrayDeque中不允许放入null，当elements[head] == null时，意味着容器为空。 public E pollFirst() { E result = elements[head]; if (result == null)//null值意味着deque为空 return null; elements[h] = null;//let GC work head = (head + 1) &amp; (elements.length - 1);//下标越界处理 return result; } pollLast()pollLast()的作用是删除并返回Deque尾端元素，也即是tail位置前面的那个元素。 public E pollLast() { int t = (tail - 1) &amp; (elements.length - 1);//tail的上一个位置是最后一个元素 E result = elements[t]; if (result == null)//null值意味着deque为空 return null; elements[t] = null;//let GC work tail = t; return result; } peekFirst()peekFirst()的作用是返回但不删除Deque首端元素，也即是head位置处的元素，直接返回elements[head]即可。 public E peekFirst() { return elements[head]; // elements[head] is null if deque empty } peekLast()peekLast()的作用是返回但不删除Deque尾端元素，也即是tail位置前面的那个元素。 public E peekLast() { return elements[(tail - 1) &amp; (elements.length - 1)]; }","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"Java集合-LinkedList源码解析","slug":"Java集合-LinkedList源码解析","date":"2019-10-30T01:00:27.000Z","updated":"2020-10-29T15:38:58.045Z","comments":true,"path":"2019/10/30/java-ji-he-linkedlist-yuan-ma-jie-xi/","link":"","permalink":"http://dreamcat.ink/2019/10/30/java-ji-he-linkedlist-yuan-ma-jie-xi/","excerpt":"","text":"引言Java的集合框架，LinkedList源码解析等… 概述LinkedList同时实现了List接口和Deque接口，也就是说它既可以看作一个顺序容器，又可以看作一个队列（Queue），同时又可以看作一个栈（Stack）。这样看来，LinkedList简直就是个全能冠军。当你需要使用栈或者队列时，可以考虑使用LinkedList，一方面是因为Java官方已经声明不建议使用Stack类，更遗憾的是，Java里根本没有一个叫做Queue的类（它是个接口名字）。关于栈或队列，现在的首选是ArrayDeque，它有着比LinkedList（当作栈或队列使用时）有着更好的性能。 LinkedList的实现方式决定了所有跟下标相关的操作都是线性时间，而在首段或者末尾删除元素只需要常数时间。为追求效率LinkedList没有实现同步（synchronized），如果需要多个线程并发访问，可以先采用Collections.synchronizedList()方法对其进行包装。 LinkedLists实现底层数据结构LinkedList底层通过双向链表实现，本节将着重讲解插入和删除元素时双向链表的维护过程，也即是之间解跟List接口相关的函数，而将Queue和Stack以及Deque相关的知识放在下一节讲。双向链表的每个节点用内部类Node表示。LinkedList通过first和last引用分别指向链表的第一个和最后一个元素。注意这里没有所谓的哑元，当链表为空的时候first和last都指向null。 transient int size = 0; transient Node&lt;E> first; transient Node&lt;E> last; // Node是私有的内部类 private static class Node&lt;E> { E item; Node&lt;E> next; Node&lt;E> prev; Node(Node&lt;E> prev, E element, Node&lt;E> next) { this.item = element; this.next = next; this.prev = prev; } } 构造函数 public LinkedList() { } public LinkedList(Collection&lt;? extends E> c) { this(); addAll(c); } getFirst()，getLast() public E getFirst() { final Node&lt;E> f = first; if (f == null) throw new NoSuchElementException(); return f.item; } public E getLast() { final Node&lt;E> l = last; if (l == null) throw new NoSuchElementException(); return l.item; } removeFirst()，removeLast()，remove(e)，remove(index)remove()方法也有两个版本，一个是删除跟指定元素相等的第一个元素remove(Object o)，另一个是删除指定下标处的元素remove(int index)。 删除元素 - 指的是删除第一次出现的这个元素, 如果没有这个元素，则返回false；判读的依据是equals方法， 如果equals，则直接unlink这个node；由于LinkedList可存放null元素，故也可以删除第一次出现null的元素； public boolean remove(Object o) { if (o == null) { for (Node&lt;E> x = first; x != null; x = x.next) { if (x.item == null) { unlink(x); return true; } } } else { for (Node&lt;E> x = first; x != null; x = x.next) { if (o.equals(x.item)) { unlink(x); return true; } } } return false; } E unlink(Node&lt;E> x) { // assert x != null; final E element = x.item; final Node&lt;E> next = x.next; final Node&lt;E> prev = x.prev; if (prev == null) {// 第一个元素 first = next; } else { prev.next = next; x.prev = null; } if (next == null) {// 最后一个元素 last = prev; } else { next.prev = prev; x.next = null; } x.item = null; // GC size--; modCount++; return element; } remove(int index)使用的是下标计数， 只需要判断该index是否有元素即可，如果有则直接unlink这个node。 public E remove(int index) { checkElementIndex(index); return unlink(node(index)); } 删除head元素 public E removeFirst() { final Node&lt;E> f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f); } private E unlinkFirst(Node&lt;E> f) { // assert f == first &amp;&amp; f != null; final E element = f.item; final Node&lt;E> next = f.next; f.item = null; f.next = null; // help GC first = next; if (next == null) last = null; else next.prev = null; size--; modCount++; return element; } 删除last元素 public E removeLast() { final Node&lt;E> l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l); } private E unlinkLast(Node&lt;E> l) { // assert l == last &amp;&amp; l != null; final E element = l.item; final Node&lt;E> prev = l.prev; l.item = null; l.prev = null; // help GC last = prev; if (prev == null) first = null; else prev.next = null; size--; modCount++; return element; } add()add()\\方法有两个版本，一个是add(E e)，该方法在*LinkedList*的末尾插入元素，因为有last指向链表末尾，在末尾插入元素的花费是常数时间。只需要简单修改几个相关引用即可；另一个是add(int index, E element)，该方法是在指定下表处插入元素，需要先通过线性查找找到具体位置，然后修改相关引用完成插入操作。 public boolean add(E e) { linkLast(e); return true; } void linkLast(E e) { final Node&lt;E> l = last; final Node&lt;E> newNode = new Node&lt;>(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++; } add(int index, E element), 当index==size时，等同于add(E e); 如果不是，则分两步：1.先根据index找到要插入的位置,即node(index)方法；2.修改引用，完成插入操作。 public void add(int index, E element) { checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index)); } 上面代码中的node(int index)函数有一点小小的trick，因为链表双向的，可以从开始往后找，也可以从结尾往前找，具体朝那个方向找取决于条件index &lt; (size &gt;&gt; 1)，也即是index是靠近前端还是后端。从这里也可以看出，linkedList通过index检索元素的效率没有arrayList高。 Node&lt;E> node(int index) { // assert isElementIndex(index); if (index &lt; (size >> 1)) { Node&lt;E> x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; } else { Node&lt;E> x = last; for (int i = size - 1; i > index; i--) x = x.prev; return x; } } addAll()addAll(index, c) 实现方式并不是直接调用add(index,e)来实现，主要是因为效率的问题，另一个是fail-fast中modCount只会增加1次； public boolean addAll(Collection&lt;? extends E> c) { return addAll(size, c); } public boolean addAll(int index, Collection&lt;? extends E> c) { checkPositionIndex(index); Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; Node&lt;E> pred, succ; if (index == size) { succ = null; pred = last; } else { succ = node(index); pred = succ.prev; } for (Object o : a) { @SuppressWarnings(\"unchecked\") E e = (E) o; Node&lt;E> newNode = new Node&lt;>(pred, e, null); if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; } if (succ == null) { last = pred; } else { pred.next = succ; succ.prev = pred; } size += numNew; modCount++; return true; } clear()为了让GC更快可以回收放置的元素，需要将node之间的引用关系赋空。 public void clear() { // Clearing all of the links between nodes is \"unnecessary\", but: // - helps a generational GC if the discarded nodes inhabit // more than one generation // - is sure to free memory even if there is a reachable Iterator for (Node&lt;E> x = first; x != null; ) { Node&lt;E> next = x.next; x.item = null; x.next = null; x.prev = null; x = next; } first = last = null; size = 0; modCount++; } Positional Access 方法通过index获取元素 public E get(int index) { checkElementIndex(index); return node(index).item; } 将某个位置的元素重新赋值 public E set(int index, E element) { checkElementIndex(index); Node&lt;E> x = node(index); E oldVal = x.item; x.item = element; return oldVal; } 将元素插入到指定index位置 public void add(int index, E element) { checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index)); } 删除指定位置的元素 public E remove(int index) { checkElementIndex(index); return unlink(node(index)); } 其他位置的方法 private boolean isElementIndex(int index) { return index >= 0 &amp;&amp; index &lt; size; } private boolean isPositionIndex(int index) { return index >= 0 &amp;&amp; index &lt;= size; } private String outOfBoundsMsg(int index) { return \"Index: \"+index+\", Size: \"+size; } private void checkElementIndex(int index) { if (!isElementIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } private void checkPositionIndex(int index) { if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } 查找操作查找操作的本质是查找元素的下标: 查找第一次出现的index, 如果找不到返回-1； public int indexOf(Object o) { int index = 0; if (o == null) { for (Node&lt;E> x = first; x != null; x = x.next) { if (x.item == null) return index; index++; } } else { for (Node&lt;E> x = first; x != null; x = x.next) { if (o.equals(x.item)) return index; index++; } } return -1; } 查找最后一次出现的index, 如果找不到返回-1； public int lastIndexOf(Object o) { int index = size; if (o == null) { for (Node&lt;E> x = last; x != null; x = x.prev) { index--; if (x.item == null) return index; } } else { for (Node&lt;E> x = last; x != null; x = x.prev) { index--; if (o.equals(x.item)) return index; } } return -1; } Queue方法 public E peek() { final Node&lt;E> f = first; return (f == null) ? null : f.item; } public E element() { return getFirst(); } public E poll() { final Node&lt;E> f = first; return (f == null) ? null : unlinkFirst(f); } public E remove() { return removeFirst(); } public boolean offer(E e) { return add(e); } Deque方法 public boolean offerFirst(E e) { addFirst(e); return true; } public boolean offerLast(E e) { addLast(e); return true; } public E peekFirst() { final Node&lt;E> f = first; return (f == null) ? null : f.item; } public E peekLast() { final Node&lt;E> l = last; return (l == null) ? null : l.item; } public E pollFirst() { final Node&lt;E> f = first; return (f == null) ? null : unlinkFirst(f); } public E pollLast() { final Node&lt;E> l = last; return (l == null) ? null : unlinkLast(l); } public void push(E e) { addFirst(e); } public E pop() { return removeFirst(); } public boolean removeFirstOccurrence(Object o) { return remove(o); } public boolean removeLastOccurrence(Object o) { if (o == null) { for (Node&lt;E> x = last; x != null; x = x.prev) { if (x.item == null) { unlink(x); return true; } } } else { for (Node&lt;E> x = last; x != null; x = x.prev) { if (o.equals(x.item)) { unlink(x); return true; } } } return false; }","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"Java集合-ArrayList源码解析","slug":"Java集合-ArrayList源码解析","date":"2019-10-29T15:51:46.000Z","updated":"2020-10-29T15:44:36.368Z","comments":true,"path":"2019/10/29/java-ji-he-arraylist-yuan-ma-jie-xi/","link":"","permalink":"http://dreamcat.ink/2019/10/29/java-ji-he-arraylist-yuan-ma-jie-xi/","excerpt":"","text":"引言Java的集合框架，ArrayList源码解析等… 概述 ArrayList实现了List接口，是顺序容器，即元素存放的数据与放进去的顺序相同，允许放入null元素，底层通过数组实现。 除该类未实现同步外，其余跟Vector大致相同。 每个ArrayList都有一个容量（capacity），表示底层数组的实际大小，容器内存储元素的个数不能多于当前容量。 当向容器中添加元素时，如果容量不足，容器会自动增大底层数组的大小。 前面已经提过，Java泛型只是编译器提供的语法糖，所以这里的数组是一个Object数组，以便能够容纳任何类型的对象。 size(), isEmpty(), get(), set()方法均能在常数时间内完成，add()方法的时间开销跟插入位置有关，addAll()方法的时间开销跟添加元素的个数成正比。其余方法大都是线性时间。 为追求效率，ArrayList没有实现同步（synchronized），如果需要多个线程并发访问，用户可以手动同步，也可使用Vector替代。 ArrayList的实现底层数据结构transient Object[] elementData; // Object 数组 private int size; // 大小 构造函数 // 参数为容量的构造参数 public ArrayList(int initialCapacity) { if (initialCapacity > 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; // 默认 } else { throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); } } // 无参的构造参数 public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; // 默认容量 } public ArrayList(Collection&lt;? extends E> c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; } } 自动扩容 每当向数组中添加元素时，都要去检查添加后元素的个数是否会超出当前数组的长度，如果超出，数组将会进行扩容，以满足添加数据的需求。 数组扩容通过一个公开的方法ensureCapacity(int minCapacity)来实现。在实际添加大量元素前，我也可以使用ensureCapacity来手动增加ArrayList实例的容量，以减少递增式再分配的数量。 数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍。 这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。 当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。 或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。 public void ensureCapacity(int minCapacity) { int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity > minExpand) { ensureExplicitCapacity(minCapacity); } } private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length > 0) grow(minCapacity); } private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity >> 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE > 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } private static int hugeCapacity(int minCapacity) { if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity > MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } add()，addAll() 跟C++ 的vector不同，ArrayList没有push_back()方法，对应的方法是add(E e)，ArrayList也没有insert()方法，对应的方法是add(int index, E e)。 这两个方法都是向容器中添加新元素，这可能会导致capacity不足，因此在添加元素之前，都需要进行剩余空间检查，如果需要则自动扩容。扩容操作最终是通过grow()方法完成的。 public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } add(int index, E e)需要先对元素进行移动，然后完成插入操作，也就意味着该方法有着线性的时间复杂度。 addAll()方法能够一次添加多个元素，根据位置不同也有两个把本，一个是在末尾添加的addAll(Collection&lt;? extends E&gt; c)方法，一个是从指定位置开始插入的addAll(int index, Collection&lt;? extends E&gt; c)方法。跟add()方法类似，在插入之前也需要进行空间检查，如果需要则自动扩容；如果从指定位置插入，也会存在移动元素的情况。 addAll()的时间复杂度不仅跟插入元素的多少有关，也跟插入的位置相关。 public boolean addAll(Collection&lt;? extends E> c) { Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; } public boolean addAll(int index, Collection&lt;? extends E> c) { rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved > 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; } set()既然底层是一个数组ArrayList的set()方法也就变得非常简单，直接对数组的指定位置赋值即可。 public E set(int index, E element) { rangeCheck(index);//下标越界检查 E oldValue = elementData(index); elementData[index] = element;//赋值到指定位置，复制的仅仅是引用 return oldValue; } get()get()方法同样很简单，唯一要注意的是由于底层数组是Object[]，得到元素后需要进行类型转换。 public E get(int index) { rangeCheck(index); return (E) elementData[index];//注意类型转换 } remove()remove()方法也有两个版本，一个是remove(int index)删除指定位置的元素，另一个是remove(Object o)删除第一个满足o.equals(elementData[index])的元素。删除操作是add()操作的逆过程，需要将删除点之后的元素向前移动一个位置。需要注意的是为了让GC起作用，必须显式的为最后一个位置赋null值。 public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved > 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; //清除该位置的引用，让GC起作用 return oldValue; } 关于Java GC这里需要特别说明一下，有了垃圾收集器并不意味着一定不会有内存泄漏。对象能否被GC的依据是是否还有引用指向它，上面代码中如果不手动赋null值，除非对应的位置被其他元素覆盖，否则原来的对象就一直不会被回收。 trimToSize()ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。代码如下： public void trimToSize() { modCount++; if (size &lt; elementData.length) { elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); } } indexOf(), lastIndexOf() public int indexOf(Object o) { if (o == null) { for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; } else { for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; } return -1; } public int lastIndexOf(Object o) { if (o == null) { for (int i = size-1; i >= 0; i--) if (elementData[i]==null) return i; } else { for (int i = size-1; i >= 0; i--) if (o.equals(elementData[i])) return i; } return -1; } Fail-Fast机制：ArrayList也采用了快速失败的机制，通过记录modCount参数来实现。在面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险。","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"}]},{"title":"Spring-Springboot整合Shiro","slug":"Spring-Springboot整合Shiro","date":"2019-09-12T08:06:15.000Z","updated":"2020-10-29T15:48:28.716Z","comments":true,"path":"2019/09/12/spring-springboot-zheng-he-shiro/","link":"","permalink":"http://dreamcat.ink/2019/09/12/spring-springboot-zheng-he-shiro/","excerpt":"","text":"​ 关于Shiro是什么,我就不解释了,互联网看一下,我这边直接演示 pom部分依赖 &lt;!-- https://mvnrepository.com/artifact/org.apache.shiro/shiro-spring --> &lt;dependency> &lt;groupId>org.apache.shiro&lt;/groupId> &lt;artifactId>shiro-spring&lt;/artifactId> &lt;version>1.4.1&lt;/version> &lt;/dependency> 核心部分CustomRealm和ShiroConfig ShiroConfig@Configuration public class ShiroConfig { @Bean(name = \"shiroFilter\") public ShiroFilterFactoryBean shiroFilter(SecurityManager securityManager) { ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); shiroFilterFactoryBean.setSecurityManager(securityManager); shiroFilterFactoryBean.setLoginUrl(\"/login\"); shiroFilterFactoryBean.setUnauthorizedUrl(\"/notRole\"); Map&lt;String, String> filterChainDefinitionMap = new LinkedHashMap&lt;>(); // &lt;!-- authc:所有url都必须认证通过才可以访问; anon:所有url都都可以匿名访问--> filterChainDefinitionMap.put(\"/webjars/**\", \"anon\"); filterChainDefinitionMap.put(\"/login\", \"anon\"); filterChainDefinitionMap.put(\"/\", \"anon\"); filterChainDefinitionMap.put(\"/front/**\", \"anon\"); filterChainDefinitionMap.put(\"/api/**\", \"anon\"); filterChainDefinitionMap.put(\"/admin/**\", \"authc\"); filterChainDefinitionMap.put(\"/user/**\", \"authc\"); //主要这行代码必须放在所有权限设置的最后，不然会导致所有 url 都被拦截 剩余的都需要认证 filterChainDefinitionMap.put(\"/**\", \"authc\"); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return shiroFilterFactoryBean; } @Bean public SecurityManager securityManager() { DefaultWebSecurityManager defaultSecurityManager = new DefaultWebSecurityManager(); defaultSecurityManager.setRealm(customRealm()); return defaultSecurityManager; } @Bean public CustomRealm customRealm() { CustomRealm customRealm = new CustomRealm(); // 告诉realm,使用credentialsMatcher加密算法类来验证密文 customRealm.setCredentialsMatcher(hashedCredentialsMatcher()); customRealm.setCachingEnabled(false); return customRealm; } @Bean public LifecycleBeanPostProcessor lifecycleBeanPostProcessor() { return new LifecycleBeanPostProcessor(); } /** * * * 开启Shiro的注解(如@RequiresRoles,@RequiresPermissions),需借助SpringAOP扫描使用Shiro注解的类,并在必要时进行安全逻辑验证 * * * 配置以下两个bean(DefaultAdvisorAutoProxyCreator(可选)和AuthorizationAttributeSourceAdvisor)即可实现此功能 * * @return */ @Bean @DependsOn({\"lifecycleBeanPostProcessor\"}) public DefaultAdvisorAutoProxyCreator advisorAutoProxyCreator() { DefaultAdvisorAutoProxyCreator advisorAutoProxyCreator = new DefaultAdvisorAutoProxyCreator(); advisorAutoProxyCreator.setProxyTargetClass(true); return advisorAutoProxyCreator; } @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor() { AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor = new AuthorizationAttributeSourceAdvisor(); authorizationAttributeSourceAdvisor.setSecurityManager(securityManager()); return authorizationAttributeSourceAdvisor; } @Bean(name = \"credentialsMatcher\") public HashedCredentialsMatcher hashedCredentialsMatcher() { HashedCredentialsMatcher hashedCredentialsMatcher = new HashedCredentialsMatcher(); // 散列算法:这里使用MD5算法; hashedCredentialsMatcher.setHashAlgorithmName(\"md5\"); // 散列的次数，比如散列两次，相当于 md5(md5(\"\")); hashedCredentialsMatcher.setHashIterations(2); // storedCredentialsHexEncoded默认是true，此时用的是密码加密用的是Hex编码；false时用Base64编码 hashedCredentialsMatcher.setStoredCredentialsHexEncoded(true); return hashedCredentialsMatcher; } } CustomRealmpublic class CustomRealm extends AuthorizingRealm { @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) { String username = (String) SecurityUtils.getSubject().getPrincipal(); SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); Set&lt;String> stringSet = new HashSet&lt;>(); stringSet.add(\"user:show\"); stringSet.add(\"user:admin\"); info.setStringPermissions(stringSet); return info; } @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException { System.out.println(\"-------身份认证方法--------\"); String userName = (String) authenticationToken.getPrincipal(); String userPwd = new String((char[]) authenticationToken.getCredentials()); //根据用户名从数据库获取密码 若是加密的,去加密的 String password = \"123\"; // 暂时写死了 // 前端的userPwd可以传加密后的 if (userName == null) { throw new AccountException(\"用户名不正确\"); } else if (!userPwd.equals(password)) { throw new AccountException(\"密码不正确\"); } // 若加密,用这行代码 // return new SimpleAuthenticationInfo(userName, password, ByteSource.Util.bytes(userName+\"salt\"), getName()); return new SimpleAuthenticationInfo(userName, password, getName()); } } md5工具public class Util { public static String MD5Pwd(String username, String pwd) { // 加密算法MD5 // salt盐 username + salt // 迭代次数 String md5Pwd = new SimpleHash(\"MD5\", pwd, ByteSource.Util.bytes(username + \"salt\"), 2).toHex(); return md5Pwd; } } 简单测试@RestController public class HomeIndexController { @GetMapping(value = \"/login\") public String defaultLogin() { return \"请登录\"; } @PostMapping(value = \"/login\") public String login(@RequestParam(\"username\") String username, @RequestParam(\"password\") String password) { // 从SecurityUtils里边创建一个 subject Subject subject = SecurityUtils.getSubject(); // 在认证提交前准备 token（令牌） UsernamePasswordToken token = new UsernamePasswordToken(username, password); // 执行认证登陆 try { subject.login(token); } catch (UnknownAccountException uae) { return \"未知账户\"; } catch (IncorrectCredentialsException ice) { return \"密码不正确\"; } catch (LockedAccountException lae) { return \"账户已锁定\"; } catch (ExcessiveAttemptsException eae) { return \"用户名或密码错误次数过多\"; } catch (AuthenticationException ae) { return \"用户名或密码不正确！\"; } if (subject.isAuthenticated()) { return \"登录成功\"; } else { token.clear(); return \"登录失败\"; } } }","categories":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"}]},{"title":"Spring-Vue和Shiro的md5加密","slug":"Spring-Vue和Shiro的md5加密","date":"2019-09-12T06:31:43.000Z","updated":"2020-10-29T15:46:49.687Z","comments":true,"path":"2019/09/12/spring-vue-he-shiro-de-md5-jia-mi/","link":"","permalink":"http://dreamcat.ink/2019/09/12/spring-vue-he-shiro-de-md5-jia-mi/","excerpt":"","text":"​ 因为用Vue做为前端开发，后端采用Springboot和Shiro的整合，当遇到密码加密的情况时候，想让前端直接发送md5之后的字符串。所以，演示前端和后端在shiro的md5上面的对应。 说明 这里只是掩饰前后端md5对应，所以关于springboot和shiro整合，之后会单独写一篇文章 Vue简单实用 cnpm i js-md5 -S import md5 from 'js-md5'; var username = 'mf'; var salt = username + 'salt'; var password = '123'; var a = md5(salt + password, 32) // 32 是输出的位数 Shiro使用SimpleHash public static String MD5Pwd(String username, String pwd) { // 加密算法MD5 // salt盐 username + salt // 迭代次数 String md5Pwd = new SimpleHash(\"MD5\", pwd, ByteSource.Util.bytes(username + \"salt\"), 2).toHex(); return md5Pwd; } 这里的盐是username+”salt”,并且迭代次数为2,相当于md5(md5(‘’)) 并且SimpleHash迭代的时候,盐只用了一次 看下源码 protected byte[] hash(byte[] bytes, byte[] salt, int hashIterations) throws UnknownAlgorithmException { MessageDigest digest = this.getDigest(this.getAlgorithmName()); if (salt != null) { digest.reset(); digest.update(salt); } byte[] hashed = digest.digest(bytes); int iterations = hashIterations - 1; for(int i = 0; i &lt; iterations; ++i) { digest.reset(); hashed = digest.digest(hashed); // byte 进行迭代 } return hashed; } 对应的Vue中 封装HexString2Bytes 十六进制字符串转字节数组 export const hexString2Bytes = (str) => { var pos = 0; var len = str.length; if (len % 2 != 0) { return null; } len /= 2; var arrBytes = new Array(); for (var i = 0; i &lt; len; i++) { var s = str.substr(pos, 2); var v = parseInt(s, 16); arrBytes.push(v); pos += 2; } return arrBytes; } 字节数组转十六进制字符串，对负值填坑 export const bytes2HexString = (arrBytes) => { var str = \"\"; for (var i = 0; i &lt; arrBytes.length; i++) { var tmp; var num=arrBytes[i]; if (num &lt; 0) { //此处填坑，当byte因为符合位导致数值为负时候，需要对数据进行处理 tmp =(255+num+1).toString(16); } else { tmp = num.toString(16); } if (tmp.length == 1) { tmp = \"0\" + tmp; } str += tmp; } return str; } 封装对应后端md5的方法 import md5 from 'js-md5'; export const myMD5 = (str, salt, iter) => { var a = md5(salt + str, 32); if(iter > 1) { iter = iter - 1; for (var i = 0; i &lt; iter; i++) { a = md5(hexString2Bytes(a), 32); } } return a; }","categories":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"}]},{"title":"Spring-MybatisPlus代码生成器","slug":"Spring-MybatisPlus代码生成器","date":"2019-09-12T06:25:38.000Z","updated":"2020-10-30T12:45:52.020Z","comments":true,"path":"2019/09/12/spring-mybatisplus-dai-ma-sheng-cheng-qi/","link":"","permalink":"http://dreamcat.ink/2019/09/12/spring-mybatisplus-dai-ma-sheng-cheng-qi/","excerpt":"","text":"​ Mybatis不仅提供了代码生成器，而Mybatis Plus也提供了代码生成器，非常的简单明了。看一下怎么玩的吧。 官网Mybatis Plus 官网，可以看API文档进行开发。当然，代码生成器也在其中 pom部分依赖文件&lt;!-- mybatis-plus --> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId> &lt;version>3.1.1&lt;/version> &lt;/dependency> &lt;!-- lombok --> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;optional>true&lt;/optional> &lt;/dependency> &lt;!-- mybatis-plus-generator --> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-generator&lt;/artifactId> &lt;version>3.2.0&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.velocity&lt;/groupId> &lt;artifactId>velocity-engine-core&lt;/artifactId> &lt;version>2.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.freemarker&lt;/groupId> &lt;artifactId>freemarker&lt;/artifactId> &lt;version>2.3.29&lt;/version> &lt;/dependency> 在包下创建java文件package com.mf.vuehr; import java.util.HashMap; import java.util.Map; import com.baomidou.mybatisplus.annotation.DbType; import com.baomidou.mybatisplus.generator.AutoGenerator; import com.baomidou.mybatisplus.generator.InjectionConfig; import com.baomidou.mybatisplus.generator.config.*; import com.baomidou.mybatisplus.generator.config.rules.NamingStrategy; /** * &lt;p> * 代码生成器演示 * &lt;/p> */ public class MpGenerator { final static String dirPath = \"C://\"; // 地址 /** * &lt;p> * MySQL 生成演示 * &lt;/p> */ public static void main(String[] args) { AutoGenerator mpg = new AutoGenerator(); // 全局配置 GlobalConfig gc = new GlobalConfig(); gc.setOutputDir(dirPath); gc.setAuthor(\"mf\"); gc.setFileOverride(true); //是否覆盖 gc.setActiveRecord(true);// 不需要ActiveRecord特性的请改为false gc.setEnableCache(false);// XML 二级缓存 gc.setBaseResultMap(true);// XML ResultMap gc.setBaseColumnList(true);// XML columList mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setDbType(DbType.MYSQL); dsc.setDriverName(\"com.mysql.cj.jdbc.Driver\"); dsc.setUsername(\"root\"); dsc.setPassword(\"123456\"); dsc.setUrl(\"jdbc:mysql://127.0.0.1:3306/vhr?serverTimezone=UTC\"); mpg.setDataSource(dsc); // 策略配置 StrategyConfig strategy = new StrategyConfig(); // strategy.setCapitalMode(true);// 全局大写命名 ORACLE 注意 strategy.setTablePrefix(new String[] { \"tb_\", \"tsys_\" });// 此处可以修改为您的表前缀 strategy.setNaming(NamingStrategy.underline_to_camel);// 表名生成策略 strategy.setEntityLombokModel(true); // strategy.setEntityBuilderModel(false); mpg.setStrategy(strategy); // 包配置 PackageConfig pc = new PackageConfig(); pc.setParent(\"com.mf\"); pc.setModuleName(\"vuehr\"); pc.setController(\"controller\"); pc.setEntity(\"bean\"); pc.setMapper(\"mapper\"); pc.setService(\"service\"); pc.setServiceImpl(\"serviceImpl\"); pc.setXml(\"mapperXml\"); mpg.setPackageInfo(pc); // 注入自定义配置，可以在 VM 中使用 cfg.abc 【可无】 InjectionConfig cfg = new InjectionConfig() { @Override public void initMap() { Map&lt;String, Object> map = new HashMap&lt;String, Object>(); map.put(\"abc\", this.getConfig().getGlobalConfig().getAuthor() + \"-mp\"); this.setMap(map); } }; mpg.setCfg(cfg); // 执行生成 mpg.execute(); System.err.println(mpg.getCfg().getMap().get(\"abc\")); } }","categories":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://dreamcat.ink/tags/mybatis/"}]},{"title":"Spring-Springboot中Cors跨域问题","slug":"Spring-Springboot中Cors跨域问题","date":"2019-09-06T15:07:33.000Z","updated":"2020-10-29T15:48:35.341Z","comments":true,"path":"2019/09/06/spring-springboot-zhong-cors-kua-yu-wen-ti/","link":"","permalink":"http://dreamcat.ink/2019/09/06/spring-springboot-zhong-cors-kua-yu-wen-ti/","excerpt":"","text":"应用场景 一般 security 针对某一个接口 针对某一些列接口 针对全局 一般针对某一个接口 使用@CrossOrigin @CrossOrigin(origins = {\"http://localhost:9000\", \"null\"}) @RequestMapping(value = \"/test\", method = RequestMethod.GET) public String greetings() { return \"{\\\"project\\\":\\\"just a test\\\"}\"; } 针对某一系列接口@CrossOrigin(origins = {\"http://localhost:9000\", \"null\"}) @RestController @SpringBootApplication public class SpringBootCorsTestApplication { 针对全局package com.mf.vuehr.config; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.config.annotation.CorsRegistry; import org.springframework.web.servlet.config.annotation.WebMvcConfigurer; @Configuration public class WebMvcConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(\"/**\") .allowedOrigins(\"http://localhost:8080\") //允许的origin .allowedMethods(\"*\") //允许的方法 .allowedHeaders(\"*\") .allowCredentials(true) //是否允许携带cookie .maxAge(3600); } } security 使用过滤器,且注意优先级 package com.mf.vuehr.config; import org.springframework.context.annotation.Configuration; import org.springframework.core.Ordered; import org.springframework.core.annotation.Order; import javax.servlet.*; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; @Order(Ordered.HIGHEST_PRECEDENCE) @Configuration public class CORSFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) servletRequest; HttpServletResponse response = (HttpServletResponse) servletResponse; response.setHeader(\"Access-Control-Allow-Origin\",\"http://localhost:8080\"); response.setHeader(\"Access-Control-Allow-Credentials\",\"true\"); response.setHeader(\"Access-Control-Allow-Methods\",\"POST,GET,OPTIONS,PUT,DELETE,PATCH,HEAD\"); response.setHeader(\"Access-Control-Allow-Max-Age\",\"3600\"); response.setHeader(\"Access-Control-Allow-Headers\",\"Content-Type,XFILENAME,x-requested-with,XFILECATEGORY,XFILESIZE\"); if(\"OPTIONS\".equalsIgnoreCase(request.getMethod())){ response.setStatus(HttpServletResponse.SC_OK); }else{ filterChain.doFilter(servletRequest,servletResponse); } } @Override public void destroy() { } }","categories":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"}]},{"title":"Vue中axios遇见的一些问题","slug":"Vue中axios遇见的一些问题","date":"2019-09-06T15:01:33.000Z","updated":"2020-10-29T15:46:37.748Z","comments":true,"path":"2019/09/06/vue-zhong-axios-yu-jian-de-yi-xie-wen-ti/","link":"","permalink":"http://dreamcat.ink/2019/09/06/vue-zhong-axios-yu-jian-de-yi-xie-wen-ti/","excerpt":"","text":"引言 ​ 之前最近做vue项目，做图片上传的功能，使用get给后台发送数据，后台能收到，使用post给后台发送的时候后台显示一直没有收到数据 参数为null。当headers遇到下面的情况时. application/x-www-form-urlencodedURLSearchParamsconst url ='http://****你的接口****'; var params = new URLSearchParams(); params.append('key1', 'value1'); //你要传给后台的参数值 key/value params.append('key2', 'value2'); params.append('key3', 'value3'); this.$axios({ method: 'post', url:url, data:params }).then((res)=>{ }); 使用qsimport qs from 'qs'; let postData = this.$qs.stringify({ key1:value1, key2:value2, key3:value3, }); this.$axios({ method: 'post', url:'url', data:postData }).then((res)=>{ });","categories":[{"name":"vue","slug":"vue","permalink":"http://dreamcat.ink/categories/vue/"}],"tags":[{"name":"vue","slug":"vue","permalink":"http://dreamcat.ink/tags/vue/"}]},{"title":"Windows安装Vue","slug":"windows安装vue","date":"2019-08-25T15:07:33.000Z","updated":"2020-10-29T15:45:34.667Z","comments":true,"path":"2019/08/25/windows-an-zhuang-vue/","link":"","permalink":"http://dreamcat.ink/2019/08/25/windows-an-zhuang-vue/","excerpt":"","text":"前提 安装nvm(可省略) 安装nodejs(必须) vue-cli更换镜像 终端输入npm install -g cnpm --registry=https://registry.npm.taobao.org 2.0 打开终端输入sudo npm install -g vue-cli 3.0 打开终端输入sudo npm install -g @vue/cli 创建项目 打开终端输入vue create vue-test 选择Manually select features 选择babel、Router、Vuex、CSS、Lint /Formatter和Unit Testing 选择less处理器 选择standard config 选择Lint on save 选择Jest 选择In dedicated config files 初始化完毕,启动项目npm run server 打包项目npm run build","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"vue","slug":"vue","permalink":"http://dreamcat.ink/tags/vue/"}]},{"title":"解决vue跨域的问题","slug":"解决vue跨域的问题","date":"2019-08-08T11:16:50.000Z","updated":"2020-10-30T12:37:31.777Z","comments":true,"path":"2019/08/08/jie-jue-vue-kua-yu-de-wen-ti/","link":"","permalink":"http://dreamcat.ink/2019/08/08/jie-jue-vue-kua-yu-de-wen-ti/","excerpt":"","text":"引言 是这样的问题，前不久做的前后端分离的一个项目，为了简单快速，前端选择了vue框架，后端选择了flask的框架，但flask这不是重点，部署用的是nginx，那么之前跨域利用了nginx的一些配置解决的，但使用了flask整合的sessions之后，发现前端vue每次请求登陆的时候，sessions每次都不一样，都会被覆盖。所有，开启了解决这个问题的支线任务了。 Vue配置​ 用Vue框架，必然会使用axios插件进行http请求，那必然是对axios进行简单配置。 axios 官方文档 对于上述问题， // `withCredentials` 表示跨域请求时是否需要使用凭证 withCredentials: false, // 默认的 因此，我们需要全局开启开凭证,，可以在main.js中全局配置axios.defaults.withCredentials = true; 前端更改完毕 nginx​ 因为利用nginx进行header配置，因此，直接贴配置 server { listen 8001; server_name localhost; access_log logs/access_8001.log main; error_log logs/error_8001.log info; location / { add_header Access-Control-Allow-Origin \"$http_origin\"; # 这里不能是'*'，因为开启凭证，则不允许‘*’，需要指定域名 add_header Access-Control-Allow-Methods 'GET, POST, OPTIONS'; add_header Access-Control-Allow-Credentials 'true'; add_header Access-Control-Allow-Headers 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization'; if ($request_method = 'OPTIONS') { return 204; } # 这里是flask和uwsgi整合在nginx的配置 include uwsgi_params; uwsgi_pass 127.0.0.1:5555; uwsgi_param UWSGI_CHDIR /project/medicine/MedicineFlask/; uwsgi_param UWSGI_SCRIPT run:app; } }","categories":[{"name":"vue","slug":"vue","permalink":"http://dreamcat.ink/categories/vue/"}],"tags":[{"name":"vue","slug":"vue","permalink":"http://dreamcat.ink/tags/vue/"}]},{"title":"Spring-SpringBoot整合MybatisPlus和Redis...","slug":"Spring-SpringBoot整合MybatisPlus和Redis...","date":"2019-07-23T02:07:33.000Z","updated":"2020-10-29T15:48:23.864Z","comments":true,"path":"2019/07/23/spring-springboot-zheng-he-mybatisplus-he-redis.../","link":"","permalink":"http://dreamcat.ink/2019/07/23/spring-springboot-zheng-he-mybatisplus-he-redis.../","excerpt":"","text":"引用工具准备 Java8 IDEA Maven用不用自带都可以，本例子用的自带 Mysql Navicat for Mysql 工具安装 参考我的博客-java安装 IDEA官网 mysql安装-参考我的博客 Navicat for Mysql &amp;&amp; crack 创建数据库/* Navicat MySQL Data Transfer Source Server : localhost Source Server Version : 80016 Source Host : localhost:3306 Source Database : ssm Target Server Type : MYSQL Target Server Version : 80016 File Encoding : 65001 Date: 2019-07-22 15:21:44 */ -- 创建ssm数据库 Create database ssm default character set utf8; use ssm; SET FOREIGN_KEY_CHECKS=0; -- ---------------------------- -- Table structure for flower -- ---------------------------- DROP TABLE IF EXISTS `flower`; CREATE TABLE `flower` ( `id` int(10) NOT NULL AUTO_INCREMENT COMMENT '编号', `name` varchar(30) NOT NULL COMMENT '花名', `price` float NOT NULL COMMENT '价格', `production` varchar(30) NOT NULL COMMENT '原产地', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of flower -- ---------------------------- INSERT INTO `flower` VALUES ('1', '矮牵牛', '1.5', '南美阿根廷'); INSERT INTO `flower` VALUES ('2', '百日草', '5', '摩西跟'); INSERT INTO `flower` VALUES ('3', '半枝莲', '4.3', '巴西'); INSERT INTO `flower` VALUES ('4', '牡丹花', '10.5', '河南洛阳'); INSERT INTO `flower` VALUES ('5', '喇叭花', '10.5', '河南焦作'); 创建工程 pom文件&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;parent> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-parent&lt;/artifactId> &lt;version>2.1.6.RELEASE&lt;/version> &lt;relativePath/> &lt;!-- lookup parent from repository --> &lt;/parent> &lt;groupId>com.example&lt;/groupId> &lt;artifactId>demo&lt;/artifactId> &lt;version>0.0.1-SNAPSHOT&lt;/version> &lt;name>demo&lt;/name> &lt;description>Demo project for Spring Boot&lt;/description> &lt;properties> &lt;project.build.sourceEncoding>UTF-8&lt;/project.build.sourceEncoding> &lt;project.reporting.outputEncoding>UTF-8&lt;/project.reporting.outputEncoding> &lt;java.version>1.8&lt;/java.version> &lt;/properties> &lt;dependencies> &lt;!-- web --> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> &lt;!-- lombok --> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;optional>true&lt;/optional> &lt;/dependency> &lt;!-- mybatis-plus --> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId> &lt;version>3.1.1&lt;/version> &lt;/dependency> &lt;!-- jdbc --> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-jdbc&lt;/artifactId> &lt;/dependency> &lt;!-- mysql --> &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;scope>runtime&lt;/scope> &lt;/dependency> &lt;!-- redis --> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-data-redis&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.session&lt;/groupId> &lt;artifactId>spring-session-data-redis&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-test&lt;/artifactId> &lt;scope>test&lt;/scope> &lt;/dependency> &lt;!-- https://mvnrepository.com/artifact/com.alibaba/druid --> &lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>druid&lt;/artifactId> &lt;version>1.1.10&lt;/version> &lt;/dependency> &lt;/dependencies> &lt;build> &lt;plugins> &lt;plugin> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-maven-plugin&lt;/artifactId> &lt;/plugin> &lt;/plugins> &lt;/build> &lt;/project> 工程目录 配置文件 application.yml spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm?serverTimezone=UTC&amp;characterEncoding=utf-8 username: root password: 123456 type: com.alibaba.druid.pool.DruidDataSource redis: database: 0 host: 127.0.0.1 port: 6379 password: null timeout: 5000ms jedis: pool: # 连接池最大连接数（使用负值表示没有限制） max-active: 8 # 连接池最大阻塞等待时间（使用负值表示没有限制） max-wait: -1 # 连接池中的最大空闲连接 max-idle: 8 config/MybatisPlusConfig.java package com.example.demo.config; import com.baomidou.mybatisplus.extension.plugins.PaginationInterceptor; import org.mybatis.spring.annotation.MapperScan; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration @MapperScan(“com.example.demo.dao”) public class MybatisPlusConfig { /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } } - config/RedisConfig.java ```java package com.example.demo.config; import com.fasterxml.jackson.annotation.JsonAutoDetect; import com.fasterxml.jackson.annotation.PropertyAccessor; import com.fasterxml.jackson.databind.ObjectMapper; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.cache.CacheManager; import org.springframework.cache.annotation.CachingConfigurerSupport; import org.springframework.cache.annotation.EnableCaching; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.cache.RedisCacheConfiguration; import org.springframework.data.redis.cache.RedisCacheManager; import org.springframework.data.redis.cache.RedisCacheWriter; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer; import org.springframework.data.redis.serializer.StringRedisSerializer; import java.time.Duration; @Configuration @EnableCaching public class RedisConfig extends CachingConfigurerSupport { @Autowired private RedisConnectionFactory connectionFactory; @Bean public CacheManager cacheManager() { RedisCacheConfiguration redisCacheConfiguration = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofHours(1)); // 设置缓存有效期一小时 return RedisCacheManager .builder(RedisCacheWriter.nonLockingRedisCacheWriter(connectionFactory)) .cacheDefaults(redisCacheConfiguration).build(); } @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate() { RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;String, Object&gt;(); redisTemplate.setConnectionFactory(connectionFactory); //使用Jackson2JsonRedisSerializer来序列化和反序列化redis的value值（默认使用JDK的序列化方式） Jackson2JsonRedisSerializer serializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper mapper = new ObjectMapper(); mapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); mapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); serializer.setObjectMapper(mapper); redisTemplate.setValueSerializer(serializer); //使用StringRedisSerializer来序列化和反序列化redis的key值 StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); redisTemplate.setKeySerializer(stringRedisSerializer); redisTemplate.setHashKeySerializer(stringRedisSerializer); redisTemplate.setValueSerializer(serializer); redisTemplate.setHashValueSerializer(serializer); redisTemplate.afterPropertiesSet(); //这里设置redis事务一致 // redisTemplate.setEnableTransactionSupport(true); redisTemplate.setEnableTransactionSupport(false); return redisTemplate; } @Bean public StringRedisTemplate stringRedisTemplate() { StringRedisTemplate stringRedisTemplate = new StringRedisTemplate(); stringRedisTemplate.setConnectionFactory(connectionFactory); return stringRedisTemplate; } } config/SessionConfig.java package com.example.demo.config; import org.springframework.context.annotation.Configuration; import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession; @Configuration @EnableRedisHttpSession(maxInactiveIntervalInSeconds = 86400*30) public class SessionConfig { } config/DruidConfiguration.java package com.example.demo.config; import com.alibaba.druid.pool.DruidDataSource; import com.alibaba.druid.support.http.StatViewServlet; import com.alibaba.druid.support.http.WebStatFilter; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.boot.web.servlet.FilterRegistrationBean; import org.springframework.boot.web.servlet.ServletRegistrationBean; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class DruidConfiguration { @Bean public ServletRegistrationBean druidServlet() { ServletRegistrationBean servletRegistrationBean = new ServletRegistrationBean(new StatViewServlet(), \"/druid/*\"); // IP白名单 servletRegistrationBean.addInitParameter(\"allow\", \"\"); // IP黑名单(共同存在时，deny优先于allow) servletRegistrationBean.addInitParameter(\"deny\", \"192.168.1.1\"); //控制台管理用户 servletRegistrationBean.addInitParameter(\"loginUsername\", \"admin\"); servletRegistrationBean.addInitParameter(\"loginPassword\", \"admin\"); //是否能够重置数据 禁用HTML页面上的“Reset All”功能 servletRegistrationBean.addInitParameter(\"resetEnable\", \"false\"); return servletRegistrationBean; } @Bean public FilterRegistrationBean filterRegistrationBean() { FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new WebStatFilter()); filterRegistrationBean.addUrlPatterns(\"/*\"); filterRegistrationBean.addInitParameter(\"exclusions\", \"*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*\"); return filterRegistrationBean; } @Bean @ConfigurationProperties(prefix = \"spring.datasource\") public DruidDataSource druid(){ return new DruidDataSource(); } } 添加bean和dao bean/Flower.java package com.example.demo.bean; import lombok.Data; import java.io.Serializable; @Data public class Flower implements Serializable { private Integer id; private String name; private Float price; private String production; } dao/FlowerMapper.java package com.example.demo.dao; import com.example.demo.bean.Flower; import org.apache.ibatis.annotations.Mapper; import org.springframework.stereotype.Component; import com.baomidou.mybatisplus.core.mapper.BaseMapper; @Component //@Mapper public interface FlowerMapper extends BaseMapper&lt;Flower>{ } 测试MybatisPlus 测试文件中添加MybatisPlusTests.java package com.example.demo; import com.baomidou.mybatisplus.core.toolkit.Wrappers; import com.example.demo.bean.Flower; import com.example.demo.dao.FlowerMapper; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; @RunWith(SpringRunner.class) @SpringBootTest public class MybatisPlusTests { @Autowired private FlowerMapper flowerMapper; @Test public void testSelectOne() { Flower flower = flowerMapper.selectById(1); System.out.println(flower); } @Test public void testInsert() { Flower flower = new Flower(); flower.setName(&quot;紫罗兰&quot;); flower.setPrice((float)3.5); flower.setProduction(&quot;巴西&quot;); int flower_id = flowerMapper.insert(flower); System.out.println(flower_id); } @Test public void testDelete() { int deleteById = flowerMapper.deleteById(7); System.out.println(deleteById); } @Test public void testUpdate() { flowerMapper.update( null, Wrappers.&lt;Flower&gt;lambdaUpdate().set(Flower::getName, &quot;大西瓜&quot;).eq(Flower::getId, 2) ); } } ## 测试Redis - utils中添加RedisUtils.java ```java package com.example.demo.utils; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.stereotype.Component; import org.springframework.util.CollectionUtils; import java.util.List; import java.util.Map; import java.util.Set; import java.util.concurrent.TimeUnit; /** * Redis工具类 */ @Component public final class RedisUtils { @Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; /** * 指定缓存失效时间 * @param key * @param time 时间(秒) * @return */ public boolean expire(String key, long time) { try { if (time &gt; 0) { redisTemplate.expire(key, time, TimeUnit.SECONDS); } return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 根据key获取过期时间 * @param key * @return */ public long getExpire(String key) { return redisTemplate.getExpire(key, TimeUnit.SECONDS); } /** * 判断key是否存在 * @param key * @return */ public boolean hasKey(String key) { try { return redisTemplate.hasKey(key); } catch (Exception e) { e.printStackTrace(); return false; } } /** * 删除缓存 * @param key 可以传一个值 或多个 */ public void del(String... key) { if (key != null &amp;&amp; key.length &gt; 0) { if (key.length == 1) { redisTemplate.delete(key[0]); } else { redisTemplate.delete(CollectionUtils.arrayToList(key)); } } } /** * 普通缓存获取 * @param key * @return */ public Object get(String key) { return key == null ? null : redisTemplate.opsForValue().get(key); } /** * 普通缓存放入 * @param key * @param value * @return true成功 false失败 */ public boolean set(String key, Object value) { try { redisTemplate.opsForValue().set(key, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 普通缓存放入并设置时间 * @param key * @param value * @param time 时间(秒) time要大于0 如果time小于等于0 将设置无限期 * @return true成功 false 失败 */ public boolean set(String key, Object value, long time) { try { if (time &gt; 0) { redisTemplate.opsForValue().set(key, value, time, TimeUnit.SECONDS); } else { set(key, value); } return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 递增 * @param key * @param delta * @return */ public long incr(String key, long delta) { if (delta &lt; 0) { throw new RuntimeException(&quot;递增因子必须大于0&quot;); } return redisTemplate.opsForValue().increment(key, delta); } /** * 递减 * @param key * @param delta * @return */ public long decr(String key, long delta) { if (delta &lt; 0) { throw new RuntimeException(&quot;递减因子必须大于0&quot;); } return redisTemplate.opsForValue().increment(key, -delta); } /** * HashGet * @param key 不能为null * @param item 不能为null * @return */ public Object hget(String key, String item) { return redisTemplate.opsForHash().get(key, item); } /** * 获取hashKey对应的所有键值 * @param key * @return 对应的多个键值 */ public Map&lt;Object, Object&gt; hmget(String key) { return redisTemplate.opsForHash().entries(key); } /** * HashSet * @param key * @param map * @return true 成功 false 失败 */ public boolean hmset(String key, Map&lt;String, Object&gt; map) { try { redisTemplate.opsForHash().putAll(key, map); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * HashSet 并设置时间 * @param key * @param map * @param time * @return */ public boolean hmset(String key, Map&lt;String, Object&gt; map, long time) { try { redisTemplate.opsForHash().putAll(key, map); if (time &gt; 0) { expire(key, time); } return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 向一张hash表中放入数据,如果不存在将创建 * @param key * @param item * @param value * @return */ public boolean hset(String key, String item, Object value) { try { redisTemplate.opsForHash().put(key, item, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 向一张hash表中放入数据,如果不存在将创建 * @param key * @param item * @param value * @param time 时间(秒) 注意:如果已存在的hash表有时间,这里将会替换原有的时间 * @return */ public boolean hset(String key, String item, Object value, long time) { try { redisTemplate.opsForHash().put(key, item, value); if (time &gt; 0) { expire(key, time); } return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 删除hash表中的值 * @param key * @param item */ public void hdel(String key, Object... item) { redisTemplate.opsForHash().delete(key, item); } /** * 判断hash表中是否有该项的值 * @param key * @param item * @return */ public boolean hHasKey(String key, String item) { return redisTemplate.opsForHash().hasKey(key, item); } /** * hash递增 如果不存在,就会创建一个 并把新增后的值返回 * @param key * @param item * @param by 要增加几(大于0) * @return */ public double hincr(String key, String item, double by) { return redisTemplate.opsForHash().increment(key, item, by); } /** * hash递减 * @param key * @param item * @param by * @return */ public double hdecr(String key, String item, double by) { return redisTemplate.opsForHash().increment(key, item, -by); } // ============================set============================= /** * 根据key获取Set中的所有值 * @param key 键 * @return */ public Set&lt;Object&gt; sGet(String key) { try { return redisTemplate.opsForSet().members(key); } catch (Exception e) { e.printStackTrace(); return null; } } /** * 根据value从一个set中查询,是否存在 * @param key 键 * @param value 值 * @return true 存在 false不存在 */ public boolean sHasKey(String key, Object value) { try { return redisTemplate.opsForSet().isMember(key, value); } catch (Exception e) { e.printStackTrace(); return false; } } /** * 将数据放入set缓存 * @param key 键 * @param values 值 可以是多个 * @return 成功个数 */ public long sSet(String key, Object... values) { try { return redisTemplate.opsForSet().add(key, values); } catch (Exception e) { e.printStackTrace(); return 0; } } /** * 将set数据放入缓存 * @param key 键 * @param time 时间(秒) * @param values 值 可以是多个 * @return 成功个数 */ public long sSetAndTime(String key, long time, Object... values) { try { Long count = redisTemplate.opsForSet().add(key, values); if (time &gt; 0) expire(key, time); return count; } catch (Exception e) { e.printStackTrace(); return 0; } } /** * 获取set缓存的长度 * @param key 键 * @return */ public long sGetSetSize(String key) { try { return redisTemplate.opsForSet().size(key); } catch (Exception e) { e.printStackTrace(); return 0; } } /** * 移除值为value的 * @param key 键 * @param values 值 可以是多个 * @return 移除的个数 */ public long setRemove(String key, Object... values) { try { Long count = redisTemplate.opsForSet().remove(key, values); return count; } catch (Exception e) { e.printStackTrace(); return 0; } } // ===============================list================================= /** * 获取list缓存的内容 * @param key 键 * @param end 结束 0 到 -1代表所有值 * @return */ public List&lt;Object&gt; lGet(String key, long start, long end) { try { return redisTemplate.opsForList().range(key, start, end); } catch (Exception e) { e.printStackTrace(); return null; } } /** * 获取list缓存的长度 * @param key 键 * @return */ public long lGetListSize(String key) { try { return redisTemplate.opsForList().size(key); } catch (Exception e) { e.printStackTrace(); return 0; } } /** * 通过索引 获取list中的值 * @param key 键 * @param index 索引 index&gt;=0时， 0 表头，1 第二个元素，依次类推；index&lt;0时，-1，表尾，-2倒数第二个元素，依次类推 * @return */ public Object lGetIndex(String key, long index) { try { return redisTemplate.opsForList().index(key, index); } catch (Exception e) { e.printStackTrace(); return null; } } /** * 将list放入缓存 * @param key 键 * @param value 值 * @return */ public boolean lSet(String key, Object value) { try { redisTemplate.opsForList().rightPush(key, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 将list放入缓存 * @param key 键 * @param value 值 * @param time 时间(秒) * @return */ public boolean lSet(String key, Object value, long time) { try { redisTemplate.opsForList().rightPush(key, value); if (time &gt; 0) expire(key, time); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 将list放入缓存 * @param key 键 * @param value 值 * @return */ public boolean lSet(String key, List&lt;Object&gt; value) { try { redisTemplate.opsForList().rightPushAll(key, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 将list放入缓存 * * @param key 键 * @param value 值 * @param time 时间(秒) * @return */ public boolean lSet(String key, List&lt;Object&gt; value, long time) { try { redisTemplate.opsForList().rightPushAll(key, value); if (time &gt; 0) expire(key, time); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 根据索引修改list中的某条数据 * @param key 键 * @param index 索引 * @param value 值 * @return */ public boolean lUpdateIndex(String key, long index, Object value) { try { redisTemplate.opsForList().set(key, index, value); return true; } catch (Exception e) { e.printStackTrace(); return false; } } /** * 移除N个值为value * @param key 键 * @param count 移除多少个 * @param value 值 * @return 移除的个数 */ public long lRemove(String key, long count, Object value) { try { Long remove = redisTemplate.opsForList().remove(key, count, value); return remove; } catch (Exception e) { e.printStackTrace(); return 0; } } } 添加测试文件RedisTests.java package com.example.demo; import com.example.demo.utils.RedisUtils; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.test.context.junit4.SpringRunner; @RunWith(SpringRunner.class) @SpringBootTest public class RedisTests { @Autowired private StringRedisTemplate stringRedisTemplate; @Autowired private RedisUtils redisUtils; @Test public void test() throws Exception { System.out.println(redisUtils.hasKey(&quot;ccc&quot;)); System.out.println(redisUtils.set(&quot;aaa&quot;,&quot;111&quot;)); System.out.println(redisUtils.get(&quot;aaa&quot;)); } } ## 测试Druid - 输入`localhost:8080/druid/login.html` - ![](https://raw.githubusercontent.com/DreamCats/PicBed/master/20191121165831.png) - ![](https://raw.githubusercontent.com/DreamCats/PicBed/master/20191121165851.png) ## 测试controller中的redis - 测试controller，在controller文件夹下编写FlowerController.java - ```java package com.example.demo.controller; import com.example.demo.bean.Flower; import com.example.demo.dao.FlowerMapper; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.cache.annotation.Cacheable; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RestController; @RestController public class FlowerController { @Autowired private FlowerMapper flowerMapper; @Cacheable(value = &quot;flower&quot;) @GetMapping(&quot;/flower/{id}&quot;) public Flower getFlower(@PathVariable(&quot;id&quot;) Integer id) { System.out.println(&quot;没有缓存!!!&quot;); return flowerMapper.selectById(id); } } 启动并输入localhsot:8080/flower/1","categories":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"}]},{"title":"Spring-ssm整合(spring、springMVC和mybatis)IDEA版本","slug":"Spring-ssm整合-spring、springMVC和mybatis-IDEA版本","date":"2019-07-21T07:37:19.000Z","updated":"2020-10-29T15:48:32.753Z","comments":true,"path":"2019/07/21/spring-ssm-zheng-he-spring-springmvc-he-mybatis-idea-ban-ben/","link":"","permalink":"http://dreamcat.ink/2019/07/21/spring-ssm-zheng-he-spring-springmvc-he-mybatis-idea-ban-ben/","excerpt":"","text":"引用工具准备 操作系统：win10，和linux或者mac思路差不多是一样的 Java8 IDEA Maven Tomcat8 Mysql Navicat for Mysql 工具安装Java8 参考我的博客-java安装 IDEA官网 Maven—&gt;apache-maven-3.6.1-bin.zip 下载解压，解压之后的目录如C:\\Web\\maven 进入conf目录，打开settings.xml文件，添加lib目录以及换淘宝源 &lt;localRepository>C:\\Web\\mlib&lt;/localRepository> &lt;!--找到mirrors 在该标签下添加以下代码 --> &lt;mirror> &lt;id>alimaven&lt;/id> &lt;name>aliyun maven&lt;/name> &lt;url>http://maven.aliyun.com/nexus/content/groups/public/&lt;/url> &lt;mirrorOf>central&lt;/mirrorOf> &lt;/mirror> 保存settings.xml文件，接下来配置环境变量 win+s—&gt;输入环境变量—&gt;打开环境变量，选择系统或者个人都行，打开path—&gt;添加一条C:\\Web\\maven\\bin win+r—&gt;输入MVN -V—&gt;即可看到mvn的版本号 Tomcat8—&gt;64-bit Windows zip 其实和maven的操作是差不多的，同样解压C:\\Web\\tomcat8 可以去bin目录下去启动tomcat8，这里就不演示了。 mysql安装-参考我的博客 Navicat for Mysql &amp;&amp; crack 创建数据库/* Navicat MySQL Data Transfer Source Server : localhost Source Server Version : 80016 Source Host : localhost:3306 Source Database : ssm Target Server Type : MYSQL Target Server Version : 80016 File Encoding : 65001 Date: 2019-07-22 15:21:44 */ -- 创建ssm数据库 Create database ssm default character set utf8; use ssm; SET FOREIGN_KEY_CHECKS=0; -- ---------------------------- -- Table structure for flower -- ---------------------------- DROP TABLE IF EXISTS `flower`; CREATE TABLE `flower` ( `id` int(10) NOT NULL AUTO_INCREMENT COMMENT '编号', `name` varchar(30) NOT NULL COMMENT '花名', `price` float NOT NULL COMMENT '价格', `production` varchar(30) NOT NULL COMMENT '原产地', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of flower -- ---------------------------- INSERT INTO `flower` VALUES ('1', '矮牵牛', '1.5', '南美阿根廷'); INSERT INTO `flower` VALUES ('2', '百日草', '5', '摩西跟'); INSERT INTO `flower` VALUES ('3', '半枝莲', '4.3', '巴西'); INSERT INTO `flower` VALUES ('4', '牡丹花', '10.5', '河南洛阳'); INSERT INTO `flower` VALUES ('5', '喇叭花', '10.5', '河南焦作'); 创建Maven工程 如图所示 添加pom.xml文件&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;groupId>com.mf&lt;/groupId> &lt;artifactId>demo&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;packaging>war&lt;/packaging> &lt;name>ssm Maven Webapp&lt;/name> &lt;!-- FIXME change it to the project's website --> &lt;url>http://www.example.com&lt;/url> &lt;properties> &lt;project.build.sourceEncoding>UTF-8&lt;/project.build.sourceEncoding> &lt;maven.compiler.source>1.7&lt;/maven.compiler.source> &lt;maven.compiler.target>1.7&lt;/maven.compiler.target> &lt;!-- spring版本号 --> &lt;spring.version>4.2.5.RELEASE&lt;/spring.version> &lt;!-- mybatis版本号 --> &lt;mybatis.version>3.2.4&lt;/mybatis.version> &lt;!-- log4j日志文件管理包版本 --> &lt;slf4j.version>1.6.6&lt;/slf4j.version> &lt;log4j.version>1.2.12&lt;/log4j.version> &lt;/properties> &lt;dependencies> &lt;!-- spring核心包 --> &lt;!-- springframe start --> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-core&lt;/artifactId> &lt;version>${spring.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-web&lt;/artifactId> &lt;version>${spring.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-oxm&lt;/artifactId> &lt;version>${spring.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-tx&lt;/artifactId> &lt;version>${spring.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-jdbc&lt;/artifactId> &lt;version>${spring.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-webmvc&lt;/artifactId> &lt;version>${spring.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-aop&lt;/artifactId> &lt;version>${spring.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-context-support&lt;/artifactId> &lt;version>${spring.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework&lt;/groupId> &lt;artifactId>spring-test&lt;/artifactId> &lt;version>${spring.version}&lt;/version> &lt;/dependency> &lt;!-- springframe end --> &lt;!-- mybatis核心包 --> &lt;dependency> &lt;groupId>org.mybatis&lt;/groupId> &lt;artifactId>mybatis&lt;/artifactId> &lt;version>${mybatis.version}&lt;/version> &lt;/dependency> &lt;!-- mybatis/spring包 --> &lt;dependency> &lt;groupId>org.mybatis&lt;/groupId> &lt;artifactId>mybatis-spring&lt;/artifactId> &lt;version>1.2.2&lt;/version> &lt;/dependency> &lt;!-- mysql驱动包 --> &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;version>8.0.16&lt;/version> &lt;/dependency> &lt;!-- jstl --> &lt;dependency> &lt;groupId>javax.servlet&lt;/groupId> &lt;artifactId>jstl&lt;/artifactId> &lt;version>1.2&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>javax.servlet.jsp&lt;/groupId> &lt;artifactId>jsp-api&lt;/artifactId> &lt;version>2.2.1-b03&lt;/version> &lt;scope>provided&lt;/scope> &lt;/dependency> &lt;!-- servlet --> &lt;dependency> &lt;groupId>javax.servlet&lt;/groupId> &lt;artifactId>servlet-api&lt;/artifactId> &lt;version>2.5&lt;/version> &lt;scope>provided&lt;/scope> &lt;/dependency> &lt;!-- json数据 --> &lt;dependency> &lt;groupId>com.fasterxml.jackson.core&lt;/groupId> &lt;artifactId>jackson-core&lt;/artifactId> &lt;version>2.7.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>com.fasterxml.jackson.core&lt;/groupId> &lt;artifactId>jackson-annotations&lt;/artifactId> &lt;version>2.7.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>com.fasterxml.jackson.core&lt;/groupId> &lt;artifactId>jackson-databind&lt;/artifactId> &lt;version>2.7.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>net.sf.json-lib&lt;/groupId> &lt;artifactId>json-lib&lt;/artifactId> &lt;version>2.4&lt;/version> &lt;classifier>jdk15&lt;/classifier> &lt;/dependency> &lt;!-- commons --> &lt;dependency> &lt;groupId>commons-lang&lt;/groupId> &lt;artifactId>commons-lang&lt;/artifactId> &lt;version>2.4&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>commons-logging&lt;/groupId> &lt;artifactId>commons-logging&lt;/artifactId> &lt;version>1.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>commons-pool&lt;/groupId> &lt;artifactId>commons-pool&lt;/artifactId> &lt;version>1.5.6&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>commons-dbcp&lt;/groupId> &lt;artifactId>commons-dbcp&lt;/artifactId> &lt;version>1.4&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>commons-beanutils&lt;/groupId> &lt;artifactId>commons-beanutils&lt;/artifactId> &lt;version>1.8.3&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>commons-httpclient&lt;/groupId> &lt;artifactId>commons-httpclient&lt;/artifactId> &lt;version>3.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>commons-collections&lt;/groupId> &lt;artifactId>commons-collections&lt;/artifactId> &lt;version>3.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>commons-codec&lt;/groupId> &lt;artifactId>commons-codec&lt;/artifactId> &lt;version>1.9&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>javax.annotation&lt;/groupId> &lt;artifactId>jsr250-api&lt;/artifactId> &lt;version>1.0&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>net.sf.ezmorph&lt;/groupId> &lt;artifactId>ezmorph&lt;/artifactId> &lt;version>1.0.6&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>javax.activation&lt;/groupId> &lt;artifactId>activation&lt;/artifactId> &lt;version>1.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>taglibs&lt;/groupId> &lt;artifactId>standard&lt;/artifactId> &lt;version>1.1.2&lt;/version> &lt;/dependency> &lt;!-- 日志文件管理包 --> &lt;!-- log start --> &lt;dependency> &lt;groupId>log4j&lt;/groupId> &lt;artifactId>log4j&lt;/artifactId> &lt;version>${log4j.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.slf4j&lt;/groupId> &lt;artifactId>slf4j-api&lt;/artifactId> &lt;version>${slf4j.version}&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.slf4j&lt;/groupId> &lt;artifactId>slf4j-log4j12&lt;/artifactId> &lt;version>${slf4j.version}&lt;/version> &lt;/dependency> &lt;!--上传--> &lt;dependency> &lt;groupId>commons-fileupload&lt;/groupId> &lt;artifactId>commons-fileupload&lt;/artifactId> &lt;version>1.3.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>commons-io&lt;/groupId> &lt;artifactId>commons-io&lt;/artifactId> &lt;version>2.4&lt;/version> &lt;/dependency> &lt;!-- log end --> &lt;dependency> &lt;groupId>junit&lt;/groupId> &lt;artifactId>junit&lt;/artifactId> &lt;version>4.11&lt;/version> &lt;scope>test&lt;/scope> &lt;/dependency> &lt;/dependencies> &lt;build> &lt;finalName>demo&lt;/finalName> &lt;plugins> &lt;plugin> &lt;groupId>org.mybatis.generator&lt;/groupId> &lt;artifactId>mybatis-generator-maven-plugin&lt;/artifactId> &lt;version>1.3.2&lt;/version> &lt;configuration> &lt;verbose>true&lt;/verbose> &lt;overwrite>true&lt;/overwrite> &lt;/configuration> &lt;/plugin> &lt;/plugins> &lt;/build> &lt;/project> 工程目录 在src-&gt;main中分别添加java和resources文件夹 在src—&gt;main—&gt;java中添加包名com.mf.demo 接下来添加以下四个包名 bean层，也可以model层，也可以pojo层，也可以叫entity层 dao层，也可以是mapper层 service层 controller层 在resources下添加spring文件夹和sqlmap文件夹 在spring文件夹添加applicationContext.xml 在resources下添加4个配置文件 generator.properties、generatorConfig.xml、jdbc.properties和log4j.properties webapp下的WEB-INF下添加views文件夹 配置文件Spring和SpringMVC spirng/applicationContext.xml &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:beans=\"http://www.springframework.org/schema/beans\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd\" > &lt;!-- SpringMVC整合 --> &lt;!-- ①：对com.springmvc包中的所有类进行扫描，以完成Bean创建和自动依赖注入的功能 --> &lt;context:component-scan base-package=\"com.mf.demo\"/> &lt;mvc:annotation-driven /> &lt;!-- 静态资源访问 --> &lt;!--如果webapp下你新建了文件夹，想访问里面的静态资源，那么就要在这配置一下--> &lt;!-- &lt;mvc:resources location=\"/images/\" mapping=\"/images/**\"/>--> &lt;!-- &lt;mvc:resources location=\"/css/\" mapping=\"/css/**\"/>--> &lt;!-- &lt;mvc:resources location=\"/styles/\" mapping=\"/styles/**\"/>--> &lt;!-- &lt;mvc:resources location=\"/js/\" mapping=\"/js/**\"/>--> &lt;!-- Configures the @Controller programming model &lt;mvc:annotation-driven />--> &lt;!-- ②：启动Spring MVC的注解功能，完成请求和注解POJO的映射 --> &lt;bean class=\"org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter\"> &lt;property name=\"messageConverters\"> &lt;list> &lt;ref bean=\"mappingJacksonHttpMessageConverter\"/> &lt;/list> &lt;/property> &lt;/bean> &lt;bean id=\"mappingJacksonHttpMessageConverter\" class=\"org.springframework.http.converter.json.MappingJackson2HttpMessageConverter\"> &lt;property name=\"supportedMediaTypes\"> &lt;list> &lt;value>application/json;charset=UTF-8&lt;/value> &lt;/list> &lt;/property> &lt;/bean> &lt;!-- 配置视图解析器，把控制器的逻辑视频映射为真正的视图 --&gt; &lt;!-- /WEB-INF/jsp/start.jsp --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;viewClass&quot; value=&quot;org.springframework.web.servlet.view.JstlView&quot;/&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/views/&quot; /&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot; /&gt; &lt;/bean&gt; &lt;!-- 拦截器 --&gt; &lt;mvc:interceptors&gt; &lt;!-- 国际化操作拦截器 如果采用基于（请求/Session/Cookie）则必需配置 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.i18n.LocaleChangeInterceptor&quot; /&gt; &lt;/mvc:interceptors&gt; &lt;!-- 定义无Controller的path&lt;-&gt;view直接映射 --&gt; &lt;!-- &lt;mvc:view-controller path=&quot;/&quot; view-name=&quot;redirect:/&quot; /&gt; --&gt; ``` web.xml&lt;!DOCTYPE web-app PUBLIC \"-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN\" \"http://java.sun.com/dtd/web-app_2_3.dtd\" > &lt;web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://java.sun.com/xml/ns/javaee\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" version=\"3.0\"> &lt;display-name>Archetype Created Web Application&lt;/display-name> &lt;!-- 配置编码方式--> &lt;filter> &lt;filter-name>encodingFilter&lt;/filter-name> &lt;filter-class>org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class> &lt;init-param> &lt;param-name>encoding&lt;/param-name> &lt;param-value>UTF-8&lt;/param-value> &lt;/init-param> &lt;init-param> &lt;param-name>forceEncoding&lt;/param-name> &lt;param-value>true&lt;/param-value> &lt;/init-param> &lt;/filter> &lt;filter-mapping> &lt;filter-name>encodingFilter&lt;/filter-name> &lt;url-pattern>/*&lt;/url-pattern> &lt;/filter-mapping> &lt;!-- 配置springmvc的前端控制器 指向spring-mvc.xml 程序在启动的时候就加载springmvc 可以接受所有请求 load-on-startup：表示启动容器时初始化该Servlet； --> &lt;servlet> &lt;servlet-name>springServlet&lt;/servlet-name> &lt;servlet-class>org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class> &lt;!-- 可以自定义servlet.xml配置文件的位置和名称， 默认为WEB-INF目录下，名称为[&lt;servlet-name>]-servlet.xml，如spring-servlet.xml --> &lt;init-param> &lt;param-name>contextConfigLocation&lt;/param-name> &lt;param-value> classpath:spring/applicationContext.xml&lt;/param-value> &lt;/init-param> &lt;load-on-startup>1&lt;/load-on-startup> &lt;/servlet> &lt;!-- 将前端URL请求和后台处理方法controller建立对应关系--> &lt;servlet-mapping> &lt;servlet-name>springServlet&lt;/servlet-name> &lt;url-pattern>/&lt;/url-pattern> &lt;/servlet-mapping> &lt;!-- Spring配置 --> &lt;listener> &lt;listener-class> org.springframework.web.context.ContextLoaderListener &lt;/listener-class> &lt;/listener> &lt;!-- 取消对某一类文件的拦截--> &lt;servlet-mapping> &lt;servlet-name>default&lt;/servlet-name> &lt;url-pattern>*.md&lt;/url-pattern> &lt;/servlet-mapping> &lt;context-param> &lt;param-name>contextConfigLocation&lt;/param-name> &lt;param-value> classpath:spring/applicationContext.xml&lt;/param-value> &lt;/context-param> &lt;!-- 欢迎页面--> &lt;welcome-file-list> &lt;welcome-file>/WEB-INF/index.jsp&lt;/welcome-file> &lt;/welcome-file-list> &lt;!--404错误展示页面，可自行配置--> &lt;!--&lt;error-page>--> &lt;!--&lt;error-code>404&lt;/error-code>--> &lt;!--&lt;location>/WEB-INF/views/404.jsp&lt;/location>--> &lt;!--&lt;/error-page>--> &lt;!--设置session失效时间为30分钟 --> &lt;session-config> &lt;session-timeout>600&lt;/session-timeout> &lt;/session-config> &lt;/web-app> 配置tomcat启动 配置jdbc和mybaits 在applicationContext.xml文件中继续添加 &lt;!-- 配置会话工厂SqlSessionFactory --> &lt;!-- spring和mymatis整合 --> &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"> &lt;!-- 数据源 --> &lt;property name=\"dataSource\" ref=\"dataSource\"/> &lt;property name=\"mapperLocations\" value=\"classpath:sqlmap/*Mapper.xml\"/> &lt;property name=\"typeAliasesPackage\" value=\"com.mf.demo.bean\" /> &lt;/bean> &lt;!-- 在spring容器中配置mapper的扫描器产生的动态代理对象在spring的容器中自动注册，bean的id就是mapper类名（首字母小写）--> &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"> &lt;!-- 指定扫描包的路径，就是mapper接口的路径，多个包中间以 半角逗号隔开 --> &lt;property name=\"basePackage\" value=\"com.mf.demo.dao\"/> &lt;!-- 配置sqlSessionFactoryBeanName --> &lt;property name=\"sqlSessionFactoryBeanName\" value=\"sqlSessionFactory\"/> &lt;/bean> &lt;!-- 配置dbcp数据库连接池 --> &lt;!-- &lt;context:property-placeholder location=\"classpath:db.properties\"/> --> &lt;!--数据库配置 --> &lt;bean id = \"propertyConfigurer\" class = \"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"> &lt;property name=\"locations\"> &lt;list> &lt;value>classpath:jdbc.properties&lt;/value> &lt;/list> &lt;/property> &lt;/bean> &lt;!-- 数据库连接池 --> &lt;bean id=\"dataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\" destroy-method=\"close\"> &lt;property name=\"driverClassName\" value=\"${jdbc.driver}\"/> &lt;property name=\"url\" value=\"${jdbc.url}\"/> &lt;property name=\"username\" value=\"${jdbc.username}\"/> &lt;property name=\"password\" value=\"${jdbc.password}\"/> &lt;!-- &lt;property name=\"initialSize\" value=\"1\"/>--> &lt;!-- &lt;property name=\"maxActive\" value=\"100\"/>--> &lt;!-- &lt;property name=\"maxIdle\" value=\"5\"/>--> &lt;!-- &lt;property name=\"maxWait\" value=\"80000\"/>--> &lt;/bean> &lt;!-- 配置事务管理器 --> &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"> &lt;property name=\"dataSource\" ref=\"dataSource\" /> &lt;/bean> 在jdbc.properties中添加 jdbc.driver=com.mysql.cj.jdbc.Driver # 版本较低 jdbc.driver=com.mysql.jdbc.Driver # 版本较高 jdbc.driver=com.mysql.cj.jdbc.Driver # 配置url jdbc.url=jdbc:mysql://localhost:3306/ssm?serverTimezone=UTC # 版本较低 jdbc.url=jdbc:mysql://localhost:3306/ssm? &amp;版本较低 # 版本较高 jdbc.url=jdbc:mysql://localhost:3306/ssm?serverTimezone=UTC&amp;characterEncoding=utf-8 jdbc.username=root # 密码 jdbc.password=123456 #initialSize=0 #maxActive=20 #maxIdle=20 #minIdle=1 #maxWait=60000 在log4j.properties中添加 log4j.rootLogger = INFO,D log4j.logger.toConsole=debug,stdout log4j.appender.stdout = org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target = System.out log4j.appender.stdout.layout = org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern =%d{yyyy-MM-dd HH:mm:ss} [%5p] - %c -%F(%L) -%m%n log4j.logger.daily=INFO,D log4j.appender.D = org.apache.log4j.DailyRollingFileAppender log4j.appender.D.File = ${catalina.home}/logs/helloworld/helloworld.log log4j.appender.D.Append = true log4j.appender.D.Threshold = INFO log4j.appender.D.layout = org.apache.log4j.PatternLayout log4j.appender.D.layout.ConversionPattern =%d{yyyy-MM-dd HH:mm:ss} [%5p] - %c -%F(%L) -%m%n ##log4j.logger.org.apache.ibatis=debug,stdout ##log4j.logger.java.sql=debug,stdout - 在generator.properties中添加 - ```properties # 高版本 driver=com.mysql.cj.jdbc.Driver # 高版本 url=jdbc:mysql://localhost:3306/ssm?serverTimezone=UTC username=root password=123456 #entity 包名和java目录 modelPackage=com.mf.demo.bean modelProject=src/main/java #sqlmap包名和resources目录 sqlPackage=sqlmap sqlProject=src/main/resources #mapper包名和java目录 mapperPackage=com.mf.demo.dao mapperProject=src/main/java table=flower 在generatorConfig.xml中添加 &lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?> &lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"> &lt;generatorConfiguration> &lt;!--导入属性配置 --> &lt;properties resource=\"generator.properties\"/> &lt;classPathEntry location=\"C:\\Web\\mlib\\mysql\\mysql-connector-java\\8.0.16\\mysql-connector-java-8.0.16.jar\" /> &lt;context id=\"context1\"> &lt;!-- 注释 --> &lt;commentGenerator> &lt;property name=\"suppressAllComments\" value=\"true\" />&lt;!-- 是否取消注释 --> &lt;property name=\"suppressDate\" value=\"true\" /> &lt;!-- 是否生成注释代时间戳 --> &lt;/commentGenerator> &lt;jdbcConnection driverClass=\"${driver}\" connectionURL=\"${url}\" userId=\"${username}\" password=\"${password}\" /> &lt;!-- 类型转换 --> &lt;javaTypeResolver> &lt;!-- 是否使用bigDecimal， false可自动转化以下类型（Long, Integer, Short, etc.） --> &lt;property name=\"forceBigDecimals\" value=\"false\" /> &lt;/javaTypeResolver> &lt;javaModelGenerator targetPackage=\"${modelPackage}\" targetProject=\"${modelProject}\" /> &lt;sqlMapGenerator targetPackage=\"${sqlPackage}\" targetProject=\"${sqlProject}\" /> &lt;javaClientGenerator targetPackage=\"${mapperPackage}\" targetProject=\"${mapperProject}\" type=\"XMLMAPPER\" /> &lt;!-- 如果需要通配所有表 直接用sql的通配符 %即可 --> &lt;table schema=\"\" tableName=\"${table}\" enableCountByExample=\"false\" enableUpdateByExample=\"false\" enableDeleteByExample=\"false\" enableSelectByExample=\"false\" selectByExampleQueryId=\"false\"/> &lt;/context> &lt;/generatorConfiguration> 单元测试 package com.mf.demo.dao; import com.mf.demo.bean.Flower; import org.junit.Before; import org.junit.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; public class FlowerMapperTest { private ApplicationContext applicationContext; @Autowired private FlowerMapper flowerMapper; @Before public void setUp() throws Exception { // 加载spring配置 applicationContext = new ClassPathXmlApplicationContext(&quot;classpath:spring/applicationContext.xml&quot;); // 导入需要测试的 bean flowerMapper = applicationContext.getBean(FlowerMapper.class); } @Test public void selectByPrimaryKey() { Flower flower = flowerMapper.selectByPrimaryKey(2); System.out.println(flower.toString()); } } - ![](https://raw.githubusercontent.com/DreamCats/PicBed/master/20191121170519.png) ## Contorller - User - ```java package com.mf.demo.bean; public class User { String id; String name; public String getId() { return id; } public void setId(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return &quot;User{&quot; + &quot;id=&#39;&quot; + id + &#39;\\&#39;&#39; + &quot;, name=&#39;&quot; + name + &#39;\\&#39;&#39; + &#39;}&#39;; } } UserDao package com.mf.demo.dao; import com.mf.springmvc.bean.User; import org.springframework.stereotype.Repository; @Repository public class UserDao { public User getUser() { return new User(); } } UserController package com.mf.demo.controller; import com.mf.springmvc.bean.User; import com.mf.springmvc.dao.UserDao; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class UserController { @Autowired private UserDao userDao; @RequestMapping(value=”/getUser”) public User getUser() { return userDao.getUser(); } } - 浏览器输入`localhost:8080/getUser`即可","categories":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"}]},{"title":"StringBuffer和StringBuilder原理解析","slug":"StringBuffer和StringBuilder原理解析","date":"2019-07-17T11:52:18.000Z","updated":"2020-10-29T15:47:37.238Z","comments":true,"path":"2019/07/17/stringbuffer-he-stringbuilder-yuan-li-jie-xi/","link":"","permalink":"http://dreamcat.ink/2019/07/17/stringbuffer-he-stringbuilder-yuan-li-jie-xi/","excerpt":"","text":"​ StringBuilder和StringBuffer作用就是处理字符串，但String类本身也具有很多方法可以用来处理字符串，那么为什么还要引入这两个类呢？ 例子 class Demo { public static void main(String[] args) { // String String str0 = \"hel\"; long start = System.currentTimeMillis(); for (int i = 0; i &lt; 100000; i++) { str0 += i; } System.out.println(System.currentTimeMillis() - start); // StringBuilder StringBuilder sb = new StringBuilder(\"hel\"); long start1 = System.currentTimeMillis(); for (int i = 0; i &lt; 100000 i++) { sb.append(i); } System.out.println(System.currentTimeMillis() - start1); // StringBuffer StringBuffer sbf = new StringBuffer(\"hel\"); long start2 = System.currentTimeMillis();3 for (int i = 0; i &lt; 100000; i++) { sbf.append(i); } System.out.println(System.currentTimeMillis() - start2); } } // 结果 29467 0 16 从以上结果可看出，执行时间差别太大了，String最慢最差，而StringBuffer和StringBuilder相差不是很大 其实，String差是因为String本身不可变，我们对String的任何操作都会返回一个新的对象，然后当前String变量指向新的对象，而原来的String对象就会被GC回收，那么在循环就会大量快速的创建新的对象，大量原来的对象会不断的被GC回收，消耗的时间是乃是非常恐怖的，而且内存占用非常大。 三者区别 String StringBuffer StringBuilder final修饰，不可继承 final修饰，不可继承 final修饰，不可继承 字符串常量，创建后不可变 字符串变量，可动态修改 字符串变量，可动态修改 不存在线程安全问题 线程安全，所有public方法由synchronized 线程不安全 大量字符串拼接效率最低 大量字符串拼接效率非常高 大量字符串拼接效率最高 append() StringBuffer和StringBuilder实现非常相似，简单看一下StringBuilder的append()方法基本原理 StringBuilder sb1 = new StringBuilder(); StringBuilder sb2 = new StringBuilder(100); StringBuilder对字符串的操作是通过char[]来实现的，通过默认构造器创建的StringBuilder，其中内部创建的char[]的默认长度为16，当然可以调用重载的构造器传递初始长度。 public StringBuilder() { super(16); } 每次调用的append(str)方法时，会首先判断组长度是否以添加传递来的字符串 public AbstractStringBuilder append(String str) { if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChar(0, len, value, count); count += len; return this; } public void ensureCapacityInternal(int minimumCapacity) { if (minimumCapacity - value.length > 0) { value = Arrays.copyOf(value, newCapacity(minimumCapacity)); } } 如果传递的字符串长度 + 数组已存放的字符的长度 &gt; 数组的长度，这时就需要进行数据扩容了。 private int newCapacity(int minCapacity) { int newCapacity = (value.length &lt;&lt; 1) + 2; if (newCapacity - minCapacity &lt; 0) { newCapacity = minCapacity; } return (newCapacity &lt;= 0 || MAX_ARRAY_SIZE - newCapacity &lt; 0) ? hugeCapacity(minCapacity) : newCapacity; } 扩容规则如下：默认将数组长度设置为’(当前数组长度 * 2) + 2’,但如果按此规则扩容后的数组也不足以添加新的字符串，就需要将数组长度设置为”数组内字符长度 + 传递的字符串长度”。 所以假如我们知道拼接的字符串的长度有100多字符，我们就可以设置初始长度150或200，这样就可以避免或减少数组扩容的次数，从而提高效率。","categories":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"}],"tags":[{"name":"java基础","slug":"java基础","permalink":"http://dreamcat.ink/tags/java%E5%9F%BA%E7%A1%80/"}]},{"title":"常用的sql语句","slug":"常用的sql语句","date":"2019-07-14T09:31:56.000Z","updated":"2020-10-29T15:55:13.068Z","comments":true,"path":"2019/07/14/chang-yong-de-sql-yu-ju/","link":"","permalink":"http://dreamcat.ink/2019/07/14/chang-yong-de-sql-yu-ju/","excerpt":"","text":"创建数据库并制定编码 Create database 数据库名 default character set utf8; Create database ssm default character set utf8; 删除数据库 drop database 数据名; drop database ssm; 创建表 一般情况： Create table 表名 ( 列名 类型 约束 auto_increment comment '备注'; ); create tablie flower ( id int(10) primary key auto_increment comment '编号', name varchar(30) not null comment '花名', price float not null comment '价格', production varchar(30) not null comment '原产地' ); create tablie flower ( `id` int(10) not null auto_increment comment '编号', `name` varchar(30) not null comment '花名', `price` float not null comment '价格', `production` varchar(30) not null comment '原产地', primary key (`id`) ); 插入信息 insert into 表明(姓名，性别，年龄) values('李一', '女', '18'); insert into `flower` (`id`, `name`, `price`, `production`) values (defalut, '矮牵牛', 2.5, '南美阿根廷') 查询信息 select * from 表名; select * from flower; select * from 表名 where ID=5 select * from flower where ID=1 更新信息 update 表名 set name='李一' where name='王五'; update flower set price=1.2 where id=1; 删除信息 delete from 表名 where id=1; delete from flower where id=1; 删除表 drop table 表名; drop table flower; 查看表信息 desc 表名; desc flower; 一对多关系 可能是写的不熟练吧，所以要做个笔记，以免之后再次忘掉。 例子：假如有这样的一对多关系，首先有一个User模型，其中该模型有id、name和gender字段。还有一个Artical模型，其中该模型有id、title、content和user_id字段，而user_id则是外键。如下图： 那么在mysql或者mysql的一些工具下创建输入以下代码： create database demo default character set utf8; -- 创建数据库 use demo; -- 创建user表格 create table user ( `id` int(10) not null auto_increment comment '编号', `name` varchar(16) not null comment '姓名', `gender` varchar(4) not null comment '性别', primary key (`id`) ); -- 创建artical create table artical ( `id` int(10) not null auto_increment comment '文章编号', `title` varchar(32) not null comment '标题', `content` text default null comment '内容', `user_id` int(10) not null comment '用户编号', primary key (`id`), key (`user_id`), constraint `artical_idfk_1` foreign key (`user_id`) references `user` (`id`) -- 这句话非常重要 constraint ··· 约束 ); -- 在user中插入数据 insert into user (`id`,`name`,`gender`) values(default, 'Maifeng', '男'); insert into user (`id`,`name`,`gender`) values(default, 'Liumeng', '女'); -- 在artical中插入数据 insert into `artical` (`id`, `title`, `content`, `user_id`) values (default, '我是mm', 'null', 1); insert into `artical` (`id`, `title`, `content`, `user_id`) values (default, '我是xxx', 'null', 2); 如下图user和artical表 更改auto_increment alter table 表名 auto_increment=5; alter table flower auto_increment=5; 常用sql语句数据库# 查看所有数据库 show databases; # 创建一个数据库 create database demo; # 删除一个数据库 drop dababase demo; # 使用这个数据库 use demo; 表# 查看所有的表 show tables; # 创建一个表 create table n(`id` int, `name` varchar(10)); create table m(`id` int, `name` varchar(10), primary key(`id`), foreign key(`id`) references n(`id`), unique); create table m(`id` int, `name` varchar(10)); # 直接将查询结果导入或复制到新创建的表 create table n select * from m; # 将创建的表与一个存在的表的数据结构类似 create table m like n; # 创建一个临时表 # 临时表将在你连接MySQL期间存在。当断开连接时，MySQL将自动删除表并释放所用的空间。也可手动删除。 create temporary table l(`id` int, name varchar(10)); # 直接将查询结果导入或复制到新的临时表 create temporary table tt select * from n; # 删除一个已经存在的表 drop table if exists m; # 更改存在表的名称 alter table n rename m; rename table n to m; # 查看表的结构(5种,效果相同) desc n; describe n; show columns in n; show columns from n; explain n; # 查看表的创建语句 show create table n; 表的结构# 添加字段 alter table n add age varchar(2); # 删除字段 alter table n drop age; # 更改字段属性和属性 alter table n change age a int; # 只更改字段属性 alter table n modify age varchar(7); 表的数据# 增加数据 insert into n values(1, 'tom', '23'), (2, 'john', '22'); insert into n select * from n; # 把数据复制一遍重新插入 # 删除数据 delete from n where id = 2; # 更改数据 update n set name = 'tom' where id = 2; # 数据查找 select * from n where name like '%h%'; # 数据排序(反序) select * from n order by name, id desc; 键# 添加主键 alter table n add primary key(id); # 删除主键 alter table n drop primary key; # 添加外键 alter table m add foreign key(id) references n(id); # 自动生成键名m_ibfk_1 # 自定义名称的外键 alter table m add constraint fk_id foreign key(id) references n(id); # 删除外键 alter table m drop foreign key `fk_id`; # 修改外键 alter table m drop foreign key `fk_id`, add constraint fk_id2 foreign key(id) references n(id); # 添加唯一键 alter table n add unique (name); alter table n add unique u_name (name); alter table n add unique index u_name (name); alter table n add constraint u_name unique (name); create unique index u_name on n(name); # 添加索引 alter table n add index (age); alter table n add index i_age (age); create index i_age on n(age); # 删除索引或唯一键 drop index u_name on n; drop index i_age on n; 视图# 创建视图 create view v as select id, name from n; create view v(id, name) as select id, name from n; # 查看视图(与表操作类似) select * from v; desc v; # 查看视图语句 show create view v; # 更改视图 CREATE OR REPLACE VIEW v AS SELECT name, age FROM n; ALTER VIEW v AS SELECT name FROM n; # 删除视图 drop view if exists v; 联接# 内联接 select * from m inner join n on m.id = n.id; # 左外连接 select * from m left join n on m.id = n.id; # 右外连接 select * from m right join n on m.id = n.id; # 交叉连接 select * from m cross join n; # 标准写法 select * from m,n; # 类似于全连接full join 的联接用法 select id, name from m union select id, name from n 函数# 聚合函数 select count(id) as total from n; # 总数 select sum(age) as all_age from n; # 总和 select avg(age) as all_age from n; # 平均值 select max(age) as all_age from n; # 最大值 select min(age) as all_age from n; # 最小值 # 数学函数 select abs(-5); # 绝对值 select bin(15), oct(15), hex(15); # 二进制，八进制，十六进制 SELECT pi(); # 圆周率3.141593 SELECT ceil(5.5); # 大于x的最小整数值6 SELECT floor(5.5); # 小于x的最大整数值5 SELECT greatest(3,1,4,1,5,9,2,6); # 返回集合中最大的值9 SELECT least(3,1,4,1,5,9,2,6); # 返回集合中最小的值1 SELECT mod(5,3); # 余数2 SELECT rand(); # 返回０到１内的随机值，每次不一样 SELECT rand(5); # 提供一个参数(种子)使RAND()随机数生成器生成一个指定的值。 SELECT round(1415.1415); # 四舍五入1415 SELECT round(1415.1415, 3); # 四舍五入三位数1415.142 SELECT round(1415.1415, -1); # 四舍五入整数位数1420 SELECT truncate(1415.1415, 3); # 截短为3位小数1415.141 SELECT truncate(1415.1415, -1); # 截短为-1位小数1410 SELECT sign(-5); # 符号的值负数-1 SELECT sign(5); # 符号的值正数1 SELECT sqrt(9); # 平方根3 SELECT sqrt(9); # 平方根3 # 字符串函数 SELECT concat('a', 'p', 'p', 'le'); # 连接字符串-apple SELECT concat_ws(',', 'a', 'p', 'p', 'le'); # 连接用','分割字符串-a,p,p,le SELECT insert('chinese', 3, 2, 'IN'); # 将字符串'chinese'从3位置开始的2个字符替换为'IN'-chINese SELECT left('chinese', 4); # 返回字符串'chinese'左边的4个字符-chin SELECT right('chinese', 3); # 返回字符串'chinese'右边的3个字符-ese SELECT substring('chinese', 3); # 返回字符串'chinese'第三个字符之后的子字符串-inese SELECT substring('chinese', -3); # 返回字符串'chinese'倒数第三个字符之后的子字符串-ese SELECT substring('chinese', 3, 2); # 返回字符串'chinese'第三个字符之后的两个字符-in SELECT trim(' chinese '); # 切割字符串' chinese '两边的空字符-'chinese' SELECT ltrim(' chinese '); # 切割字符串' chinese '两边的空字符-'chinese ' SELECT rtrim(' chinese '); # 切割字符串' chinese '两边的空字符-' chinese' SELECT repeat('boy', 3); # 重复字符'boy'三次-'boyboyboy' SELECT reverse('chinese'); # 反向排序-'esenihc' SELECT length('chinese'); # 返回字符串的长度-7 SELECT upper('chINese'), lower('chINese'); # 大写小写 CHINESE chinese SELECT ucase('chINese'), lcase('chINese'); # 大写小写 CHINESE chinese SELECT position('i' IN 'chinese'); # 返回'i'在'chinese'的第一个位置-3 SELECT position('e' IN 'chinese'); # 返回'i'在'chinese'的第一个位置-5 SELECT strcmp('abc', 'abd'); # 比较字符串，第一个参数小于第二个返回负数- -1 SELECT strcmp('abc', 'abb'); # 比较字符串，第一个参数大于第二个返回正数- 1 # 时间函数 SELECT current_date, current_time, now(); # 2018-01-13 12:33:43 2018-01-13 12:33:43 SELECT hour(current_time), minute(current_time), second(current_time); # 12 31 34 SELECT year(current_date), month(current_date), week(current_date); # 2018 1 1 SELECT quarter(current_date); # 1 SELECT monthname(current_date), dayname(current_date); # January Saturday SELECT dayofweek(current_date), dayofmonth(current_date), dayofyear(current_date); # 7 13 13 # 控制流函数 SELECT if(3>2, 't', 'f'), if(3 &lt; = != &lt;> >= &lt;= 二、按逻辑表达式筛选 逻辑运算符： 作用：用于连接条件表达式 &amp;&amp; || ! and or not &amp;&amp;和and：两个条件都为true，结果为true，反之为false ||或or： 只要有一个条件为true，结果为true，反之为false !或not： 如果连接的条件本身为false，结果为true，反之为false 三、模糊查询 like between and in is null */ # 1. 查询年龄大于22的用户信息 SELECT * FROM `t_user` WHERE `age` > 22; # 2. 查询年龄不等于22的用户和密码 SELECT `user_name`, `password` FROM `t_user` WHERE `age` &lt;> 20; # 查询年龄在21到23之间的用户名和密码 SELECT `user_name`, `password` FROM `t_user` WHERE `age` >=21 AND `age` &lt;= 23; SELECT `user_name`, `password`, `age` FROM `t_user` WHERE `age` BETWEEN 21 AND 23; # 查询年龄21和23的用户名和密码 SELECT `user_name`, `password` FROM `t_user` WHERE `age` IN(21, 23); # 模糊查询 like /* 特点： ①一般和通配符搭配使用 通配符： % 任意多个字符,包含0个字符 _ 任意单个字符 */ # 查询用户名包字符a的用户信息 SELECT * FROM `t_user` WHERE `user_name` LIKE '%a%'; SELECT * FROM `t_user` WHERE `user_name` LIKE '_a%'; 排序查询/* 排序 select 查询列表 from 表 order by 排序列表 asc|desc. asc:升序，desc降序 可以放单个字段，也可以放多个字段，可以表达式，也可以放函数 order by 放最后 除limit语句 */ 题目： #1.查询员工的姓名和部门号和年薪，按年薪降序 按姓名升序 SELECT last_name, department_id, salary*12*(1+IFNULL(commission_pct,0)) 年薪 FROM employees ORDER BY 年薪 DESC, last_name ASC; #2.选择工资不在8000到17000的员工的姓名和工资，按工资降序 SELECT last_name , salary FROM employees WHERE salary NOT BETWEEN 8000 AND 17000 ORDER BY salary DESC ; #3.查询邮箱中包含e的员工信息，并先按邮箱的字节数降序，再按部门号升序 SELECT *,LENGTH(email) FROM employees WHERE email LIKE '%e%' ORDER BY LENGTH(email) DESC,department_id ASC; 常见函数/* 概念：类似于java的方法，将一组逻辑语句封装在方法体中，对外暴露方法名 好处：1、隐藏了实现细节 2、提高代码的重用性 调用：select 函数名(实参列表) 【from 表】; 特点： ①叫什么（函数名） ②干什么（函数功能） 分类： 1、单行函数 如 concat、length、ifnull等 2、分组函数 功能：做统计使用，又称为统计函数、聚合函数、组函数 常见函数： 一、单行函数 字符函数： length:获取字节个数(utf-8一个汉字代表3个字节,gbk为2个字节) concat substr instr trim upper lower lpad rpad replace 数学函数： round ceil floor truncate mod 日期函数： now curdate curtime year month monthname day hour minute second str_to_date date_format 其他函数： version database user 控制函数 if case */ # 1. 字符函数 SELECT LENGTH('join'); SELECT LENGTH('张三丰hahaha'); SHOW VARIABLES LIKE '%char%'; # 2. 拼接字符串 SELECT CONCAT(`user_name`, '_', `password`) infos FROM t_user; #3.upper、lower SELECT UPPER('john'); SELECT LOWER('joHn'); #示例：将姓变大写，名变小写，然后拼接 SELECT CONCAT(UPPER(`user_name`), LOWER(`user_name`)) infos FROM t_user; #4.substr、substring #注意：索引从1开始 #截取从指定索引处后面所有字符 SELECT SUBSTR('我是dreamcat',7) out_put; #截取从指定索引处指定字符长度的字符 SELECT SUBSTR('我是dreamcat',1,3) out_put; SELECT CONCAT(UPPER(SUBSTR(`user_name`, 1, 1)), '_', LOWER(SUBSTR(`user_name`, 2))) out_put FROM t_user; #5.instr 返回子串第一次出现的索引，如果找不到返回0 SELECT INSTR('我是dreamcat', 'dream') out_put; #6.trim SELECT LENGTH( TRIM( ' dreamcat. ' ) ) out_put; SELECT TRIM('aa' FROM 'aaaaaaaaa张aaaaaaaaaaaa翠山aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa') AS out_put; #7.lpad 用指定的字符实现左填充指定长度 SELECT LPAD('殷素素',5,'*') AS out_put; #8.rpad 用指定的字符实现右填充指定长度 SELECT RPAD('殷素素',12,'ab') AS out_put; #9.replace 替换 SELECT REPLACE('周芷若周芷若周芷若周芷若张无忌爱上了周芷若','周芷若','赵敏') AS out_put; #二、数学函数 #round 四舍五入 SELECT ROUND(-1.55); SELECT ROUND(1.567,2); #ceil 向上取整,返回>=该参数的最小整数 SELECT CEIL(-1.02); SELECT CEIL(1.02); #floor 向下取整，返回&lt;=该参数的最大整数 SELECT FLOOR(-9.99); SELECT FLOOR(9.99); #truncate 截断 SELECT TRUNCATE(1.69999,1); #mod取余 SELECT MOD(10,-3); SELECT 10%3; #三、日期函数 #now 返回当前系统日期+时间 SELECT NOW(); #curdate 返回当前系统日期，不包含时间 SELECT CURDATE(); #curtime 返回当前时间，不包含日期 SELECT CURTIME(); #可以获取指定的部分，年、月、日、小时、分钟、秒 SELECT YEAR(NOW()) 年; SELECT YEAR('1998-1-1') 年; SELECT MONTH(NOW()) 月; SELECT MONTHNAME(NOW()) 月; #str_to_date 将字符通过指定的格式转换成日期 SELECT STR_TO_DATE('1998-3-2','%Y-%c-%d') AS out_put; #date_format 将日期转换成字符 SELECT DATE_FORMAT(NOW(),'%y年%m月%d日') AS out_put; #四、其他函数 SELECT VERSION(); SELECT DATABASE(); SELECT USER(); #五、流程控制函数 #1.if函数： if else 的效果 SELECT IF(10&lt;5,'大','小'); SELECT *, IF(`age` &lt; 24, 'young', 'old') 'new age' FROM t_user; #2.case函数的使用一： switch case 的效果 /* java中 switch(变量或表达式){ case 常量1：语句1;break; ... default:语句n;break; } mysql中 case 要判断的字段或表达式 when 常量1 then 要显示的值1或语句1; when 常量2 then 要显示的值2或语句2; ... else 要显示的值n或语句n; end */ SELECT *, CASE `age` WHEN 21 THEN 21+1 WHEN 22 THEN 22+2 WHEN 23 THEN 23+3 ELSE age END 'NEW age' FROM t_user; 题目： #1. 显示系统时间(注：日期+时间) SELECT NOW(); #2. 查询员工号，姓名，工资，以及工资提高百分之20%后的结果（new salary） SELECT employee_id , last_name , salary , salary * 1.2 'new salary' FROM employees; #3. 将员工的姓名按首字母排序，并写出姓名的长度（length） SELECT LENGTH (last_name) 长度, SUBSTR(last_name,1,1) 首字母, last_name FROM employees ORDER BY 首字母; #4. 做一个查询，产生下面的结果 #&lt;last_name> earns &lt;salary> monthly but wants &lt;salary*3> #Dream Salary #King earns 24000 monthly but wants 72000 SELECT CONCAT(last_name, ' earns ', salary, ' monthly but wants ', salary * 3) AS 'Dream Salary' FROM employees WHERE salary = 24000; 分组函数#二、分组函数 /* 功能：用作统计使用，又称为聚合函数或统计函数或组函数 分类： sum 求和、avg 平均值、max 最大值 、min 最小值 、count 计算个数 特点： 1、sum、avg一般用于处理数值型 max、min、count可以处理任何类型 2、以上分组函数都忽略null值 3、可以和distinct搭配实现去重的运算 4、count函数的单独介绍 一般使用count(*)用作统计行数 5、和分组函数一同查询的字段要求是group by后的字段 */ #1、简单 的使用 SELECT SUM(age) FROM t_user; SELECT AVG(age) FROM t_user; SELECT MIN(age) FROM t_user; SELECT MAX(age) FROM t_user; SELECT COUNT(age) FROM t_user; SELECT SUM(user_name) FROM t_user; SELECT AVG(user_name) FROM t_user; SELECT COUNT(user_name) FROM t_user; SELECT SUM(DISTINCT password ) FROM t_user; SELECT COUNT(DISTINCT password) FROM t_user; SELECT COUNT(*) FROM t_user; SELECT COUNT(1) FROM t_user; 分组查询 #进阶5：分组查询 /* 语法： select 查询列表 from 表 【where 筛选条件】 group by 分组的字段 【order by 排序的字段】; 特点： 1、和分组函数一同查询的字段必须是group by后出现的字段 2、筛选分为两类：分组前筛选和分组后筛选 针对的表 位置 连接的关键字 分组前筛选 原始表 group by前 where 分组后筛选 group by后的结果集 group by后 having 问题1：分组函数做筛选能不能放在where后面 答：不能 问题2：where——group by——having 一般来讲，能用分组前筛选的，尽量使用分组前筛选，提高效率 3、分组可以按单个字段也可以按多个字段 4、可以搭配着排序使用 */ SELECT COUNT(*) FROM t_user WHERE password = '123'; SELECT AVG(age), password FROM t_user GROUP BY password; SELECT AVG(age), gender FROM t_user GROUP BY gender ; 题目： #1.查询各job_id的员工工资的最大值，最小值，平均值，总和，并按job_id升序 SELECT MAX(salary), MIN(salary), AVG(salary), SUM(salary) FROM employees GROUP BY job_id ORDER BY job_id ; #2.查询员工最高工资和最低工资的差距（DIFFERENCE） SELECT MAX(salary) - MIN(salary) AS DIFFERENCE FROM employees; #3.查询各个管理者手下员工的最低工资，其中最低工资不能低于6000，没有管理者的员工不计算在内 SELECT MIN(salary), manager_id FROM employees WHERE manager_id IS NOT NULL GROUP BY manager_id HAVING MIN(salary) >= 6000; #4.查询所有部门的编号，员工数量和工资平均值,并按平均工资降序 SELECT department_id , COUNT(*), AVG(salary) FROM employees GROUP BY department_id ORDER BY AVG(salary) DESC; #5.选择具有各个job_id的员工人数 SELECT COUNT(*), job_id FROM employees GROUP BY job_id; 连接查询 #进阶6：连接查询 /* 含义：又称多表查询，当查询的字段来自于多个表时，就会用到连接查询 笛卡尔乘积现象：表1 有m行，表2有n行，结果=m*n行 发生原因：没有有效的连接条件 如何避免：添加有效的连接条件 分类： 按年代分类： sql92标准:仅仅支持内连接 sql99标准【推荐】：支持内连接+外连接（左外和右外）+交叉连接 按功能分类： 内连接： 等值连接 非等值连接 自连接 外连接： 左外连接 右外连接 全外连接 交叉连接 */ #1.显示所有员工的姓名，部门号和部门名称。 SELECT e.last_name , d.department_id , d.department_name FROM employees e , departments d WHERE e.department_id = d.department_id ; #2.查询90号部门员工的job_id和90号部门的location_id SELECT e.job_id , d.location_id FROM employees e , departments d WHERE e.department_id = d.department_id AND e.department_id = 90; #3. 选择所有有奖金的员工的 last_name , department_name , location_id , city SELECT last_name , department_name , l.location_id , city FROM employees e, departments d, locations l WHERE e.department_id = d.department_id AND d.location_id = l.location_id AND e.commission_pct IS NOT NULL; #4.选择city在Toronto工作的员工的 last_name , job_id , department_id , department_name SELECT e.last_name , e.job_id , e.department_id , d.department_name FROM employees e, departments d, locations l WHERE e.department_id = d.department_id AND d.location_id = l.location_id AND l.city = 'Toronto'; #5.查询每个工种、每个部门的部门名、工种名和最低工资 SELECT d.department_name , j.job_title , MIN(e.salary) FROM employees e , departments d , jobs j WHERE e.department_id = d.department_id AND e.job_id = j.job_id GROUP BY d.department_name , j.job_title ; #6.查询每个国家下的部门个数大于2的国家编号 SELECT l.country_id , COUNT(*) FROM locations l , departments d WHERE d.location_id = l.location_id GROUP BY country_id HAVING COUNT(*) > 2; #7 、选择指定员工的姓名，员工号，以及他的管理者的姓名和员工号，结果类似于下面的格式 SELECT e.last_name employees, e.employee_id \"Emp#\", m.last_name manager, m.employee_id \"Mgr#\" FROM employees e, employees m WHERE e.manager_id = m.employee_id AND e.last_name = 'kochhar'; #二、sql99语法 /* 语法： select 查询列表 from 表1 别名 【连接类型】 join 表2 别名 on 连接条件 【where 筛选条件】 【group by 分组】 【having 筛选条件】 【order by 排序列表】 分类： 内连接（★）：inner 外连接 左外(★):left 【outer】 右外(★)：right 【outer】 全外：full【outer】 交叉连接：cross */ #一）内连接 /* 语法： select 查询列表 from 表1 别名 inner join 表2 别名 on 连接条件; 分类： 等值 非等值 自连接 特点： ①添加排序、分组、筛选 ②inner可以省略 ③ 筛选条件放在where后面，连接条件放在on后面，提高分离性，便于阅读 ④inner join连接和sql92语法中的等值连接效果是一样的，都是查询多表的交集 */ #二、外连接 /* 应用场景：用于查询一个表中有，另一个表没有的记录 特点： 1、外连接的查询结果为主表中的所有记录 如果从表中有和它匹配的，则显示匹配的值 如果从表中没有和它匹配的，则显示null 外连接查询结果=内连接结果+主表中有而从表没有的记录 2、左外连接，left join左边的是主表 右外连接，right join右边的是主表 3、左外和右外交换两个表的顺序，可以实现同样的效果 4、全外连接=内连接的结果+表1中有但表2没有的+表2中有但表1没有的 */ 子查询#进阶7：子查询 /* 含义： 出现在其他语句中的select语句，称为子查询或内查询 外部的查询语句，称为主查询或外查询 分类： 按子查询出现的位置： select后面： 仅仅支持标量子查询 from后面： 支持表子查询 where或having后面：★ 标量子查询（单行） √ 列子查询 （多行） √ 行子查询 exists后面（相关子查询） 表子查询 按结果集的行列数不同： 标量子查询（结果集只有一行一列） 列子查询（结果集只有一列多行） 行子查询（结果集有一行多列） 表子查询（结果集一般为多行多列） */ #一、where或having后面 /* 1、标量子查询（单行子查询） 2、列子查询（多行子查询） 3、行子查询（多列多行） 特点： ①子查询放在小括号内 ②子查询一般放在条件的右侧 ③标量子查询，一般搭配着单行操作符使用 > &lt; >= &lt;= = &lt;> 列子查询，一般搭配着多行操作符使用 in、any/some、all ④子查询的执行优先于主查询执行，主查询的条件用到了子查询的结果 */ 分页查询#进阶8：分页查询 ★ /* 应用场景：当要显示的数据，一页显示不全，需要分页提交sql请求 语法： select 查询列表 from 表 【join type join 表2 on 连接条件 where 筛选条件 group by 分组字段 having 分组后的筛选 order by 排序的字段】 limit 【offset,】size; offset要显示条目的起始索引（起始索引从0开始） size 要显示的条目个数 特点： ①limit语句放在查询语句的最后 ②公式 要显示的页数 page，每页的条目数size select 查询列表 from 表 limit (page-1)*size,size; size=10 page 1 0 2 10 3 20 */","categories":[{"name":"db","slug":"db","permalink":"http://dreamcat.ink/categories/db/"}],"tags":[{"name":"db","slug":"db","permalink":"http://dreamcat.ink/tags/db/"}]},{"title":"收集常用的网站(持续更新...)","slug":"收集常用的网站(持续更新...)","date":"2019-07-11T01:29:33.000Z","updated":"2020-10-30T13:06:49.625Z","comments":true,"path":"2019/07/11/shou-ji-chang-yong-de-wang-zhan-chi-xu-geng-xin.../","link":"","permalink":"http://dreamcat.ink/2019/07/11/shou-ji-chang-yong-de-wang-zhan-chi-xu-geng-xin.../","excerpt":"","text":"引言 收集一些常用的在线网站，包括在线工具、第三方视频、文章博客、磁力和软件等… 在线工具 在线JSON校验格式工具 正则表达式在线测试 RGB颜色查询对照表 在线正则表达式测试 UrlEncode编码解码 时间戳在线转换 草料二维码生成器 在线Java编辑器 MarkDown转HTML 在线图标下载 Greasy Fork 油猴脚本 科技小c 高清壁纸wall 在线图片尺寸大小修改 Unicode转中文 ppt模版下载 CSDN自助下载 4k壁纸 xclient-mac软件下载 iterm2主题 常用的workflow mac软件下载-1 mac软件下载-2 第三方视频 免费影视网站 动漫-1 动漫-2 BT磁力 磁力导航 网盘搜索 DogeDoge搜索 云盘精灵 盘天才 其他 chromium-575458","categories":[{"name":"web","slug":"web","permalink":"http://dreamcat.ink/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://dreamcat.ink/tags/web/"}]},{"title":"十大经典排序算法","slug":"十大经典排序算法","date":"2019-07-09T13:53:17.000Z","updated":"2020-10-29T15:54:23.946Z","comments":true,"path":"2019/07/09/shi-da-jing-dian-pai-xu-suan-fa/","link":"","permalink":"http://dreamcat.ink/2019/07/09/shi-da-jing-dian-pai-xu-suan-fa/","excerpt":"","text":"​ 经典的排序算法，包括实现源码等。 排序性能和复杂度 不同情况下排序选择 排序场景 排序效率 Random 希尔&gt;快排&gt;归并 Few unique 快排&gt;希尔&gt;归并 Reversed 快排&gt;希尔&gt;归并 Almost sorted 插入排序&gt;基数排序&gt;快排&gt;希尔&gt;归并 总结来看: 快速排序和希尔排序在排序速度上表现是比较优秀的,而归并排序稍微次之. 冒泡排序它会遍历若干次要排序的数列，每次遍历时，它都会从前往后依次的比较相邻两个数的大小；如果前者比后者大，则交换它们的位置。这样，一次遍历之后，最大的元素就在数列的末尾！ 采用相同的方法再次遍历时，第二大的元素就被排列在最大元素之前。重复此操作，直到整个数列都有序为止！ 代码片段public class BubbleSort { public static void main(String[] args) { int[] arr = {5, 2, 3, 1, 4}; System.out.println(\"Bubble sort ...\"); System.out.println(Arrays.toString(arr)); int[] resArr = bubbleSort(arr); System.out.println(Arrays.toString(resArr)); } public static int[] bubbleSort(int[] sourceArray) { int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); for (int i = 1; i &lt; arr.length; i++) { for (int j = 0; j &lt; arr.length - i; j++) { // 如果前面的数比后面的数大，则交换 if (arr[j] > arr[j + 1]) { swap(arr, j, j + 1); } } } return arr; } public static void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } } 选择排序首先在未排序的数列中找到最小(or最大)元素，然后将其存放到数列的起始位置；接着，再从剩余未排序的元素中继续寻找最小(or最大)元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 代码片段public class SelectSort { public static void main(String[] args) { int[] arr = {5, 2, 3, 1, 4}; System.out.println(\"Select sort ...\"); System.out.println(Arrays.toString(arr)); int[] resArr = selectSort(arr); System.out.println(Arrays.toString(resArr)); } public static int[] selectSort(int[] sourceArray) { int [] arr = Arrays.copyOf(sourceArray, sourceArray.length); // N-1轮 for (int i = 0; i &lt; arr.length - 1; i++) { int min = i; // 每轮需要N-i次比较 for (int j = i + 1; j &lt; arr.length; j++) { min = arr[j] &lt; arr[min] ? j : min; // 保存最小值坐标 } // 将找到最小值的坐标与i交换 swap(arr, i, min); } return arr; } public static void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } } 插入排序把n个待排序的元素看成为一个有序表和一个无序表。开始时有序表中只包含1个元素，无序表中包含有n-1个元素，排序过程中每次从无序表中取出第一个元素，将它插入到有序表中的适当位置，使之成为新的有序表，重复n-1次可完成排序过程。 代码片段public class InsertSort { public static void main(String[] args) { int[] arr = {5, 2, 3, 1, 4}; System.out.println(\"Select sort ...\"); System.out.println(Arrays.toString(arr)); int[] resArr = insertSort(arr); System.out.println(Arrays.toString(resArr)); } public static int[] insertSort(int[] sourceArray) { int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); for (int i = 1; i &lt; arr.length; i++) { for (int j = i - 1; j >= 0 &amp;&amp; arr[j] > arr[j + 1]; j--) { swap(arr, j, j + 1); } } return arr; } public static void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } } 希尔排序希尔排序实质上是一种分组插入方法。它的基本思想是: 对于n个待排序的数列，取一个小于n的整数gap(gap被称为步长)将待排序元素分成若干个组子序列，所有距离为gap的倍数的记录放在同一个组中；然后，对各组内的元素进行直接插入排序。 这一趟排序完成之后，每一个组的元素都是有序的。然后减小gap的值，并重复执行上述的分组和排序。重复这样的操作，当gap=1时，整个数列就是有序的。 代码片段import java.util.Arrays; class ShellSort implements Sorts { @Override public int[] sort(int[] sourceArray) { // 复制 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int gap = 1; while (gap &lt; arr.length) gap = gap * 3 + 1; while (gap > 0) { for (int i = gap; i &lt; arr.length; i++) { int tmp = arr[i]; int j = i - gap; while (j >= 0 &amp;&amp; arr[j] > tmp) { arr[j + gap] = arr[j]; j -= gap; } arr[j + gap] = tmp; } gap = (int) Math.floor(gap / 3); } return arr; } } 归并排序 分解 – 将当前区间一分为二，即求分裂点 mid = (low + high)/2; 求解 – 递归地对两个子区间a[low…mid] 和 a[mid+1…high]进行归并排序。递归的终结条件是子区间长度为1。 合并 – 将已排序的两个子区间a[low…mid]和 a[mid+1…high]归并为一个有序的区间a[low…high]。 代码片段import java.util.Arrays; /** * @program JavaBooks * @description: 归并排序 * @author: mf * @create: 2019/08/13 19:38 */ public class MergeSort { public static void main(String[] args) { int[] arr = {10,34,5,3,4,2,1}; System.out.println(Arrays.toString(arr)); mergeSort(arr); System.out.println(Arrays.toString(arr)); } public static void mergeSort(int[] arr) { if (arr == null || arr.length &lt; 2) { return; } mergeSort(arr, 0, arr.length - 1); } public static void mergeSort(int[] arr, int left, int right) { if (left == right) return; int mid = left + ((right - left) >> 1); // left mergeSort(arr, left, mid); // right mergeSort(arr, mid + 1, right); // merge merge(arr, left, mid, right); } public static void merge(int[] arr, int left, int mid, int right) { int[] help = new int[right - left + 1]; int i = 0; int p1 = left; int p2 = mid + 1; while (p1 &lt;= mid &amp;&amp; p2 &lt;= right) { help[i++] = arr[p1] &lt; arr[p2] ? arr[p1++] : arr[p2++]; } while(p1 &lt;= mid) { help[i++] = arr[p1++]; } while(p2 &lt;= right) { help[i++] = arr[p2++]; } for (int j = 0; j &lt; help.length; j++) { arr[left + j] = help[j]; } } } 快速排序参考 快速排序-简书 它的基本思想是: 选择一个基准数，通过一趟排序将要排序的数据分割成独立的两部分；其中一部分的所有数据都比另外一部分的所有数据都要小。然后，再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 代码片段public class QuickSort { public static void main(String[] args) { int[] arr = {9, 10, 8, 3, 6}; System.out.println(Arrays.toString(arr)); int[] resArr = quickSort(arr); System.out.println(Arrays.toString(resArr)); } public static int[] quickSort(int[] sourceArr) { int[] arr = Arrays.copyOf(sourceArr, sourceArr.length); return quickSort(arr, 0, arr.length - 1); } public static int[] quickSort(int[] arr, int left, int right) { if (left &lt; right) { int partitionIndex = partition(arr, left, right); // 左半部分递归 quickSort(arr, left, partitionIndex - 1); // 右半部分递归 quickSort(arr, partitionIndex + 1, right); } return arr; } public static int partition(int[] arr, int left, int right) { int pivot = left; int index = pivot + 1; for (int i = index; i &lt;= right; i++) { if (arr[i] &lt; arr[pivot]) { swap(arr, i, index++); } } swap(arr, pivot, index - 1); return index - 1; } public static void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } } 堆排序过程最大堆进行升序排序的基本思想: ① 初始化堆: 将数列a[1…n]构造成最大堆。 ② 交换数据: 将a[1]和a[n]交换，使a[n]是a[1…n]中的最大值；然后将a[1…n-1]重新调整为最大堆。 接着，将a[1]和a[n-1]交换，使a[n-1]是a[1…n-1]中的最大值；然后将a[1…n-2]重新调整为最大值。 依次类推，直到整个数列都是有序的。 代码片段public class HeapSort { public static void main(String[] args) { int[] arr = {3, 5, 1 , 4, 2}; System.out.println(\"Heap Sort...\"); System.out.println(Arrays.toString(arr)); heapSort(arr); System.out.println(Arrays.toString(arr)); } public static void heapSort(int[] arr) { if (arr == null || arr.length &lt; 2) return; for (int i = 0; i &lt; arr.length; i++) { heapInsert(arr, i); // 依次从0～i形成大根堆 } int heapSize = arr.length; swap(arr, 0, --heapSize); while (heapSize > 0) { heapify(arr, 0, heapSize); swap(arr, 0, --heapSize); } } public static void heapInsert(int[] arr, int index) { // 建立大根堆 while (arr[index] > arr[(index - 1) / 2]) { swap(arr, index, (index - 1) / 2); index = (index - 1) / 2; } } public static void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } public static void heapify(int[] arr, int index, int heapSize) { // 调整成大根堆 int left = index * 2 + 1; while (left &lt; heapSize) { int largest = left + 1 &lt; heapSize &amp;&amp; arr[left + 1] > arr[left] ? left + 1 : left; largest = arr[largest] > arr[index] ? largest : index; if (largest == index) { break; // 自己已经是最大了， 直接跳出 } swap(arr, largest, index); index = largest; left = index * 2 + 1; } } } 计数排序过程 花O(n)的时间扫描一下整个序列A，获取最小值min和最大值max 开辟一块新的空间创建新的数组B，长度为（max-min + 1） 数组B中index的元素记录的值是A中某元素出现的次数 最后输出目标整数序列，具体的逻辑是遍历数组B，输出相应元素以及对应的个数 代码片段import java.util.Arrays; class CountingSort implements Sorts { @Override public int[] sort(int[] sourceArray) { // 复制 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int maxValue = getMaxValue(arr); // 关键在这里 return countingSort(arr, maxValue); } private int[] countingSort(int[] arr, int maxValue) { int bucketlen = maxValue + 1; int[] bucket = new int[bucketlen]; for (int value: arr) bucket[value]++; int sortedIndex = 0; for (int j = 0; j &lt; bucketlen; j++) { while (bucket[j] > 0) { arr[sortedIndex++] = j; bucket[j]--; } } return arr; } private int getMaxValue(int[] arr) { int maxValue = arr[0]; for (int value: arr) { if (maxValue &lt; value) maxValue = value; } return maxValue; } } 桶排序假设待排序的数组a中共有N个整数，并且已知数组a中数据的范围[0, MAX)。在桶排序时，创建容量为MAX的桶数组r，并将桶数组元素都初始化为0；将容量为MAX的桶数组中的每一个单元都看作一个”桶”。 在排序时，逐个遍历数组a，将数组a的值，作为”桶数组r”的下标。当a中数据被读取时，就将桶的值加1。例如，读取到数组a[3]=5，则将r[5]的值+1。 代码片段import java.util.Arrays; class BucketSort implements Sorts { @Override public int[] sort (int[] sourceArray) { // 复制 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); return BucketSort(arr, 5); } private int[] BucketSort(int[] arr, int bucketSize) { if (arr.length == 0) return arr; int minValue = arr[0]; int maxValue = arr[0]; for (int value: arr) { if (value &lt; minValue) minValue = value; if (value > maxValue) maxValue = value; } int bucketCount = (int) Math.floor((maxValue - minValue) / bucketSize) + 1; int[][] buckets = new int[bucketCount][0]; // 利用映射函数将数据分配到各个桶中 for (int i = 0; i &lt; arr.length; i++) { int index = (int) Math.floor((arr[i] - minValue) / bucketSize); buckets[index] = arrAppend(buckets[index], arr[i]); } int arrIndex = 0; for (int[] bucket : buckets) { if (bucket.length &lt;= 0) continue; // 对每个桶进行排序，这里使用了插入排序 InsertSort insertSort = new InsertSort(); bucket = insertSort.sort(bucket); for (int value : bucket) arr[arrIndex++] = value; } return arr; } private int[] arrAppend(int[] arr, int value) { arr = Arrays.copyOf(arr, arr.length + 1); arr[arr.length - 1] = value; return arr; } } 基数排序它的基本思想是: 将整数按位数切割成不同的数字，然后按每个位数分别比较。 具体做法是: 将所有待比较数值统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列。 代码片段import java.util.Arrays; class RadixSort implements Sorts { @Override public int[] sort(int[] sourceArray) { // 复制 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int maxDigit = getMaxDigit(arr); return radixSort(arr, maxDigit); } private int getMaxDigit(int[] arr){ int maxValue = getMaxValue(arr); return getNumLengtht(maxValue); } private int getMaxValue(int[] arr) { int maxValue = arr[0]; for (int value : arr) { if (maxValue &lt; value) maxValue = value; } return maxValue; } private int getNumLengtht(long num) { if (num == 0) return 1; int lengtht = 0; for (long temp = num; temp != 0l temp /= 10) lengtht ++; return lengtht; } private int[] radixSort(int[] arr, int maxDigit) { int mod = 10; int dev = 1; for (int i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) { // 考虑负数的情况，这里扩展一倍队列数，其中[0-9]对应数，[10-19]对应正数(bucket + 10) int[][] counter = new int[mod * 2][0]; for (int j = 0; j &lt; arr.length; j++) { int bucket = ((arr[j] % mod) / dev) + mod; counter[bucket] = arrayAppend(counter[bucket], arr[j]); } int pos = 0; for (int[] bucket : counter) { for (int value : bucket) arr[pos++] = value; } } return arr; } private int[] arrayAppend(int[] arr, int value) { arr = Arrays.copyOf(arr, arr.length + 1); arr[arr.length - 1] = value; return arr; } }","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://dreamcat.ink/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://dreamcat.ink/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"windows、mac和linux安装Java","slug":"windows、mac和linux安装Java","date":"2019-07-08T15:21:46.000Z","updated":"2020-10-30T12:45:27.735Z","comments":true,"path":"2019/07/08/windows-mac-he-linux-an-zhuang-java/","link":"","permalink":"http://dreamcat.ink/2019/07/08/windows-mac-he-linux-an-zhuang-java/","excerpt":"","text":"准备工作 官网下载链接 windows10 mac linux（ubuntu和centos） windows10 下载安装包，选择windows x64 我一般在c盘创建Java目录，其次在该目录下创建jdk和jre目录 双击exe文件安装，首先安装的是jdk，选择C:\\Java\\jdk，当安装完毕之后，自动弹出安装jre，C:\\Java\\jre 配置环境，首先win+s，搜索环境变量，在用户变量下新建 变量名：JAVA_HOME 变量值：C:\\Java\\jdk 再新建 变量名：CLASSPATH 变量值：.;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar; 在Path环境变量下，新建两个，分别是 %JAVA_HOME%\\bin %JAVA_HOME%\\jre\\bin win+r 输入cmd 确定，在命令窗口输入java -version 则出现java version版本号 javac utf-8编码问题的话，可添加环境变量 JAVA_TOOL_OPTIONS值为Dfile.encoding=UTF-8 mac mac自带java，自带就够了 使用homebrew，非常简便 终端输入brew tap caskroom/versions brew cask install java8 or brew cask install caskroom/versions/java8 brew安装好自动会配置环境 ubuntu16.04 or centos7 Ubuntu和centos安装步骤差不多一样的 官网下载linux-x64.tar.gz 将xxx.linux-x64.tar.gz 更改为java java移动到/usr/local/ 根目录下vi _bashrc 添加： export JAVA_HOME=/usr/local/java export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib export PATH=${JAVA_HOME}/bin:${PATH} 记的source java -version","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"java","slug":"java","permalink":"http://dreamcat.ink/tags/java/"}]},{"title":"命令行的奇淫技巧","slug":"命令行的奇淫技巧","date":"2019-06-08T14:05:09.000Z","updated":"2020-10-30T13:00:09.873Z","comments":true,"path":"2019/06/08/ming-ling-xing-de-qi-yin-ji-qiao/","link":"","permalink":"http://dreamcat.ink/2019/06/08/ming-ling-xing-de-qi-yin-ji-qiao/","excerpt":"","text":"引言常用的命令行技巧… 前言 主要针对常用的一些命令行的使用 主要是unix的操作系统 常用 cd:这个就不需要多讲了 clear:这个也不需要多讲了 mkdir:当前目录创建文件夹 touch:当前目录下创建文件 ls:查看目录下的文件 ls -a:查看文件，包括隐藏文件 ls -l:详细查看文件 top:查看cpu和内存等 df -h:查看各个磁盘使用的状态 du -hd1:查看当前目录下文件的大小 du -h:查看当前目录下文件的大小，包括子目录 nautilus ./:打开当前文件管理器 pwd:查看当前路径 w:查看机器运行的时间 统计文件数目lsls -l | wc -l计数当前目录的文件和文件夹。 它会计算所有的文件和目录。 ls -la | wc -l统计当前目录包含隐藏文件和目录在内的文件和文件夹。 findfind . -type f | wc -l递归计算当前目录的文件，包括隐藏文件。 find . type d | wc -l递归计算包含隐藏目录在内的目录数。 find . -name &#39;*.txt&#39; | wc -l根据文件扩展名计数文件数量。 这里我们要计算 .txt 文件。 日常使用 可以通过Tab键实现自动补全参数 使用ctrl-r搜索命令行的历史记录 按下ctrl-w删除键入的最后的一个单词 按下ctrl-u可以删除行内光标所在位置之前的内容，alt-b和alt-f可以在以单词为单位移动光标，ctrl-a可以将光标移至行首，ctrl-e可以将光标移至行尾 回到前一个工作路径：cd - pstree -p以一种优雅的方式展示进程树 kill -STOP[pid]停止一个进程 使用nohup或disown使一个后台进程持续运行 eg：nohup python -u demo.py &gt; ./logs/demo.log 2&gt;&amp;1 &amp; 使用netstat -lntp检查哪些进程在监听端口 使用uptime或w查看系统已经运行对长时间 使用alias来常见常用命令的快捷形式，例如:alias ll=&#39;ls -latr&#39;创建了一个新的命令别名，可以把别名放在~./bashrc 文件及数据处理 在当前目录下通过文件名查找一个文件，使用类似于这样的命令：find . -name &#39;*something&#39; 使用wc去计算新行数-l，字符数-m，单词数-w以及字节数-c，例如ls | wc -l 、ls -lR | grep &quot;^-&quot; | wc -l du -sh *查看某个文件的大小 du -h --max-depth=1查看当前目下文件的大小 du -hd1查看当前目录下文件的大小–mac df -hl 查看磁盘情况 和服务器交互 下载文件scp username@servername:/path/filename /var/www/local_dir（本地目录） 上传文件scp /path/filename username@servername:/path 下载文件夹scp -r username@servername:/path/filename /var/www/local_dir（本地目录） 下载文件夹scp -r /path/filename username@servername:/path 若是更改端口， 则前面 加上-p bash互换 zsh切bashchsh -s /bin/bash bash切zshchsh -s /bin/zsh 有一些挺有用 bc计算器 cal漂亮的日历 tree以树的形式显示路径和文件 watch重复运行同一个命令，展示结果有更改的部分 持续补充…","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"终端","slug":"终端","permalink":"http://dreamcat.ink/tags/%E7%BB%88%E7%AB%AF/"}]},{"title":"Ubuntu16.04 优化与配置(深度学习可选)","slug":"Ubuntu16-04-优化与配置-深度学习可选","date":"2019-06-01T05:27:10.000Z","updated":"2020-10-29T15:47:27.431Z","comments":true,"path":"2019/06/01/ubuntu16-04-you-hua-yu-pei-zhi-shen-du-xue-xi-ke-xuan/","link":"","permalink":"http://dreamcat.ink/2019/06/01/ubuntu16-04-you-hua-yu-pei-zhi-shen-du-xue-xi-ke-xuan/","excerpt":"","text":"引言Ubuntu优化和配置… 准备工作 提前已安装 深度学习（可选安装） 优化1. 卸载 卸载自带officesudo apt-get remove libreoffice-common 卸载亚马逊链接sudo apt-get remove unity-webapps-common 2. 主题美化 安装unity-tweak-tool工具sudo apt-get install unity-tweak-tool 安装主题 sudo add-apt-repository ppa:noobslab/themes sudo apt-get update sudo apt-get install flatabulous-theme 安装图标 sudo add-apt-repository ppa:noobslab/icons sudo apt-get update sudo apt-get install ultra-flat-icons 进入unity-tweak-tool软件修改主题和图标即可 3.终端美化 gitsudo apt-get install git zshsudo apt-get install zsh 切换bashchsh -s /bin/zsh 接下来参考我的博客-终端美化 必要软件1. vim sudo apt-get install vim 个人配置~/.vimrc \" 启用语法分析着色 syntax enable syntax on \" 颜色主题 colorscheme default \" 当前行高亮 au WinLeave * set nocursorline nocursorcolumn au WinEnter * set cursorline set cursorline \" 设定Tab表示的空格数 set tabstop=4 \" 设定输入Tab表示的空格数 set softtabstop=4 \" 将Tab视为若干空格 set expandtab \" 将Tab视为若干空格 set backspace=2 \" 显示行号 set number \" 右下角显示待补全命令 set showcmd \" 搜索字符串时高亮所有结果，:nohlsearch取消高亮 set hlsearch \"设置匹配模式,类似当输入一个左括号时会匹配相应的那个右括号 set showmatch \"设置历史记录条数 set history=1000 \"设置搜索时忽略大小写 set ignorecase \"实时显示搜索结果 set incsearch \"设置在vim中可以使用鼠标 set mouse=a \"检测文件类型 filetype on \"针对不同的文件采取不同的缩进方式 filetype indent on \"自动判断编码时 依次尝试以下编码 set fileencodings=ucs-bom,utf-8,cp936,gb18030,big5,euc-jp,euc-kr,latin1 \"括号自动补全 自己可选 \"inoremap ( ()i \"inoremap [ []i \"inoremap { {}i \"inoremap < i \"inoremap \" \"\"i \"inoremap ' '' i \"指定配色方案为256色 set t_Co=256 \"设置自动对齐空格数 set shiftwidth=4 \"直接使用y p进行系统级复制粘贴 set clipboard=unnamedplus 2. 文件夹名称 切英文export LANG=en_US 更改xdg-user-dirs-gtk-update 点击更新名称 切中文export LANG=zh_CN.UTF-8 更改xdg-user-dirs-gtk-update 保留旧的名称-并且勾选不再提示 3. 搜狗输入法 官网 sudo dpkg -i xxxxxx sudo apt-get install -f 打开”系统设置-&gt;语言支持-&gt;安装” 输入密码 然后把里面的IBus改为fcitx，然后关掉，然后重启电脑。 可配置输入法的顺序 4. dock 更改底部gsettings set com.canonical.Unity.Launcher launcher-position Bottom 5. chrome linux-chrome-73 sudo dpkg -i xxxxxx sudo apt-get install -f 安装google访问助手 助手 可能会出现无法安装插件 终端google-chrome --enable-easy-off-store-extension-install 此时弹出浏览器打开扩展程序拖进来即可 python安装python3.6 参考文章 安装pip3 sudo apt-get install python3-pip 安装virtualenv sudo pip3 install virtualenv virtualenvwrapper cuda9+cudnn7.4 已知用户可装好驱动nvidia-smi如果正常，继续往下看 cuda9 下载runfile local 参考文章 deepin 自带显卡切换软件，切换使用闭源驱动 安装nvdia-smi sudo apt install nvidia-smi 安装cuda9.1 sudo apt install nvidia-cuda-dev nvidia-cuda-toolkit nvidia-nsight nvidia-visual-profiler 安装cudnn7.3.1 https://developer.nvidia.com/cudnn 管理员解压cudnn sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 使用gedit ~/.bashrc,在文末添加环境变量： export LD_LIBRARY_PATH=&quot;/usr/local/cuda/lib64:$LD_LIBRARY_PATH&quot; deepin文件管理器（super+e）被替换为vscode 则终端 gio mime inode/directory dde-file-manager.desktop vscode 我的博客-vscode配置 更新vscode wget https://vscode-update.azurewebsites.net/latest/linux-deb-x64/stable -O /tmp/code_latest_amd64.deb sudo dpkg -i /tmp/code_latest_amd64.deb 耳机 sudo apt install pavucontrol 打开pavucontrol 软件“配置”菜单栏下，HDA NVIDIA的下拉选框中，拉到最底部，选择“关” 然后在内置音频的下拉选框中，选择模拟立体声双工。 键盘背光灯 终端：打开xset led named &#39;Scroll Lock&#39; 终端：关闭xset -led named &#39;Scroll Lock&#39; 修改分区 sudo apt-get install gparted 需要u盘启动 选择第一个试用ubuntu 终端sudo gparted 比如压缩/ 扩充/home 点击/，右键选择更改大小/移动 然后可以修改新大小等 点击调整大小-&gt;点击编辑-&gt;应用全部操作 接着将灰色区域移动/home旁边 然后右键点击/home，选择更改大小/移动，将箭头移动到灰色区域 点击调整大小-&gt;点击编辑-&gt;应用全部操作","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"http://dreamcat.ink/tags/ubuntu/"}]},{"title":"vscode配置主题、字体和常用插件","slug":"vscode配置主题、字体和常用插件","date":"2019-05-29T13:44:21.000Z","updated":"2020-10-30T12:45:15.902Z","comments":true,"path":"2019/05/29/vscode-pei-zhi-zhu-ti-zi-ti-he-chang-yong-cha-jian/","link":"","permalink":"http://dreamcat.ink/2019/05/29/vscode-pei-zhi-zhu-ti-zi-ti-he-chang-yong-cha-jian/","excerpt":"","text":"​ vscode配置主题、字体和常用插件 准备工作 vscode安装 vscode常用插件 vscode主题 vscode字体 vscode(ubuntu update) vscode 代码片段 平台：windows、Mac和ubuntu vscode安装1. windows vscode官网 下载安装即可 2. mac 同上 3. ubuntu 同上 vscode常用插件 Beautify（格式化插件js、html、css和json等） Bracket Pair Colorizer（括号匹配） Chinese (Simplified) Language（中文简体） Path Intellisense（路径补全） vscode-icons（美化图标） Python（python 你懂的） remote-ssh(远程) 前端暂时没需求 vscode主题 推荐One Dark Pro 左下角设置按钮选择主题-&gt;One Dark Pro vscode字体 推荐Fira Code 下载进入ttf全部双击安装即可 进入设置右上角有个{} 替换以下内容（我个人简单配置） { \"workbench.colorTheme\": \"One Dark Pro\", \"editor.fontSize\": 16, \"workbench.startupEditor\": \"newUntitledFile\", \"window.zoomLevel\": 0, \"explorer.confirmDelete\": false, \"git.ignoreMissingGitWarning\": true, \"python.pythonPath\": \"/Users/mf/PyEnvs/base/bin/python\", // python的基本环境 \"python.venvPath\": \"~/PyEnvs\", // virtualenv 虚拟环境 \"git.enableSmartCommit\": true, \"explorer.confirmDragAndDrop\": false, \"[json]\": { \"editor.defaultFormatter\": \"HookyQR.beautify\" }, \"workbench.iconTheme\": \"vscode-icons\", \"editor.fontFamily\": \"Fira Code\", \"editor.fontLigatures\": true } vscode update 终端：wget https://vscode-update.azurewebsites.net/latest/linux-deb-x64/stable -O /tmp/code_latest_amd64.deb 再次执行sudo dpkg -i /tmp/code_latest_amd64.deb 关闭vscode，重新打开即可 vscode 代码片段 打开设置-&gt;用户代码片段（第一次可能要创建名字，创建即可） 个人的代码 { \"print to info\": { \"prefix\": \"info\", \"body\": [ \"# __author__: Mai feng\", \"# __file_name__: $TM_FILENAME\", \"# __time__: $CURRENT_YEAR:$CURRENT_MONTH:$CURRENT_DATE:$CURRENT_HOUR:$CURRENT_MINUTE\", ], \"description\": \"info output to console\" }, ## eg: input \"info\" 即可 \"print to note\": { \"prefix\": \"note\", \"body\": [ \"'''$1\", \"'''\" ], \"description\": \"note output to console\" } } remote-ssh远程 安装插件 不管是什么系统，先创建config文件，例如config 文件内容： Host mylab # 名称而已 HostName xxx # 远程ip User mf # 远程账户 Port 2222 # 远程端口 Host lab408 HostName xxx User pch Port 2222 IdentityFile /Users/mf/.ssh/id_rsa # 私钥 免密专用 在vscode打开软件命令界面输入remote-ssh:open，加载config文件即可","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"vscode","slug":"vscode","permalink":"http://dreamcat.ink/tags/vscode/"}]},{"title":"windows、mac和linux安装mongodb","slug":"windows、mac和linux安装mongodb","date":"2019-05-29T12:24:24.000Z","updated":"2020-10-30T12:45:34.045Z","comments":true,"path":"2019/05/29/windows-mac-he-linux-an-zhuang-mongodb/","link":"","permalink":"http://dreamcat.ink/2019/05/29/windows-mac-he-linux-an-zhuang-mongodb/","excerpt":"","text":"准备工作 mongodb官网 windows10 mac ubuntu Centos7 安装1. windows 进入官网滚动网页至Try MongoDB for free 选择Community Server社区版本，并选择Windows的安装版本 可以在C:\\mongodb中手动创建两个空文件夹 C:\\mongodb\\data\\db C:\\mongodb\\log 并在C:\\mongodb\\log下面创建一个空的mongo.log 打开cmd cd c:\\mongodb\\bin mongod --dbpath C:\\mongodb\\data\\db --logpath=C:\\mongodb\\log\\mongodb.log --logappend 使用cmd命令窗口，并进入至c:\\mongodb\\bin目录，运行命令 mongo 将mongodb作为windows服务启动 mongod --dbpath C:\\mongodb\\data\\db --logpath=C:\\mongodb\\log\\mongodb.log --logappend --install --serviceName \"MongoDB\" 使用配置文件启动mongodb服务：在c:\\mongodb\\config创建一个文件mongodb.conf，加入配置文件与直接运行命令的效果是一样的 dbpath=C:\\mongodb\\data\\db # 数据库文件 logpath=C:\\mongodb\\log\\mongodb.log # 日志文件 logappend=true # 日志采用追加模式，配置后mongodb日志会追加到现有的日志文件，不会重新创建一个新文件 journal=true # 启用日志文件，默认启用 quiet=true # 这个选项可以过滤掉一些无用的日志信息，若需要调试使用请设置为 false port=27017 # 端口号 默认为 27017 sc create MongoDB binPath= \"C:\\mongodb\\bin\\mongod.exe --service --config=C:\\mongodb\\config\\mongodb.conf\" #可以在任何目录运行该命令 sc delete MongoDB 将mongodb加入至环境变量 将mongodb的bin目录加入至path环境变量中 2. mac 打开终端 brew install mongodb 配置 前往前往文件夹/usr/local/Cellar/mongodb/x.x.x 创建data和log文件夹 在log文件下创建mongodb.log文件 随意某处创建一个配置文件（mongodb.conf）并打开 port=27017 dbpath=/usr/local/Cellar/mongodb/3.4.4/data/ logpath=/usr/local/Cellar/mongodb/3.4.4/log/mongodb.log fork =true #port: 数据库服务使用端口 #dbpath: 数据存放的文件位置 #logpath: 日志文件的存放位置 #fork: 后台守护进程运行 启动数据库 mongod -f mongodb.conf 3. centos7 vim /etc/yum.repos.d/mongodb-org-3.4.repo [mongodb-org-3.4] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.4/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc yum -y install mongodb-org 配置文件：vim /etc/mongod.conf 命令： 启动mongodb ：systemctl start mongod.service 停止mongodb ：systemctl stop mongod.service 查到mongodb的状态：systemctl status mongod.service 关闭firewall： systemctl stop firewalld.service #停止firewall systemctl disable firewalld.service #禁止firewall开机启动 设置开机启动systemctl enable mongod.service 启动mongo 查看数据库show dbs 设置mongodb远程访问 vim /etc/mongod.conf 注释 bindIp 重启mongodb systemctl restart mongod.service 4. ubuntu16.04 第一步 #setp 1. Import the public key used by the package management system. sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6 第二步 #step 2. Create a list file for MongoDB echo \"deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list 第三步 #step 3. Reload local package database sudo apt-get update 第四步 #step 4. Install the latest stable version of MongoDB sudo apt-get install -y mongodb-org 启动服务命令 sudo service mongod stop #停止服务 sudo service mongod start #启动服务 sudo service mongod restart #重新启动服务 sudo service mongod status #查看状态 开机启动sudo systemctl enable mongod 默认位置less /etc/mongod.conf 工具robo3T","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://dreamcat.ink/tags/mongodb/"}]},{"title":"windows、mac和linux安装redis","slug":"windows、mac和linux安装redis","date":"2019-05-28T07:55:42.000Z","updated":"2020-10-30T12:45:42.837Z","comments":true,"path":"2019/05/28/windows-mac-he-linux-an-zhuang-redis/","link":"","permalink":"http://dreamcat.ink/2019/05/28/windows-mac-he-linux-an-zhuang-redis/","excerpt":"","text":"准备工作 redis windows10 mac ubuntu Centos7 安装1. windows 进入官网下载对应的版本 启动服务redis-server redis.windows.conf 添加环境变量-压缩包路径哦-比如C:\\redis 启动客户端redis-cli 接下来设置windows下的服务 cd到redis根目录下 执行redis-server --service-install redis.windows-service.conf --loglevel verbose 可以在计算机管理-&gt;服务查看redis 常用windows下的服务命令 卸载服务：redis-server --service-uninstall 开启服务：redis-server --service-start 停止服务：redis-server --service-stop 注意服务配置文件redis.windows-service.conf 2. mac brew大法好 brew install redis 配置文件/usr/local/etc/redis.conf 利用brew命令启动服务brew services start redis 3.centos73.1 安装redis yum install redis yum install epel-release yum install redis 3.2 修改配置 vi /etc/redis.conf 修改端口port 6379 修改ip0.0.0.0 修改密码requirepass 123 修改远程配置protected-mode yes 设为no 3.2 相关命令# 启动 systemctl start redis.service # 关闭 systemctl stop redis.service # 重启 systemctl restart redis.service 4. ubuntu sudo apt-get install redis-server 配置文件路径/etc/redis/redis.conf 配置说明和centos一样 工具 Redis Desktop Manager","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://dreamcat.ink/tags/redis/"}]},{"title":"windows、mac和linux安装mysql","slug":"windows、mac和linux安装mysql","date":"2019-05-27T08:43:18.000Z","updated":"2020-10-29T15:46:02.197Z","comments":true,"path":"2019/05/27/windows-mac-he-linux-an-zhuang-mysql/","link":"","permalink":"http://dreamcat.ink/2019/05/27/windows-mac-he-linux-an-zhuang-mysql/","excerpt":"","text":"1. 准备工作 windows mysql 绿色版 mac linux（centos、ubuntu） 2. 安装2.1 win绿色版 下载官网 选择对应的版本（Windows (x86, 64-bit), ZIP Archive） 解压存放D:\\db\\mysql-5.7.19-winx64 此电脑–&gt;右键–&gt;属性–&gt;高级系统设置–&gt;环境变量–&gt;系统变量–&gt;Path–&gt;编辑–&gt;新建–&gt;填入“D:\\db\\mysql-5.7.19-winx64\\bin”–&gt;确认 配置mysql.ini [mysql] # 设置mysql客户端默认字符集 default-character-set=utf8 [mysqld] #设置3306端口 port = 3306 # 设置mysql的安装目录 basedir=D:/db/mysql-5.7.19-winx64 # 设置mysql数据库的数据的存放目录 datadir=D:/db/mysqldata # 允许最大连接数 max_connections=200 # 服务端使用的字符集默认为8比特编码的latin1字符集 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB # group by 的一些问题 sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION 打开cmd（管理员）进入bin目录 安装mysql服务mysqld install 初始化mysqld --initialize 开启服务net start mysql 查找初始密码datadir=C:/db/mysqldata 找到.err文件打开找到root@localhost后面的密码 登陆mysql -uroot -p 输入密码 更改密码ALTER USER &quot;root&quot;@&quot;localhost&quot; IDENTIFIED BY &quot;123456&quot;; 使用Navicat for mysql可能出现的错误： 终端进入mysql，输入use mysql; alter user &#39;root&#39;@&#39;localhost&#39; identfied with mysql_native_password by &#39;123456&#39; 刷新flush privileges 2.2 mac 选择安装的方式利用brew brew install mysql 登陆不需要密码 修改密码ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;xxx&#39;; 2.3 linux(centos7) yum list | grep mysql（查看版本） yum install -y mysql-server mysql mysql-deve 安装工具 service mysqld start 初始化 mysqladmin -u root password &#39;root&#39; 设置密码 设置远程连接 mysql -u root -p show databases; use mysql; grant all privileges on *.* to 'root'@'%' identified by 'root'; # 赋予权限和密码 flush privileges; # 刷新 centos7 以上命令 systemctl start mariadb.service # 启动 systemctl enable mariadb.service # 开启服务 mysql_secure_installation 2.4 linux(ubuntu) sudo apt-get install mysql-server 下载过程中可能会出现密码 sudo apt-get install mysql-client sudo apt-get install libmysqlclient-dev mysql -uroot -p登陆密码即可 远程访问sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf 注释掉bind-address = 127.0.0.1 和以上centos设置远程连接相同 重启service mysql restart 2.5 deepin sudo apt-get install mysql-server mysql-client sudo mysql -u root -p enter即可进入 UPDATE mysql.user SET authentication_string = PASSWORD(&#39;123&#39;), plugin = &#39;mysql_native_password&#39; WHERE User = &#39;root&#39; AND Host = &#39;localhost&#39;; FLUSH PRIVILEGES; 重启sudo service mysql restart 3. 简单命令—后续补充","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"db","slug":"db","permalink":"http://dreamcat.ink/tags/db/"}]},{"title":"nvm管理nodejs","slug":"nvm管理nodejs","date":"2019-05-26T02:26:09.000Z","updated":"2020-10-29T15:49:05.026Z","comments":true,"path":"2019/05/26/nvm-guan-li-nodejs/","link":"","permalink":"http://dreamcat.ink/2019/05/26/nvm-guan-li-nodejs/","excerpt":"","text":"​ windows和mac管理nvm，nvm方便管理nodejs和npm等。。。 1. 准备工作 安装前卸载完全node 2. 安装github链接第一种curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash 第二种wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash 环境变量The script clones the nvm repository to ~/.nvm and adds the source line to your profile (~/.bash_profile, ~/.zshrc, ~/.profile, or~/.bashrc). export NVM_DIR=\"$HOME/.nvm\" [ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm [ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\" # This loads nvm bash_completion mac brew install nvm 环境变量 # add nvm export NVM_DIR=\"$HOME/.nvm\" [ -s \"/usr/local/opt/nvm/nvm.sh\" ] && . \"/usr/local/opt/nvm/nvm.sh\" # This loads nvm [ -s \"/usr/local/opt/nvm/etc/bash_completion\" ] && . \"/usr/local/opt/nvm/etc/bash_completion\" # This loads nvm bash_completion windows 参考链接 3. 常用命令 查看可安装的node版本 nvm ls-remote 查看已经安装的node版本 nvm ls 安装其他版本node nvm install 8.9 查看当前使用node版本 node --version 切换使用node版本 nvm use 8.9.4 卸载某个版本node nvm uninstall 0.11","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"nvm","slug":"nvm","permalink":"http://dreamcat.ink/tags/nvm/"}]},{"title":"frp内网穿透","slug":"frp内网穿透","date":"2019-05-25T14:24:46.000Z","updated":"2020-10-30T12:40:53.127Z","comments":true,"path":"2019/05/25/frp-nei-wang-chuan-tou/","link":"","permalink":"http://dreamcat.ink/2019/05/25/frp-nei-wang-chuan-tou/","excerpt":"","text":"​ frp内网穿透，非常详细的步骤说明。 1. 准备工作 vps（云服务器一台） 访问目标设备（就是你最终要访问的设备） 简单的Linux基础（会用cp等几个简单命令即可） 2. 下载frp frp-github 选择release中对应的版本 比如linux：frp_0.27.0_linux_amd64.tar.gz 3. 配置frp1. 简单介绍 frps（服务端启动） frps.ini（服务器启动配置文件） frpc（客户端启动） frpc.ini（客户端启动配置文件） 配置前先备份哦cp 2. 服务端 vim frps.ini 有以下内容 [common] bind_port = 7000 dashboard_port = 7500 token = 12345678 dashboard_user = admin dashboard_pwd = admin “bind_port”表示用于客户端和服务端连接的端口，这个端口号我们之后在配置客户端的时候要用到。 “dashboard_port”是服务端仪表板的端口，若使用7500端口，在配置完成服务启动后可以通过浏览器访问 x.x.x.x:7500 （其中x.x.x.x为VPS的IP）查看frp服务运行信息。 “token”是用于客户端和服务端连接的口令，请自行设置并记录，稍后会用到。 “dashboard_user”和“dashboard_pwd”表示打开仪表板页面登录的用户名和密码，自行设置即可。 ./frps -c frps.ini 若出现以下内容 2019/01/12 15:22:39 [I] [service.go:130] frps tcp listen on 0.0.0.0:7000 2019/01/12 15:22:39 [I] [service.go:172] http service listen on 0.0.0.0:10080 2019/01/12 15:22:39 [I] [service.go:193] https service listen on 0.0.0.0:10443 2019/01/12 15:22:39 [I] [service.go:216] Dashboard listen on 0.0.0.0:7500 2019/01/12 15:22:39 [I] [root.go:210] Start frps success 此时访问 x.x.x.x:7500 并使用自己设置的用户名密码登录，即可看到仪表板界面 后台运行nohup ./frps -c frps.ini &amp; 3. 客户端 vim frpc.ini 有以下内容 [common] server_addr = x.x.x.x # 服务器地址 server_port = 7000 # 和服务器端口对应 token = 12345678 # 和服务器token对应 [ssh] type = tcp local_ip = 127.0.0.1 local_port = 22 remote_port = 2222 “server_addr”为服务端IP地址，填入即可。 “server_port”为服务器端口，填入你设置的端口号即可，如果未改变就是7000 “token”是你在服务器上设置的连接口令，原样填入即可。 ./frpc -c frpc.ini 后台运行如服务器同上 映射web项目的端口服务端vim frps.ini [common] bind_port = 6000 token = mai vhost_http_port = 2020 # 这里很重要哈 ./frps -c frps.ini 客户端vim frpc.ini [common] server_addr = 39.108.xx.xxx server_port = 6000 token = mai #[ssh] #type = tcp #local_ip = 127.0.0.1 #local_port = 22 #remote_port = 6000 # 举例第一个 [web-flask] type = http local_port = 5000 custom_domains = flask.dreamcat.ink # 举例第二个 [web-flask2] type = http local_port = 5001 custom_domains = flask2.dreamcat.ink ./frpc -c frpc.ini 注意：custom_domains的域名需要去域名系统解析上述外网地址 访问：flask.dreamcat.ink 访问：flas2.dreamcat.ink 4. 开机自启1. 第一种最简单粗暴的方式直接在脚本/etc/rc.d/rc.local(和/etc/rc.local是同一个文件，软链)末尾添加自己的脚本然后，增加脚本执行权限。 nohup /home/dsp/config/frp/frpc -c /home/dsp/config/frp/frpc.ini & chmod +x /etc/rc.d/rc.local 2. 第二种crontab -e @reboot /home/user/test.sh 每次登陆自动执行也可以设置每次登录自动执行脚本，在/etc/profile.d/目录下新建sh脚本，/etc/profile会遍历/etc/profile.d/*.sh 第三种压缩包中有systemd，可利用这个服务开机自启 比如，将frps.server复制到etc/systemd/system/ [Unit] Description=Frp Server Service After=network.target [Service] Type=simple User=nobody Restart=on-failure RestartSec=5s ExecStart=/usr/bin/frps -c /etc/frp/frps.ini ##这里记得对应的路径 [Install] WantedBy=multi-user.target 接着可以利用systemd命令，比如 systemctl start frps #启动 systemctl stop frps #停止 systemctl restart frps #重启 systemctl status frps #查看状态 systemctl enable frps #开机启动frp systemctl disable frps # 禁止启动","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"frp","slug":"frp","permalink":"http://dreamcat.ink/tags/frp/"}]},{"title":"virtualenv和virtualenvwrapper","slug":"virtualenv和virtualenvwrapper","date":"2019-05-24T12:12:17.000Z","updated":"2020-10-29T15:47:16.732Z","comments":true,"path":"2019/05/24/virtualenv-he-virtualenvwrapper/","link":"","permalink":"http://dreamcat.ink/2019/05/24/virtualenv-he-virtualenvwrapper/","excerpt":"","text":"​ 安装virtualenv和virtualenvwrapper，方便管理隔离python环境。 1. windows pip install virtualenv virtualenvwrapper-win 更改目录\\script\\mkvirtualenv.bat修改default_workon_home的值WORKON_HOME Windows下默认虚拟环境是放在用户名下面的Envs中的，与桌面，我的文档，下载等文件夹在一块的。更改方法：计算机，属性，高级系统设置，环境变量，添加WORKON_HOME。 2. centos 或linux (sudo) pip install virtualenv virtualenvwrapper 修改.bashrc 或用 ZSH 之后的 .zshrc VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3 export WORKON_HOME=$HOME/Envs # 存储虚拟环境的目录可以任意命名，不一定就是.virtualenvs source /usr/local/bin/virtualenvwrapper.sh # 可以使用whereis virtualenvwrapper命令来查找sh文件的目录 # 我个人 source /usr/local/python3Dir/bin/virtualenvwrapper.sh 给virtualenv添加软链接ln -s /usr/local/python3Dir/bin/virtualenv /usr/bin/virtualenv 更新环境变量source .bash_profile 3. virtualenvwrapper用法 一些常用命令 mkvirtualenv 环境名：创建环境 workon：当前存在环境列表 workon 环境名：选择环境 rmvirtualenv 环境名：删除环境 mkproject mic：创建mic项目和运行环境mic mktmpenv：创建临时运行环境 lsvirtualenv：列出可用的运行环境 cdvirtualenv：进入虚拟环境目录 cdsitepackages：进入虚拟环境的site-packages目录 lssitepackages: 列出当前环境安装了的包 deactivate：退出环境 4. 常用pip freeze &gt; requirements.txt # 环境迁出，txt文件可以任意命名pip install -r requirements.txt # 环境迁入","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"python","slug":"python","permalink":"http://dreamcat.ink/tags/python/"}]},{"title":"mac终端(iterm2 zsh oh-my-zsh)","slug":"mac终端-iterm2-zsh-oh-my-zsh","date":"2019-05-24T11:54:07.000Z","updated":"2020-10-30T12:44:29.485Z","comments":true,"path":"2019/05/24/mac-zhong-duan-iterm2-zsh-oh-my-zsh/","link":"","permalink":"http://dreamcat.ink/2019/05/24/mac-zhong-duan-iterm2-zsh-oh-my-zsh/","excerpt":"","text":"引言Mac或者linux终端配置… 工具准备 iterm2 iterm2 Solarized Dark Higher Contrast 配色方案 Monaco for Powerline 字体 zsh （Mac 系统自带，无需安装） Oh-My-Zsh Oh-My-Zsh powerlevel9k 主题 开始安装安装 iTerm2 Solarized Dark Higher Contrast 配色方案： 将该配色方案文件（Solarized Dark Higher Contrast.itermcolors）复制出来，保存到本地，文件命名为 SolarizedDarkHigherContrast.itermcolors ，然后双击即可安装。安装完后打开 iTerm2 终端，依次选择菜单栏：iTerm2 –&gt; Preferences –&gt; Profiles –&gt; Colors –&gt; Colors Presets –&gt; SolarizedDarkHigherContrast，至此 iTerm2 Solarized Dark Higher Contrast 配色方案已成功安装。 安装 Monaco for Powerline 字体： 将该仓库克隆到本地，然后进入工程目录的 Monaco 目录，双击后缀名为 .otf 的字体文件即可完成该字体的安装。安装该字体的原因主要是为了和 Oh-My-Zsh 的 powerlevel9k 主题相兼容，如果不安装该字体，那么后面安装 powerlevel9kn 主题后会出现乱码。 git clone https://github.com/supermarin/powerline-fonts.git Iterm2 偏好设置-&gt;profile-&gt;Text-&gt;Font-&gt;Change Font 更改字体 安装配置 zsh： zsh 一般 Mac 已经自带了，无需额外安装。可以用 cat /etc/shells 查看 zsh 是否安装，如果列出了 /bin/zsh 则表明 zsh 已经安装了。 接下来修改 iTerm2 终端的默认 Shell，可以用 echo $SHELL 查看当前 Shell 是什么，如果不是 /bin/zsh 则用如下命令修改 iTerm2 的默认 Shell 为 zsh： chsh -s /bin/zsh 安装 Oh-My-Zsh： sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; 安装配置 Oh-My-Zsh powerlevel9k 主题： 克隆该仓库到 oh-my-zsh 用户自定义主题目录git clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k 修改 ~/.zshrc 配置文件，配置该主题ZSH_THEME=&quot;powerlevel9k/powerlevel9k&quot; 配置代码见文章底部 配置 zsh 命令语法高 zsh-syntax-highlighting 命令有语法高亮效 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting zsh-autosuggestions 代码补全插件 git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions 简单配置插件信息(.zshrc)# Add wisely, as too many plugins slow down shell startup. plugins=( git extract z zsh-syntax-highlighting zsh-autosuggestions ) git：oh-my-zsh 默认开启的，没什么好说的； extract：通用的解压缩插件，可以解压缩任何后缀的压缩文件，使用方法很简单：x 文件名； z：很智能的目录跳转插件，能记录之前 cd 过哪些目录，然后模糊匹配跳转，不需要输入全路径即可跳转，使用方法：z dir_pattern 注意source .zshrc 常用zsh（powerlevel9k）主题配置# ==== Theme Settings ==== # PowerLevel9k # 终端配色 export TERM=\"xterm-256color\" # user name 注意之前的DEFALUT_USER 注释 POWERLEVEL9K_CONTEXT_TEMPLATE=\"dream\" # 设置 oh-my-zsh powerlevel9k 主题左边元素显示 POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(context dir rbenv vcs) # 设置 oh-my-zsh powerlevel9k 主题右边元素显示 POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(virtualenv status root_indicator background_jobs time) #新起一行显示命令 (推荐！极其方便） POWERLEVEL9K_PROMPT_ON_NEWLINE=true #右侧状态栏与命令在同一行 POWERLEVEL9K_RPROMPT_ON_NEWLINE=true #缩短目录层级 POWERLEVEL9K_SHORTEN_DIR_LENGTH=1 #缩短目录策略：隐藏上层目录中间的字 #POWERLEVEL9K_SHORTEN_STRATEGY=\"truncate_middle\" #添加连接上下连接箭头更方便查看 POWERLEVEL9K_MULTILINE_FIRST_PROMPT_PREFIX=\"↱\" POWERLEVEL9K_MULTILINE_LAST_PROMPT_PREFIX=\"↳ \" # 新的命令与上面的命令隔开一行 #POWERLEVEL9K_PROMPT_ADD_NEWLINE=true # Git仓库状态的色彩指定 POWERLEVEL9K_VCS_CLEAN_FOREGROUND='blue' POWERLEVEL9K_VCS_CLEAN_BACKGROUND='black' POWERLEVEL9K_VCS_UNTRACKED_FOREGROUND='yellow' POWERLEVEL9K_VCS_UNTRACKED_BACKGROUND='black' POWERLEVEL9K_VCS_MODIFIED_FOREGROUND='red' POWERLEVEL9K_VCS_MODIFIED_BACKGROUND='black'","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"终端","slug":"终端","permalink":"http://dreamcat.ink/tags/%E7%BB%88%E7%AB%AF/"}]},{"title":"github pages和hexo搭建个人博客","slug":"github-pages和hexo搭建个人博客","date":"2019-05-24T01:57:16.000Z","updated":"2020-10-30T12:42:42.127Z","comments":true,"path":"2019/05/24/github-pages-he-hexo-da-jian-ge-ren-bo-ke/","link":"","permalink":"http://dreamcat.ink/2019/05/24/github-pages-he-hexo-da-jian-ge-ren-bo-ke/","excerpt":"","text":"1. 环境 浏览器 nodejs git github 2. 前提 github账号 git环境 3. 搭建3.1 环境搭建3.1.1 nodejs windows： nodejs下载地址 选择windwos instller 对应位数 选择目录进行安装、一路next即可 终端测试node -v 若显示版本号，则windows nodejs环境搭建成功 linux nodejs下载地址 选择linux x86/x64 选择合适的地方存放 tar -xvf node-vx.xx.x-linux-x64.tar.xz mv node-vx.xx.x-linux-x64 nodejs 建立软连接 ln -s /user_local/nodejs/bin/npm /usr/local/bin/ ln -s /user_local/nodejs/bin/node /usr/local/bin/ 终端测试node -v mac 以上官网下载 选择mac对应的installer 下载安装即可 同样测试终端node -v 3.1.2 git windows 官网 下载安装即可 终端测试git --version Linux sudo apt-get install git 终端测试git --version mac mac自带 3.2 Github3.2.1 创建用户 官网注册 牢记用户名，后续要用 3.2.2 创建github.io项目 在主页上new一个repository 如图示： 每个账户只能创建一个github.io 上图所示代表我已经创建 注意用户名和初始化README 3.3 配置git和github 打开终端设置username和email 如果个人有多个git账户，可以看另外一个篇文章 git config --global user.name \"xxx\" git config --global user.email \"xxx\" 终端命令创建ssh ssh-keygen -t rsa -C &quot;xxx&quot; 在.ssh目录下打开id_rsa.pub 复制其中的信息 github个人用户的setting中选择SSH and GPG keys 右上角选择new ssh key 按钮 title 自命 key 是刚才复制的信息 Add SSH Key 测试ssh -T git@github.com 出现successfully即可 3.4 Hexo 安装hexo-clinpm install -g hexo-cli 初始化项目hexo init Blog 简单的三条命令 hexo new test_my_site #创建一篇新文章 hexo g #生成文件 hexo s #本地 server 部署 打开浏览器输入地址： localhost:4000 hexo n \"我的博客\" == hexo new \"我的博客\" #新建文章 hexo g == hexo generate #生成 hexo s == hexo server #启动服务预览 hexo d == hexo deploy #部署 hexo clean #清除缓存，若是网页正常情况下可以忽略这条命令 3.5 部署到github 在Blog文件夹中打开_config.yml 部署配置 deploy: type: git repo: 这里填入你之前在GitHub上创建仓库的完整路径 branch: master 注意上面:之后有空格 Blog根目录下安装部署插件才能部署npm install hexo-deployer-git --save 执行三条命令 hexo clean hexo g hexo d or hexo clean hexo g -d 浏览器上输入xxx.github.io 4. 部署到阿里云上实际上就是创建个钩子 git init --bare blog.git 有一个自动生成的 hooks 文件夹。我们需要在里边新建一个新的钩子文件 post-receive git --work-tree=/www/wwwroot/dreamcat.ink/blog --git-dir=/root/blog.git checkout -f 5. 个人博客点我","categories":[{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"github","slug":"github","permalink":"http://dreamcat.ink/tags/github/"}]},{"title":"开源的github项目总结","slug":"开源的github项目总结","date":"2019-03-24T15:07:33.000Z","updated":"2020-10-29T15:44:46.695Z","comments":true,"path":"2019/03/24/kai-yuan-de-github-xiang-mu-zong-jie/","link":"","permalink":"http://dreamcat.ink/2019/03/24/kai-yuan-de-github-xiang-mu-zong-jie/","excerpt":"","text":"引言 一些有趣的github项目总结，其中包括终端、Python、Java、笔试&amp;面试、效率软件等。 终端 The Art of Command Line :系统的学习命令行的用法。star:57509 oh-my-zsh: 这玩意不用我简单介绍了吧？star:90516 git-tips Git的奇技淫巧 star：11400 powerline-fonts : mac挺好用的一款终端字体。star:169 powerlevel9k : oh-my-zsh的一款皮肤。star:9952 zsh-syntax-highlighting : 终端输入的命令有语法高亮效果。star:7326 zsh-autosuggestions : 终端代码补全插件。star:6662 Python awesome-python-login-model : 模拟登陆一些大型网站的demo，个人觉得不错，值得学习。star:8403 pyppeteer : 模拟动态加载js，比selenium稍微好用一些。star:1924 requests :Python HTTP Requests for Humans™ ✨🍰✨ star:39860 requests-html : Pythonic HTML Parsing for Humans™ star:10111 httpx A next generation HTTP client for Python. 🦋 https://www.encode.io/httpx star:1900 PySimpleGUI : 做一些简单的GUI，可以用这个，简单应用。star:1608 bokeh Interactive Web Plotting for Python。 star:10701 wxpy 微信机器人 / 可能是最优雅的微信个人号 API ✨✨ http://wxpy.readthedocs.io star:10700 Java Awesome Java A curated list of awesome frameworks, libraries and software for the Java programming language. star:21651 1. 后端 spring-boot-examples : 对于初学者学习Spring-boot，是个非常不错的例子。star:16408 SpringAll循序渐进，学习Spring Boot、Spring Boot &amp; Shiro、Spring Cloud、Spring Security &amp; Spring Security OAuth2，博客Spring系列源码。star:6181 interest : Vue+Spring boot前后端分离博客项目。star:494 litemall : 一个小商城。litemall = Spring Boot后端 + Vue管理员前端 + 微信小程序用户前端 + Vue用户移动端。star:7586 vhr微人事是一个前后端分离的人力资源管理系统，项目采用SpringBoot+Vue开发。star:4705 mybatis MyBatis SQL mapper framework for Java star:11335 miaosha ⭐⭐⭐⭐秒杀系统设计与实现.互联网工程师进阶与分析🙋🐓 star:9400 spring-boot-plus🔥spring-boot-plus集成Spring Boot 2.1.6,Mybatis,Mybatis Plus,Druid,FastJson,Redis,Rabbit MQ,Kafka等，可使用代码生成器快速开发项目. star:551 hope-boot 🌱🚀一款现代化的脚手架项目。🍻整合Springboot2 star:1543 spring-boot-demo spring boot demo 是一个用来学习 spring boot 的项目，总共包含 57 个集成demo，已经完成 47 个。star:2149 spring-boot-api-project-seed 🌱🚀一个基于Spring Boot &amp; MyBatis的种子项目，用于快速构建中小型API、RESTful API项目~ star:1543 White-Jotter: 白卷是一款使用 Vue+Spring Boot 开发的前后端分离的图书管理项目 star:115 eladmin 项目基于 Spring Boot 2.1.0 、 Jpa、 Spring Security、redis、Vue的前后端分离的后台管理系统，项目采用分模块开发方式， 权限控制采用 RBAC，支持数据字典与数据权限管理，支持一键生成前后端代码，支持动态路由 start:3163 dbblog 基于SpringBoot2.x+Vue2.x+ElementUI+Iview+Elasticsearch+RabbitMQ+Redis+Shiro的多模块前后端分离的博客项目 star:375 spring-analysis Spring源码阅读 star:4045 mall mall项目是一套电商系统，包括前台商城系统及后台管理系统，基于SpringBoot+MyBatis实现。star:21926 后端架构师技术图谱 star:42900 mall-swarmmall-swarm是一套微服务商城系统，采用了 Spring Cloud Greenwich、Spring Boot 2、MyBatis、Docker、Elasticsearch等核心技术，同时提供了基于Vue的管理后台方便快速搭建系统。star:1400 jeecg-boot一款基于代码生成器的JAVA快速开发平台，开源界“小普元”超越传统商业企业级开发平台！star:9600 dbblog 基于SpringBoot2.x+Vue2.x+ElementUI+Iview+Elasticsearch+RabbitMQ+Redis+Shiro的多模块前后端分离的博客项目 http://www.dblearn.cn star:643 springboot-seckill 基于SpringBoot + MySQL + Redis + RabbitMQ + Guava开发的高并发商品限时秒杀系统 star:943 MeetingFilm基于微服务架构的在线电影购票平台 star：111 guava Google core libraries for Java star:36400 hutool A set of tools that keep Java sweet. http://www.hutool.cn star:10800 2. 笔试&amp;&amp;面试 JavaGuide :一份涵盖大部分Java程序员所需要掌握的核心知识。star:45159 advanced-java :互联网 Java 工程师进阶知识完全扫盲：涵盖高并发、分布式、高可用、微服务等领域知识，后端同学必看，前端同学也可学习。star:22747 LeetCodeAnimation : LeetCode用动画的形式的呈现。star:32650 technology-talk 汇总java生态圈常用技术框架等。star:6229 interview_internal_reference 2019年最新总结，阿里，腾讯，百度，美团，头条等技术面试题目，以及答案，专家出题人分析汇总。 star:14335 interviews Everything you need to know to get the job. star:37598 interview_internal_reference2019年最新总结，阿里，腾讯，百度，美团，头条等技术面试题目，以及答案，专家出题人分析汇总。 star:15570 leetcode LeetCode Solutions: A Record of My Problem Solving Journey.( leetcode题解，记录自己的leetcode解题之路。) reverse-interview-zh 技术面试最后反问面试官的话 star:4500 algo 数据结构和算法必知必会的50个代码实现 star:10700 fucking-algorithm 手把手撕LeetCode题目，扒各种算法套路的裤子，not only how，but also why. English version supported! https://labuladong.gitbook.io/algo/ star：4700 JavaFamily【互联网一线大厂面试+学习指南】进阶知识完全扫盲：涵盖高并发、分布式、高可用、微服务等领域知识，作者风格幽默，看起来津津有味，把学习当做一种乐趣，何乐而不为，后端同学必看，前端同学我保证你也看得懂，看不懂你加我微信骂我渣男就好了。 star：7100 Java-Interview Java 面试必会 直通BAT star:3500 Vue&amp;&amp;前端 awesome-vue awesome things related to Vue.js star:46634 vue-form-making基于Vue的表单设计器，让表单开发简单而高效。 star:1347 fe-interview 前端面试每日 3+1，以面试题来驱动学习，提倡每日学习与思考，每天进步一点！每天早上5点纯手工发布面试题（死磕自己，愉悦大家）star:6667 vue2-elm 基于 vue2 + vuex 构建一个具有 45 个页面的大型单页面应用 star:30300 Web 前端入门和进阶学习笔记，超详细的Web前端学习图文教程。从零开始学前端，做一名精致的前端工程师。持续更新… star:55300 javascript-algorithms Algorithms and data structures implemented in JavaScript with explanations and links to further readings star:55277 nodebestpractices ✅ The largest Node.js best practices list (September 2019) https://twitter.com/nodepractices/ star:35000 gods-pen 基于vue的高扩展在线网页制作平台，可自定义组件，可添加脚本，可数据统计。A mobile page builder/editor, similar with amolink. https://godspen.ymm56.com star:1200 Daily-Interview-Question 我是木易杨，公众号「高级前端进阶」作者，每天搞定一道前端大厂面试题，祝大家天天进步，一年后会看到不一样的自己。star：17000 面试 CS-Notes : 技术面试必备基础知识、Leetcode 题解、Java、C++、Python、后端面试、操作系统、计算机网络、系统设计。star:67433 Flutter Flutter_YYeTs : 基于Flutter的人人影视移动端。star:233 Flutter-Notebook 日更的FlutterDemo合集，今天你fu了吗 star:3879 Best-Flutter-UI-Templates completely free for everyone. Its build-in Flutter Dart. star:1261 效率软件 frp : 内网穿透，你懂的。star:24539 musicbox : 网易云音乐命令行版本。star:7601 motrix : 配合百度云有着奇淫技巧。 star:11098 Electronic WeChat : 该项目虽然不再维护，但挺好用的(linux)。star:12622 hexo : 搭建博客系统框架，挺好用。star:26868 awesome-mac推荐的非常给力且实用。 star:29814 蓝灯 免费的vpn。 star:9520 LazyDocker The lazier way to manage everything docker. star:10906 postwoman 👽 API request builder - Helps you create your requests faster, saving you precious time on your development. star:2190 x64dbgAn open-source x64/x32 debugger for windows. star:34260 google-access-helper 谷歌访问助手破解版 star:3887 v2ray-core A platform for building proxies to bypass network restrictions. star:21444 v2rayN vpn star:1249 Alfred-collection A collection of all known Alfred3 workflows star:411 RevokeMsgPatcher editor for WeChat/QQ/TIM - PC版微信/QQ/TIM防撤回补丁（我已经看到了，撤回也没用了） star:1500 lx-music-desktop 一个基于 electron 的音乐软件 star:1300 UnblockNeteaseMusic Revive unavailable songs for Netease Cloud Music star:7900 CodeVar 生成可用的代码变量 (CodeVar that return u a better variable from Chinese to English . ) star：684 有趣 ChineseBQB🇨🇳Chinese sticker pack,More joy / 中国表情包大集合,更欢乐 star:2319 free-programming-books-zh_CN📚 免费的计算机编程类中文书籍，欢迎投稿 star:55296 freeCodeCamp The https://www.freeCodeCamp.org open source codebase and curriculum. Learn to code for free together with millions of people. star:304920 hosts 镜像 star:15582 free-api 收集免费的接口服务,做一个api的搬运工 star:6000 fanhaodaquan 番号大全。 star:1200 BullshitGenerator Needs to generate some texts to test if my GUI rendering codes good or not. so I made this. star:3700 howto-make-more-money: 程序员如何优雅的挣零花钱 star:8200 marktext A simple and elegant markdown editor, available for Linux, macOS and Windows. https://marktext.app star：14800 FreePAC 科学上网/梯子/自由上网/翻墙 SS/SSR/V2Ray/Brook 搭建教程 star：2146 持续更新…","categories":[{"name":"web","slug":"web","permalink":"http://dreamcat.ink/categories/web/"}],"tags":[{"name":"github","slug":"github","permalink":"http://dreamcat.ink/tags/github/"}]}],"categories":[{"name":"秋招","slug":"秋招","permalink":"http://dreamcat.ink/categories/%E7%A7%8B%E6%8B%9B/"},{"name":"工具","slug":"工具","permalink":"http://dreamcat.ink/categories/%E5%B7%A5%E5%85%B7/"},{"name":"java","slug":"java","permalink":"http://dreamcat.ink/categories/java/"},{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/categories/spring/"},{"name":"linux","slug":"linux","permalink":"http://dreamcat.ink/categories/linux/"},{"name":"vue","slug":"vue","permalink":"http://dreamcat.ink/categories/vue/"},{"name":"db","slug":"db","permalink":"http://dreamcat.ink/categories/db/"},{"name":"web","slug":"web","permalink":"http://dreamcat.ink/categories/web/"},{"name":"algorithm","slug":"algorithm","permalink":"http://dreamcat.ink/categories/algorithm/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://dreamcat.ink/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"MQ","slug":"MQ","permalink":"http://dreamcat.ink/tags/MQ/"},{"name":"dubbo","slug":"dubbo","permalink":"http://dreamcat.ink/tags/dubbo/"},{"name":"redis","slug":"redis","permalink":"http://dreamcat.ink/tags/redis/"},{"name":"db","slug":"db","permalink":"http://dreamcat.ink/tags/db/"},{"name":"mybatis","slug":"mybatis","permalink":"http://dreamcat.ink/tags/mybatis/"},{"name":"spring","slug":"spring","permalink":"http://dreamcat.ink/tags/spring/"},{"name":"jvm","slug":"jvm","permalink":"http://dreamcat.ink/tags/jvm/"},{"name":"java集合","slug":"java集合","permalink":"http://dreamcat.ink/tags/java%E9%9B%86%E5%90%88/"},{"name":"java基础","slug":"java基础","permalink":"http://dreamcat.ink/tags/java%E5%9F%BA%E7%A1%80/"},{"name":"多线程","slug":"多线程","permalink":"http://dreamcat.ink/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"github","slug":"github","permalink":"http://dreamcat.ink/tags/github/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://dreamcat.ink/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"Dubbo","slug":"Dubbo","permalink":"http://dreamcat.ink/tags/Dubbo/"},{"name":"linux","slug":"linux","permalink":"http://dreamcat.ink/tags/linux/"},{"name":"vue","slug":"vue","permalink":"http://dreamcat.ink/tags/vue/"},{"name":"web","slug":"web","permalink":"http://dreamcat.ink/tags/web/"},{"name":"算法","slug":"算法","permalink":"http://dreamcat.ink/tags/%E7%AE%97%E6%B3%95/"},{"name":"java","slug":"java","permalink":"http://dreamcat.ink/tags/java/"},{"name":"终端","slug":"终端","permalink":"http://dreamcat.ink/tags/%E7%BB%88%E7%AB%AF/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://dreamcat.ink/tags/ubuntu/"},{"name":"vscode","slug":"vscode","permalink":"http://dreamcat.ink/tags/vscode/"},{"name":"mongodb","slug":"mongodb","permalink":"http://dreamcat.ink/tags/mongodb/"},{"name":"nvm","slug":"nvm","permalink":"http://dreamcat.ink/tags/nvm/"},{"name":"frp","slug":"frp","permalink":"http://dreamcat.ink/tags/frp/"},{"name":"python","slug":"python","permalink":"http://dreamcat.ink/tags/python/"}]}